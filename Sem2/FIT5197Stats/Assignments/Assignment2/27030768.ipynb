{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9V5ptJAIEbw"
   },
   "source": [
    "# FIT5197 2025 S1 Final Assessment\n",
    "\n",
    "**SPECIAL NOTE:** Please refer to the [assessment page (Clayton)](https://learning.monash.edu/mod/assign/view.php?id=3764302), or [(Malaysia)](https://learning.monash.edu/mod/assign/view.php?id=3939231) for rules, general guidelines and marking rubrics of the assessment (the marking rubric for the kaggle competition part will be released near the deadline - around 5 working days - in the same page). Failure to comply with the provided information will result in a deduction of mark (e.g., late penalties) or breach of academic integrity."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "id": "FkfAwCwbg7is"
   },
   "source": [
    "**PLEASE ENSURE YOU READ AND COMPLETE ALL REQUIRED INFORMATION BELOW BEFORE STARTING YOUR ASSIGNMENT, AS FAILURE TO DO SO WILL RESULT IN A MARK OF ZERO.**\n",
    "\n",
    "\n",
    "**YOUR NAME**: Adrian Leong Tat Wei\n",
    "\n",
    "**STUDENT ID**: 27030768\n",
    "\n",
    "**KAGGLE NAME** Adrian Leong(required for marking – see Part 1, Question 5 or Part 2):\n",
    "\n",
    "**Important: Your Kaggle name must be identical (including capitalization, spacing, and special characters) across both Regression and Classification contests. Marks will be deducted if your Kaggle name is inconsistent or missing. No exceptions will be made!**\n",
    "\n",
    "You can find your Kaggle name by clicking on your profile icon at the top-right corner of the Kaggle website. For example, here is a screenshot of where to find it:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Please also enter your details in this [google form](https://forms.gle/HGxmCyd28XBebvi66)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_u6Dyj1xehk"
   },
   "source": [
    "# Part 1 Regression (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "marked-instrument"
   },
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual. You need to build regression models to optimally predict the variable in the survey dataset called 'happiness' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```regression_train.csv``` and ```regression_test.csv```. Using these datasets, you hope to build a model that can predict happiness level using the other variables. ```regression_train.csv``` comes with the ground-truth target label (i.e. happiness level) whereas `regression_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict happiness. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. e.g., the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Gbtyt1g7iv"
   },
   "source": [
    "**PLEASE NOTE THAT THE USE OF LIBRARIES ARE PROHIBITED IN THESE QUESTIONS UNLESS STATED OTHERWISE, ANSWERS USING LIBRARIES WILL RECEIVE 0 MARKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wound-marriage"
   },
   "source": [
    "## Question 1 (NO LIBRARIES ALLOWED) (4 Mark)\n",
    "Please load the ```regression_train.csv``` and fit a [$\\textbf{multiple linear regression model}$](https://en.wikipedia.org/wiki/Linear_regression) with 'happiness' being the target variable. According to the summary table, which predictors do you think are possibly associated with the target variable (use the significance level of 0.01), and which are the **Top 5** strongest predictors? Please write an R script to automatically fetch and print this information.\n",
    "\n",
    "**NOTE**: Manually doing the above tasks will result in 0 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 12 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-22.138272</td><td>3.5197268</td><td>-6.289770</td><td>7.814219e-10</td></tr>\n",
       "\t<tr><th scope=row>income10k - 15k</th><td>  6.268117</td><td>1.2386952</td><td> 5.060258</td><td>6.210721e-07</td></tr>\n",
       "\t<tr><th scope=row>income120k - 150k</th><td> 11.373025</td><td>1.2340393</td><td> 9.216096</td><td>1.369692e-18</td></tr>\n",
       "\t<tr><th scope=row>income150k - 200k</th><td> 11.718396</td><td>1.2434310</td><td> 9.424243</td><td>2.676570e-19</td></tr>\n",
       "\t<tr><th scope=row>income15k - 20k</th><td> 15.447908</td><td>1.2809264</td><td>12.059949</td><td>4.650658e-29</td></tr>\n",
       "\t<tr><th scope=row>income200k above</th><td> 20.109900</td><td>1.3788320</td><td>14.584735</td><td>1.902543e-39</td></tr>\n",
       "\t<tr><th scope=row>income20k - 50k</th><td> 20.839607</td><td>1.5825100</td><td>13.168704</td><td>1.604206e-33</td></tr>\n",
       "\t<tr><th scope=row>income50k - 80k</th><td> 28.546980</td><td>1.7382901</td><td>16.422449</td><td>2.033722e-47</td></tr>\n",
       "\t<tr><th scope=row>income80k - 120k</th><td> 37.315988</td><td>1.6548496</td><td>22.549474</td><td>6.160552e-75</td></tr>\n",
       "\t<tr><th scope=row>whatIsYourHeightExpressItAsANumberInMetresM175 - 180</th><td>  6.104323</td><td>2.2061513</td><td> 2.766956</td><td>5.901767e-03</td></tr>\n",
       "\t<tr><th scope=row>whatIsYourHeightExpressItAsANumberInMetresM180 - 185</th><td>  7.286107</td><td>2.3791630</td><td> 3.062466</td><td>2.332715e-03</td></tr>\n",
       "\t<tr><th scope=row>alwaysStressed</th><td> -1.064389</td><td>0.3726381</td><td>-2.856361</td><td>4.492518e-03</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 12 × 4\n",
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -22.138272 & 3.5197268 & -6.289770 & 7.814219e-10\\\\\n",
       "\tincome10k - 15k &   6.268117 & 1.2386952 &  5.060258 & 6.210721e-07\\\\\n",
       "\tincome120k - 150k &  11.373025 & 1.2340393 &  9.216096 & 1.369692e-18\\\\\n",
       "\tincome150k - 200k &  11.718396 & 1.2434310 &  9.424243 & 2.676570e-19\\\\\n",
       "\tincome15k - 20k &  15.447908 & 1.2809264 & 12.059949 & 4.650658e-29\\\\\n",
       "\tincome200k above &  20.109900 & 1.3788320 & 14.584735 & 1.902543e-39\\\\\n",
       "\tincome20k - 50k &  20.839607 & 1.5825100 & 13.168704 & 1.604206e-33\\\\\n",
       "\tincome50k - 80k &  28.546980 & 1.7382901 & 16.422449 & 2.033722e-47\\\\\n",
       "\tincome80k - 120k &  37.315988 & 1.6548496 & 22.549474 & 6.160552e-75\\\\\n",
       "\twhatIsYourHeightExpressItAsANumberInMetresM175 - 180 &   6.104323 & 2.2061513 &  2.766956 & 5.901767e-03\\\\\n",
       "\twhatIsYourHeightExpressItAsANumberInMetresM180 - 185 &   7.286107 & 2.3791630 &  3.062466 & 2.332715e-03\\\\\n",
       "\talwaysStressed &  -1.064389 & 0.3726381 & -2.856361 & 4.492518e-03\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 12 × 4\n",
       "\n",
       "| <!--/--> | Estimate &lt;dbl&gt; | Std. Error &lt;dbl&gt; | t value &lt;dbl&gt; | Pr(&gt;|t|) &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | -22.138272 | 3.5197268 | -6.289770 | 7.814219e-10 |\n",
       "| income10k - 15k |   6.268117 | 1.2386952 |  5.060258 | 6.210721e-07 |\n",
       "| income120k - 150k |  11.373025 | 1.2340393 |  9.216096 | 1.369692e-18 |\n",
       "| income150k - 200k |  11.718396 | 1.2434310 |  9.424243 | 2.676570e-19 |\n",
       "| income15k - 20k |  15.447908 | 1.2809264 | 12.059949 | 4.650658e-29 |\n",
       "| income200k above |  20.109900 | 1.3788320 | 14.584735 | 1.902543e-39 |\n",
       "| income20k - 50k |  20.839607 | 1.5825100 | 13.168704 | 1.604206e-33 |\n",
       "| income50k - 80k |  28.546980 | 1.7382901 | 16.422449 | 2.033722e-47 |\n",
       "| income80k - 120k |  37.315988 | 1.6548496 | 22.549474 | 6.160552e-75 |\n",
       "| whatIsYourHeightExpressItAsANumberInMetresM175 - 180 |   6.104323 | 2.2061513 |  2.766956 | 5.901767e-03 |\n",
       "| whatIsYourHeightExpressItAsANumberInMetresM180 - 185 |   7.286107 | 2.3791630 |  3.062466 | 2.332715e-03 |\n",
       "| alwaysStressed |  -1.064389 | 0.3726381 | -2.856361 | 4.492518e-03 |\n",
       "\n"
      ],
      "text/plain": [
       "                                                     Estimate   Std. Error\n",
       "(Intercept)                                          -22.138272 3.5197268 \n",
       "income10k - 15k                                        6.268117 1.2386952 \n",
       "income120k - 150k                                     11.373025 1.2340393 \n",
       "income150k - 200k                                     11.718396 1.2434310 \n",
       "income15k - 20k                                       15.447908 1.2809264 \n",
       "income200k above                                      20.109900 1.3788320 \n",
       "income20k - 50k                                       20.839607 1.5825100 \n",
       "income50k - 80k                                       28.546980 1.7382901 \n",
       "income80k - 120k                                      37.315988 1.6548496 \n",
       "whatIsYourHeightExpressItAsANumberInMetresM175 - 180   6.104323 2.2061513 \n",
       "whatIsYourHeightExpressItAsANumberInMetresM180 - 185   7.286107 2.3791630 \n",
       "alwaysStressed                                        -1.064389 0.3726381 \n",
       "                                                     t value   Pr(>|t|)    \n",
       "(Intercept)                                          -6.289770 7.814219e-10\n",
       "income10k - 15k                                       5.060258 6.210721e-07\n",
       "income120k - 150k                                     9.216096 1.369692e-18\n",
       "income150k - 200k                                     9.424243 2.676570e-19\n",
       "income15k - 20k                                      12.059949 4.650658e-29\n",
       "income200k above                                     14.584735 1.902543e-39\n",
       "income20k - 50k                                      13.168704 1.604206e-33\n",
       "income50k - 80k                                      16.422449 2.033722e-47\n",
       "income80k - 120k                                     22.549474 6.160552e-75\n",
       "whatIsYourHeightExpressItAsANumberInMetresM175 - 180  2.766956 5.901767e-03\n",
       "whatIsYourHeightExpressItAsANumberInMetresM180 - 185  3.062466 2.332715e-03\n",
       "alwaysStressed                                       -2.856361 4.492518e-03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 5 × 4</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>income80k - 120k</th><td>37.31599</td><td>1.654850</td><td>22.54947</td><td>6.160552e-75</td></tr>\n",
       "\t<tr><th scope=row>income50k - 80k</th><td>28.54698</td><td>1.738290</td><td>16.42245</td><td>2.033722e-47</td></tr>\n",
       "\t<tr><th scope=row>income20k - 50k</th><td>20.83961</td><td>1.582510</td><td>13.16870</td><td>1.604206e-33</td></tr>\n",
       "\t<tr><th scope=row>income200k above</th><td>20.10990</td><td>1.378832</td><td>14.58474</td><td>1.902543e-39</td></tr>\n",
       "\t<tr><th scope=row>income15k - 20k</th><td>15.44791</td><td>1.280926</td><td>12.05995</td><td>4.650658e-29</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 5 × 4\n",
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\tincome80k - 120k & 37.31599 & 1.654850 & 22.54947 & 6.160552e-75\\\\\n",
       "\tincome50k - 80k & 28.54698 & 1.738290 & 16.42245 & 2.033722e-47\\\\\n",
       "\tincome20k - 50k & 20.83961 & 1.582510 & 13.16870 & 1.604206e-33\\\\\n",
       "\tincome200k above & 20.10990 & 1.378832 & 14.58474 & 1.902543e-39\\\\\n",
       "\tincome15k - 20k & 15.44791 & 1.280926 & 12.05995 & 4.650658e-29\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 5 × 4\n",
       "\n",
       "| <!--/--> | Estimate &lt;dbl&gt; | Std. Error &lt;dbl&gt; | t value &lt;dbl&gt; | Pr(&gt;|t|) &lt;dbl&gt; |\n",
       "|---|---|---|---|---|\n",
       "| income80k - 120k | 37.31599 | 1.654850 | 22.54947 | 6.160552e-75 |\n",
       "| income50k - 80k | 28.54698 | 1.738290 | 16.42245 | 2.033722e-47 |\n",
       "| income20k - 50k | 20.83961 | 1.582510 | 13.16870 | 1.604206e-33 |\n",
       "| income200k above | 20.10990 | 1.378832 | 14.58474 | 1.902543e-39 |\n",
       "| income15k - 20k | 15.44791 | 1.280926 | 12.05995 | 4.650658e-29 |\n",
       "\n"
      ],
      "text/plain": [
       "                 Estimate Std. Error t value  Pr(>|t|)    \n",
       "income80k - 120k 37.31599 1.654850   22.54947 6.160552e-75\n",
       "income50k - 80k  28.54698 1.738290   16.42245 2.033722e-47\n",
       "income20k - 50k  20.83961 1.582510   13.16870 1.604206e-33\n",
       "income200k above 20.10990 1.378832   14.58474 1.902543e-39\n",
       "income15k - 20k  15.44791 1.280926   12.05995 4.650658e-29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# Load regression_train.csv\n",
    "regression_train <- read.csv(\"regression_train.csv\")\n",
    "lm.fit <- lm(happiness ~ ., data = regression_train)\n",
    "\n",
    "# Save the summary output to an object\n",
    "lm.fit_summary <- summary(lm.fit)\n",
    "\n",
    "# Access the coefficients table and convert to a data frame\n",
    "coefficients_table <- as.data.frame(lm.fit_summary$coefficients)\n",
    "\n",
    "# Significance level, alpha = 0.01\n",
    "alpha <- 0.01\n",
    "\n",
    "# Filter for significant variables first\n",
    "significant_coeffs <- coefficients_table[coefficients_table$`Pr(>|t|)` < alpha, ]\n",
    "\n",
    "# Print predictors associated with target variable\n",
    "significant_coeffs\n",
    "\n",
    "# Sort these significant variables by highest estimate first\n",
    "significant_and_sorted <- significant_coeffs[order(significant_coeffs$Estimate, decreasing = TRUE), ]\n",
    "\n",
    "# List top 5 strongest predictors\n",
    "head(significant_and_sorted,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "royal-finland"
   },
   "source": [
    "## Question 2 (2 Mark)\n",
    "[**R squared**](https://en.wikipedia.org/wiki/Coefficient_of_determination) from the summary table reflects that the full model doesn't fit the training dataset well; thus, you try to quantify the error between the values of the ground-truth and those of the model prediction. You want to write a function to predict 'happiness' with the given dataset and calculate the [root mean squared error (rMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the model predictions and the ground truths. Please test this function on the full model and the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:  6.672557"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# Function to predict and calculate error metrics for a given model and dataset\n",
    "calculate_RMSE <- function(model, dataset, ground_truth_col_name) {\n",
    "  # Get ground truth values\n",
    "  ground_truth <- dataset[[ground_truth_col_name]] # Using [[ ]] for column name as string\n",
    "\n",
    "  # Get model predictions\n",
    "  predictions <- predict(model, newdata = dataset)\n",
    "\n",
    "  # Calculate residuals\n",
    "  residuals <- ground_truth - predictions\n",
    "\n",
    "  # Calculate error metrics\n",
    "  rmse <- sqrt(mean(residuals^2))\n",
    "  # Return result\n",
    "  return(rmse)\n",
    "}\n",
    "\n",
    "#Also define name of target ground truth column name\n",
    "target <- \"happiness\"\n",
    "multlin_model_error <- calculate_RMSE(lm.fit, regression_train, target)\n",
    "cat(\"RMSE: \", multlin_model_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eight-newman"
   },
   "source": [
    "## Question 3 (2 Marks)\n",
    "You find the full model complicated and try to reduce the complexity by performing [bidirectional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) with [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "Calculate the **rMSE** of this new model with the function that you implemented previously. Is there anything you find unusual? Explain your findings in 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = happiness ~ income + alwaysStressed + alwaysHaveFun + \n",
       "    alwaysSerious + alwaysDepressed + iFindMostThingsAmusing + \n",
       "    iUsuallyHaveAGoodInfluenceOnEvents, data = regression_train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-33.365  -4.587  -0.030   5.203  18.888 \n",
       "\n",
       "Coefficients:\n",
       "                                   Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)                        -13.7027     0.6348 -21.587  < 2e-16 ***\n",
       "income10k - 15k                      6.9401     1.2123   5.725 1.82e-08 ***\n",
       "income120k - 150k                   12.4102     1.1859  10.465  < 2e-16 ***\n",
       "income150k - 200k                   12.1301     1.1885  10.206  < 2e-16 ***\n",
       "income15k - 20k                     14.0070     1.1820  11.851  < 2e-16 ***\n",
       "income200k above                    20.6292     1.3292  15.519  < 2e-16 ***\n",
       "income20k - 50k                     22.1535     1.4718  15.052  < 2e-16 ***\n",
       "income50k - 80k                     29.0212     1.6938  17.133  < 2e-16 ***\n",
       "income80k - 120k                    37.5672     1.5353  24.469  < 2e-16 ***\n",
       "alwaysStressed                      -1.7214     0.3071  -5.606 3.49e-08 ***\n",
       "alwaysHaveFun                        0.9188     0.2946   3.119 0.001923 ** \n",
       "alwaysSerious                       -0.7589     0.2981  -2.546 0.011203 *  \n",
       "alwaysDepressed                     -0.9029     0.3651  -2.473 0.013749 *  \n",
       "iFindMostThingsAmusing               0.9958     0.2996   3.324 0.000954 ***\n",
       "iUsuallyHaveAGoodInfluenceOnEvents   1.0721     0.3110   3.448 0.000614 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 7.443 on 485 degrees of freedom\n",
       "Multiple R-squared:  0.7206,\tAdjusted R-squared:  0.7126 \n",
       "F-statistic: 89.36 on 14 and 485 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- BIC Stepwise Model Information ---\n",
      "Number of predictors (excluding intercept): 14\n",
      "BIC for selected model: 3510.389\n",
      "RMSE for selected model: 7.330305\n",
      "\n",
      "--- Full Model Information ---\n",
      "Number of predictors (excluding intercept): 68\n",
      "BIC for full model: 3751.964\n",
      "RMSE for selected model: 6.672557\n",
      "\n",
      "The final model is noticeably less complex, but only has marginally worse performance.\n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# Perform bidirectional stepwise regression using BIC\n",
    "#\n",
    "# Arguments for step():\n",
    "#   object: The initial model.\n",
    "#   scope: Defines the range of models to consider.\n",
    "#          list(lower = ~1) means the smallest model is just an intercept.\n",
    "#          list(upper = ~.) means the largest model is the full model (all predictors).\n",
    "#   direction: \"both\" for bidirectional stepwise selection.\n",
    "#   k: The penalty per parameter. For BIC, k = log(n), where n is the number of observations.\n",
    "#\n",
    "# Important: Ensure 'variable' is your response variable and 'regression_train' is your data.\n",
    "\n",
    "n_obs <- nrow(regression_train) # Number of observations in your training data\n",
    "bic_penalty <- log(n_obs)\n",
    "\n",
    "bic_step_model <- step(\n",
    "  object = lm.fit,\n",
    "  scope = list(lower = ~1, upper = ~.),\n",
    "  direction = \"both\",\n",
    "  k = bic_penalty,\n",
    "  trace = 0 # suppress verbose output\n",
    ")\n",
    "\n",
    "# View the summary of the selected model\n",
    "summary(bic_step_model)\n",
    "\n",
    "# Compare the final model to your initial full model\n",
    "cat(\"\\n--- BIC Stepwise Model Information ---\")\n",
    "cat(\"\\nNumber of predictors (excluding intercept):\", length(coef(bic_step_model)) - 1)\n",
    "cat(\"\\nBIC for selected model:\", BIC(bic_step_model))\n",
    "cat(\"\\nRMSE for selected model:\", calculate_RMSE(bic_step_model, regression_train, target))\n",
    "\n",
    "cat(\"\\n\\n--- Full Model Information ---\")\n",
    "cat(\"\\nNumber of predictors (excluding intercept):\", length(coef(lm.fit)) - 1)\n",
    "cat(\"\\nBIC for full model:\", BIC(lm.fit))\n",
    "cat(\"\\nRMSE for selected model:\", calculate_RMSE(lm.fit, regression_train, target))\n",
    "cat(\"\\n\")\n",
    "\n",
    "cat(\"\\nThe final model is noticeably less complex, but only has marginally worse performance.\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "latter-translation"
   },
   "source": [
    "## Question 4 (2 Mark)\n",
    "Although stepwise regression has reduced the model complexity significantly, the model still contains a lot of variables that we want to remove. Therefore, you are interested in lightweight linear regression models with ONLY TWO predictors. Write a script to automatically find the best lightweight model which corresponds to the model with the least **rMSE** on the training dataset. Compare the **rMSE** of the best lightweight model with the **rMSE** of the full model - ```lm.fit``` - that you built previously. Give an explanation for these results based on consideration of the predictors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best two predictor model results: \n",
      "Formula: happiness ~ income + alwaysStressed \n",
      "\n",
      "--- Comparison of RMSEs --- \n",
      "RMSE of two predictor model: 7.885411 \n",
      "RMSE of full model: 6.672557 \n",
      "\n",
      "--- Explanation of Results --- \n",
      "The results show that despite losing nearly all of the predictors initially available, the RMSE of the two predictor model is barely lower than the full model.\n",
      "However, the full model does perform better.\n",
      "This is generally expected, as:\n",
      "1. Flexibility: The full model has a much higher number of parameters (coefficients) and thus greater flexibility to 'learn' and even memorize the training data. With 50+ predictors, it can capture intricate relationships that a two-predictor model simply cannot.\n",
      "2. Overfitting to Training Data: Despite the lower RMSE, the model might simply memorize the data rather than learning the real relationships, which can lead to poor performance on new, unseen data.\n",
      "3. Trade-off: The lightweight model sacrifices some fit to the training data (resulting in a higher RMSE) in favor of simplicity and interpretability. The benefit is that the model is less prone to overfitting and can often generalize better to new data, especially if many of the 50+ predictors in the full model were not truly strong predictors or were highly correlated.\n",
      "4. Information Content: The 'best' two predictors found by this search are those that, in combination, capture the most predictive power for ' happiness ' on their own, minimizing the error. The full model potentially includes many predictors that contribute very little or redundant information, making it overly complex without significant gains in actual predictive accuracy on independent data.\n",
      "\n",
      "To truly determine which model is 'best', you would need to evaluate both the full model and the best lightweight model on an independent test dataset or use cross-validation techniques. \n"
     ]
    }
   ],
   "source": [
    "# ANSWER BLOCK\n",
    "# Find the best two predictor model\n",
    "# Get the names of all potential predictor variables from the training data\n",
    "predictor_names <- names(regression_train)[names(regression_train) != target]\n",
    "\n",
    "# Generate all unique combinations of two predictor names\n",
    "# combn(x, m) generates all combinations of m elements from x\n",
    "two_predictor_combinations <- combn(predictor_names, 2, simplify = FALSE)\n",
    "\n",
    "# Initialize variables to store the best model's information\n",
    "min_rmse_two_predictors <- Inf\n",
    "best_two_predictor_model_formula <- NULL\n",
    "best_two_predictor_model_object <- NULL # To store the actual lm object\n",
    "\n",
    "# Loop through each combination of two predictors\n",
    "for (i in seq_along(two_predictor_combinations)) {\n",
    "  pair <- two_predictor_combinations[[i]]\n",
    "\n",
    "  # Construct the model formula string for the current pair\n",
    "  formula_str <- paste(target, \"~\", paste(pair, collapse = \" + \"))\n",
    "  model_formula <- as.formula(formula_str)\n",
    "\n",
    "  # Fit the linear model using only these two predictors\n",
    "  current_model <- lm(model_formula, data = regression_train)\n",
    "\n",
    "  # Calculate RMSE for the current model on the training data\n",
    "  current_rmse <- calculate_RMSE(current_model, regression_train, target)\n",
    "\n",
    "  # Check if this model has a lower RMSE than the current best\n",
    "  if (current_rmse < min_rmse_two_predictors) {\n",
    "    min_rmse_two_predictors <- current_rmse\n",
    "    best_two_predictor_model_formula <- formula_str\n",
    "    best_two_predictor_model_object <- current_model # Store the model object\n",
    "  }\n",
    "}\n",
    "# Display results\n",
    "cat(\"Best two predictor model results:\", \"\\n\")\n",
    "cat(\"Formula:\", best_two_predictor_model_formula, \"\\n\")\n",
    "\n",
    "# Comparison and explanation\n",
    "cat(\"\\n--- Comparison of RMSEs ---\", \"\\n\")\n",
    "cat(\"RMSE of two predictor model:\", min_rmse_two_predictors, \"\\n\")\n",
    "cat(\"RMSE of full model:\", calculate_RMSE(lm.fit, regression_train, target), \"\\n\")\n",
    "\n",
    "cat(\"\\n--- Explanation of Results ---\", \"\\n\")\n",
    "cat(\"The results show that despite losing nearly all of the predictors initially available, the RMSE of the two predictor model is barely lower than the full model.\\n\")\n",
    "cat(\"However, the full model does perform better.\\n\")\n",
    "cat(\"This is generally expected, as:\\n\")\n",
    "cat(\"1. Flexibility: The full model has a much higher number of parameters (coefficients) and thus greater flexibility to 'learn' and even memorize the training data. With 50+ predictors, it can capture intricate relationships that a two-predictor model simply cannot.\\n\")\n",
    "cat(\"2. Overfitting to Training Data: Despite the lower RMSE, the model might simply memorize the data rather than learning the real relationships, which can lead to poor performance on new, unseen data.\\n\")\n",
    "cat(\"3. Trade-off: The lightweight model sacrifices some fit to the training data (resulting in a higher RMSE) in favor of simplicity and interpretability. The benefit is that the model is less prone to overfitting and can often generalize better to new data, especially if many of the 50+ predictors in the full model were not truly strong predictors or were highly correlated.\\n\")\n",
    "cat(\"4. Information Content: The 'best' two predictors found by this search are those that, in combination, capture the most predictive power for '\", target, \"' on their own, minimizing the error. The full model potentially includes many predictors that contribute very little or redundant information, making it overly complex without significant gains in actual predictive accuracy on independent data.\\n\\n\")\n",
    "cat(\"To truly determine which model is 'best', you would need to evaluate both the full model and the best lightweight model on an independent test dataset or use cross-validation techniques. \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4P9QL6tg7ix"
   },
   "source": [
    "### ANSWER (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preceding-chicago"
   },
   "source": [
    "## Question 5 (Libraries are allowed) (40 Marks)\n",
    "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limited to the aforementioned steps in this assignment. To simulate for a realistic modelling process, this question will be in the form of a [Kaggle competition](https://www.kaggle.com/t/e2b457fe991d4093af8c0ee067d69ac1) among students to find out who has the best model.\n",
    "\n",
    "Thus, you **will be graded** by the **rMSE** performance of your model, the better your model, the higher your score. Additionally, you need to describe/document your thought process in this model building process, this is akin to showing your working properly for the mathematic sections. If you don't clearly document the reasonings behind the model you use, we will have to make some deductions on your scores.\n",
    "\n",
    "This is the [video tutorial](https://www.youtube.com/watch?v=rkXc25Uvyl4) on how to join any Kaggle competition. \n",
    "\n",
    "When you optimize your model's performance, you can use any supervised model that you know and feature selection might be a big help as well. [Check the non-exhaustive set of R functions relevant to this unit](https://learning.monash.edu/mod/resource/view.php?id=2017193) for ideas for different models to try.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "```install.packages(\"some package\", repos='http://cran.us.r-project.org')```\n",
    "\n",
    "```library(\"some package\")```\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction. Our suggestion is for you to use the standard ```R version 3.6.1```\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using all 74 features (Lasso selection skipped).\n",
      "\n",
      "\n",
      "--- Training Support Vector Regression (SVR) Model (Base Learner) ---\n",
      "\n",
      "SVR Model Training Complete.\n",
      "\n",
      "\n",
      "SVR Best Tune:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sigma        C\n",
      "1 0.00350933 4.633967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVR CV RMSE: 7.87900957273786\n",
      "\n",
      "\n",
      "\n",
      "--- Training Random Forest (RF) Model (Base Learner) ---\n",
      "\n",
      "Random Forest Model Training Complete.\n",
      "\n",
      "\n",
      "RF Best Tune:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mtry\n",
      "1   49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF CV RMSE: 7.42358321769882\n",
      "\n",
      "\n",
      "\n",
      "--- Training XGBoost (xgbTree) Model (Base Learner) ---\n",
      "\n",
      "XGBoost Model Training Complete.\n",
      "\n",
      "\n",
      "XGBoost Best Tune:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nrounds max_depth        eta    gamma colsample_bytree min_child_weight\n",
      "1     920         3 0.03418591 1.730012        0.5431362                6\n",
      "  subsample\n",
      "1 0.4138567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoost CV RMSE: 7.25263136169887\n",
      "\n",
      "\n",
      "\n",
      "--- Training Meta-Model (Method: 'svmRadial') ---\n",
      "\n",
      "Meta-Model Training Complete.\n",
      "\n",
      "\n",
      "Meta-Model (svmRadial) Best Tune:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sigma        C\n",
      "1 0.00409517 32.10153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta-Model (svmRadial) CV RMSE: 7.16824047392266\n",
      "\n",
      "\n",
      "\n",
      "--- Stacked Ensemble Performance (Meta-Model Cross-Validation Results) ---\n",
      "\n",
      "Stacked Ensemble MAE (CV): 5.65688104967063\n",
      "\n",
      "Stacked Ensemble RMSE (CV): 7.16824047392266\n",
      "\n",
      "Stacked Ensemble R-squared (CV): 0.736700070143591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries (ensure you have these installed)\n",
    "library(e1071)      # For Support Vector Machines (svmRadial method)\n",
    "library(caret)      # For streamlined model training, CV, and tuning\n",
    "library(dplyr)      # For data manipulation\n",
    "library(forcats)    # For factor handling\n",
    "library(Matrix)     # Used by dummyVars for efficient sparse matrix creation\n",
    "library(glmnet)     # For glmnet meta-model option \n",
    "library(randomForest) # For Random Forest base model (and rf meta-model option)\n",
    "library(xgboost)    # For XGBoost base model\n",
    "library(doParallel) # Parallel processing\n",
    "stopImplicitCluster() # Clears any previously lingering clusters\n",
    "registerDoParallel(cores = detectCores() - 1) # Use all but one core\n",
    "\n",
    "# --- Data Pre-processing Configuration (Keep as is) ---\n",
    "ordinal_vars <- c(\n",
    "  \"whatIsYourHeightExpressItAsANumberInMetresM\",\n",
    "  \"howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\",\n",
    "  \"howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\",\n",
    "  \"doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\"\n",
    ")\n",
    "\n",
    "multi_level_categorical_vars <- c(\n",
    "  \"doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded\",\n",
    "  \"income\"\n",
    ")\n",
    "\n",
    "custom_ordinal_orders <- list(\n",
    "  \"income\" =\n",
    "    c(\"0 - 10k\",\"10k - 15k\", \"15k - 20k\", \"20k - 50k\", \"50k - 80k\", \"80k - 120k\", \"120k - 150k\", \"150k - 200k\", \"200k above\"),\n",
    "  \"howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\" =\n",
    "    c(\"No\", \"Rarely\", \"At least once a month\",\"At least once a week\", \"More than three times a week\"),\n",
    "  \"howOftenDoYouFeelSocialallyConnectedWithYourPeersAndFriends\" =\n",
    "    c(\"Never\", \"Rarely\", \"Sometimes\", \"Always\"),\n",
    "  \"doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\" =\n",
    "    c(\"Never\", \"Rarely\", \"Sometimes\", \"Always\")\n",
    ")\n",
    "\n",
    "# --- Global Objects for Data Preparation (Learned from Training Data) ---\n",
    "dummy_spec <- NULL\n",
    "ohe_feature_names <- NULL \n",
    "\n",
    "# --- Comprehensive Data Preparation Function ---\n",
    "factor_convert_data <- function(data) {\n",
    "  for (var in ordinal_vars) {\n",
    "    if (var %in% base::names(custom_ordinal_orders)) {\n",
    "      data[[var]] <- base::factor(data[[var]],\n",
    "                            levels = custom_ordinal_orders[[var]],\n",
    "                            ordered = TRUE)\n",
    "    } else {\n",
    "      data[[var]] <- base::factor(data[[var]], ordered = TRUE)\n",
    "    }\n",
    "  }\n",
    "  for (var in multi_level_categorical_vars) {\n",
    "    if (var %in% base::names(custom_ordinal_orders)) {\n",
    "        data[[var]] <- base::factor(data[[var]],\n",
    "                            levels = custom_ordinal_orders[[var]],\n",
    "                            ordered = FALSE)\n",
    "    } else {\n",
    "      data[[var]] <- base::as.factor(data[[var]])\n",
    "    }\n",
    "  }\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "prepare_data_for_model <- function(data, is_training = TRUE, target_variable_name = \"happiness\") {\n",
    "\n",
    "  data_fe <- factor_convert_data(data)\n",
    "\n",
    "  if (is_training) {\n",
    "    if (base::any(base::is.na(data_fe))) {\n",
    "      na_count <- base::sum(base::is.na(data_fe))\n",
    "      cols_with_na <- base::names(data_fe)[base::colSums(base::is.na(data_fe)) > 0]\n",
    "      base::stop(\n",
    "        paste0(\"Error: Found \", na_count, \" NA values in training data after initial preprocessing. \",\n",
    "               \"Columns with NAs: \", paste(cols_with_na, collapse = \", \"),\n",
    "               \". Please clean your raw data or update 'custom_ordinal_orders'.\")\n",
    "      )\n",
    "    }\n",
    "    # base::message(\"Training data is clean (no NAs) after initial preprocessing.\")\n",
    "  } else {\n",
    "    if (base::any(base::is.na(data_fe))) {\n",
    "      na_count_test <- base::sum(base::is.na(data_fe))\n",
    "      base::warning(paste0(\"Warning: Found \", na_count_test, \" NA values in test data after initial preprocessing. These will be handled by predict.dummyVars or may cause issues.\"))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  predictors_for_ohe_current <- data_fe\n",
    "  if (target_variable_name %in% base::colnames(predictors_for_ohe_current)) {\n",
    "    predictors_for_ohe_current <- predictors_for_ohe_current %>% dplyr::select(-dplyr::all_of(target_variable_name))\n",
    "  }\n",
    "  if (\"RowIndex\" %in% base::colnames(predictors_for_ohe_current)) {\n",
    "    predictors_for_ohe_current <- predictors_for_ohe_current %>% dplyr::select(-RowIndex)\n",
    "  }\n",
    "\n",
    "  # base::message(\"\\nPerforming One-Hot Encoding...\")\n",
    "  # Explicitly declare local_data_processed_x to ensure it's always in scope\n",
    "  local_data_processed_x <- NULL\n",
    "  if (is_training) {\n",
    "    assign(\"dummy_spec\", caret::dummyVars(~ ., data = predictors_for_ohe_current, contrasts = FALSE), envir = .GlobalEnv)\n",
    "    local_data_processed_x <- base::as.data.frame(stats::predict(dummy_spec, newdata = predictors_for_ohe_current))\n",
    "  } else {\n",
    "    if (base::is.null(dummy_spec)) {\n",
    "      base::stop(\"Error: 'dummy_spec' not found. Run training data preparation first to fit dummyVars.\")\n",
    "    }\n",
    "    local_data_processed_x <- base::as.data.frame(stats::predict(dummy_spec, newdata = predictors_for_ohe_current))\n",
    "  }\n",
    "  # base::message(\"One-Hot Encoding Complete.\")\n",
    "\n",
    "  # Explicitly declare local_data_processed to ensure it's always in scope\n",
    "  local_data_processed <- NULL\n",
    "  if (is_training) {\n",
    "    local_data_processed <- base::cbind(local_data_processed_x, data_fe[[target_variable_name]])\n",
    "    base::colnames(local_data_processed)[base::ncol(local_data_processed)] <- target_variable_name\n",
    "\n",
    "    assign(\"ohe_feature_names\", colnames(local_data_processed_x), envir = .GlobalEnv)\n",
    "\n",
    "  } else {\n",
    "    # For test data, local_data_processed is just the OHE features\n",
    "    local_data_processed <- local_data_processed_x\n",
    "    if (base::is.null(ohe_feature_names)) {\n",
    "      base::stop(\"Error: 'ohe_feature_names' not found. Run training data preparation first to determine features.\")\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # base::message(\"\\nSubsetting data to include only selected features...\")\n",
    "  if (is_training) {\n",
    "    final_data <- local_data_processed %>%\n",
    "      dplyr::select(dplyr::all_of(ohe_feature_names), dplyr::all_of(target_variable_name))\n",
    "  } else {\n",
    "    missing_features_in_test <- ohe_feature_names[!ohe_feature_names %in% colnames(local_data_processed)]\n",
    "    if (length(missing_features_in_test) > 0) {\n",
    "      base::warning(paste0(\"Warning: The following features are missing in the test data after OHE: \",\n",
    "                           paste(missing_features_in_test, collapse = \", \"),\n",
    "                           \". dplyr::select will drop them.\"))\n",
    "    }\n",
    "    final_data <- local_data_processed %>%\n",
    "      dplyr::select(dplyr::all_of(ohe_feature_names))\n",
    "  }\n",
    "  return(final_data)\n",
    "}\n",
    "\n",
    "# --- MAIN SCRIPT EXECUTION ---\n",
    "\n",
    "# Load training data\n",
    "regression_train <- read.csv(\"regression_train.csv\")\n",
    "\n",
    "# Prepare TRAINING Data\n",
    "# base::message(\"\\n--- Preparing Training Data ---\")\n",
    "regression_train_fe_selected <- prepare_data_for_model(regression_train, is_training = TRUE, target_variable_name = \"happiness\")\n",
    "\n",
    "# Define common formula for base models\n",
    "quoted_selected_features <- paste0(\"`\", ohe_feature_names, \"`\")\n",
    "model_formula_selected <- stats::as.formula(paste(\"happiness ~\", paste(quoted_selected_features, collapse = \" + \")))\n",
    "\n",
    "# --- Base Model Training Controls ---\n",
    "# Same trainControl for each base model\n",
    "train_control <- caret::trainControl(\n",
    "  method = \"repeatedcv\",\n",
    "  number = 10,\n",
    "  repeats = 3,\n",
    "  verboseIter = FALSE,\n",
    "  search = \"random\",\n",
    "  savePredictions = \"final\",\n",
    "  allowParallel = TRUE\n",
    ")\n",
    "tune_length <- 100\n",
    "\n",
    "# --- Train SVR Model (Base Learner 1) ---\n",
    "base::set.seed(42)\n",
    "base::message(\"\\n--- Training Support Vector Regression (SVR) Model (Base Learner) ---\")\n",
    "invisible(capture.output(\n",
    "  svr_model <- caret::train(\n",
    "    model_formula_selected,\n",
    "    data = regression_train_fe_selected,\n",
    "    method = \"svmRadial\",\n",
    "    trControl = train_control,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(C = 4.633967, sigma = 0.00350933), # Hard-coded values found from random search\n",
    "    preProcess = c(\"center\", \"scale\")\n",
    "  )\n",
    "))\n",
    "base::message(\"SVR Model Training Complete.\\n\")\n",
    "base::message(\"SVR Best Tune:\\n\")\n",
    "base::print(svr_model$bestTune)\n",
    "base::message(\"SVR CV RMSE: \", svr_model$results[which.min(svr_model$results$RMSE), ]$RMSE, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Train Random Forest Model (Base Learner 2) ---\n",
    "base::set.seed(42)\n",
    "base::message(\"\\n--- Training Random Forest (RF) Model (Base Learner) ---\")\n",
    "invisible(capture.output(\n",
    "  rf_model <- caret::train(\n",
    "    model_formula_selected,\n",
    "    data = regression_train_fe_selected,\n",
    "    method = \"rf\",\n",
    "    trControl = train_control,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(mtry = 49), # Hard-coded values found from random search\n",
    "    preProcess = c(\"center\", \"scale\")\n",
    "  )\n",
    "))\n",
    "base::message(\"Random Forest Model Training Complete.\\n\")\n",
    "base::message(\"RF Best Tune:\\n\")\n",
    "base::print(rf_model$bestTune)\n",
    "base::message(\"RF CV RMSE: \", rf_model$results[which.min(rf_model$results$RMSE), ]$RMSE, \"\\n\")\n",
    "\n",
    "\n",
    "# --- Train XGBoost Model (Base Learner 3) ---\n",
    "base::set.seed(42)\n",
    "base::message(\"\\n--- Training XGBoost (xgbTree) Model (Base Learner) ---\")\n",
    "invisible(capture.output(\n",
    "  xgb_model <- caret::train(\n",
    "    model_formula_selected,\n",
    "    data = regression_train_fe_selected,\n",
    "    method = \"xgbTree\",\n",
    "    trControl = train_control,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(\n",
    "      nrounds = 920,\n",
    "      max_depth = 3,\n",
    "      eta = 0.03418591,\n",
    "      gamma = 1.730012,\n",
    "      colsample_bytree = 0.5431362,\n",
    "      min_child_weight = 6,\n",
    "      subsample = 0.4138567\n",
    "    ), # Hard-coded values found from random search\n",
    "    preProcess = c(\"center\", \"scale\")\n",
    "  )\n",
    "))\n",
    "base::message(\"XGBoost Model Training Complete.\\n\")\n",
    "base::message(\"XGBoost Best Tune:\\n\")\n",
    "base::print(xgb_model$bestTune)\n",
    "base::message(\"XGBoost CV RMSE: \", xgb_model$results[which.min(xgb_model$results$RMSE), ]$RMSE, \"\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# --- Stacking Ensemble Setup ---\n",
    "# ==============================================================================\n",
    "# base::message(\"\\n--- Setting up Stacking Ensemble ---\")\n",
    "\n",
    "# Get out-of-fold predictions from SVR, Random Forest, and XGBoost models\n",
    "svr_oof_preds <- svr_model$pred %>%\n",
    "  dplyr::filter(sigma == svr_model$bestTune$sigma,\n",
    "                C == svr_model$bestTune$C) %>%\n",
    "  dplyr::group_by(rowIndex) %>%\n",
    "  dplyr::summarise(SVR_Prediction = mean(pred, na.rm = TRUE)) %>%\n",
    "  dplyr::ungroup() %>%\n",
    "  dplyr::arrange(rowIndex)\n",
    "\n",
    "rf_oof_preds <- rf_model$pred %>%\n",
    "  dplyr::filter(mtry == rf_model$bestTune$mtry) %>%\n",
    "  dplyr::group_by(rowIndex) %>%\n",
    "  dplyr::summarise(RF_Prediction = mean(pred, na.rm = TRUE)) %>%\n",
    "  dplyr::ungroup() %>%\n",
    "  dplyr::arrange(rowIndex)\n",
    "\n",
    "xgb_oof_preds <- xgb_model$pred %>%\n",
    "  dplyr::filter(nrounds == xgb_model$bestTune$nrounds,\n",
    "                max_depth == xgb_model$bestTune$max_depth,\n",
    "                eta == xgb_model$bestTune$eta,\n",
    "                gamma == xgb_model$bestTune$gamma,\n",
    "                colsample_bytree == xgb_model$bestTune$colsample_bytree,\n",
    "                min_child_weight == xgb_model$bestTune$min_child_weight,\n",
    "                subsample == xgb_model$bestTune$subsample) %>%\n",
    "  dplyr::group_by(rowIndex) %>%\n",
    "  dplyr::summarise(XGB_Prediction = mean(pred, na.rm = TRUE)) %>%\n",
    "  dplyr::ungroup() %>%\n",
    "  dplyr::arrange(rowIndex)\n",
    "\n",
    "# Combine the out-of-fold predictions with the true happiness values for the meta-model training.\n",
    "meta_training_data <- regression_train_fe_selected %>%\n",
    "  dplyr::select(happiness) %>%\n",
    "  dplyr::mutate(rowIndex = base::seq_along(happiness)) %>%\n",
    "  dplyr::arrange(rowIndex) %>%\n",
    "  dplyr::left_join(svr_oof_preds, by = \"rowIndex\") %>%\n",
    "  dplyr::left_join(rf_oof_preds, by = \"rowIndex\") %>%\n",
    "  dplyr::left_join(xgb_oof_preds, by = \"rowIndex\") %>%\n",
    "  dplyr::select(-rowIndex)\n",
    "\n",
    "# Check for NAs in meta_training_data\n",
    "if (any(is.na(meta_training_data))) {\n",
    "  stop(\"NA values found in meta_training_data after combining OOF predictions. This should not happen if base models completed successfully.\")\n",
    "}\n",
    "\n",
    "\n",
    "# --- Meta-Model Configuration ---\n",
    "# Choose your meta-model method here: \"lm\", \"glmnet\", \"svmRadial\", \"rf\"\n",
    "meta_model_method <- \"svmRadial\" # \"svmRadial\" found to be the best by trial and error\n",
    "\n",
    "base::set.seed(42)\n",
    "base::message(paste0(\"\\n--- Training Meta-Model (Method: '\", meta_model_method, \"') ---\"))\n",
    "\n",
    "train_control_meta_model <- caret::trainControl(\n",
    "  method = \"repeatedcv\",\n",
    "  number = 5,\n",
    "  repeats = 3,\n",
    "  verboseIter = FALSE,\n",
    "  search = \"random\"\n",
    ")\n",
    "tune_length_meta_model <- 20\n",
    "\n",
    "# Train the meta-model\n",
    "invisible(capture.output(\n",
    "  meta_model <- caret::train(\n",
    "    happiness ~ SVR_Prediction + RF_Prediction + XGB_Prediction,\n",
    "    data = meta_training_data,\n",
    "    method = meta_model_method,\n",
    "    trControl = train_control_meta_model,\n",
    "    # tuneLength = tune_length_meta_model, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(sigma = 0.00409517, C = 32.10153), # Hard-coded values found from random search\n",
    "    preProcess = if(meta_model_method %in% c(\"svmRadial\", \"glmnet\")) c(\"center\", \"scale\") else NULL\n",
    "  )\n",
    "))\n",
    "base::message(\"Meta-Model Training Complete.\\n\")\n",
    "base::message(paste0(\"Meta-Model (\", meta_model_method, \") Best Tune:\\n\"))\n",
    "base::print(meta_model$bestTune)\n",
    "base::message(paste0(\"Meta-Model (\", meta_model_method, \") CV RMSE: \", meta_model$results[which.min(meta_model$results$RMSE), ]$RMSE, \"\\n\"))\n",
    "\n",
    "\n",
    "# --- Evaluate Stacked Ensemble (Using Meta-Model's CV Results) ---\n",
    "best_meta_tune <- meta_model$results[which.min(meta_model$results$RMSE), ]\n",
    "\n",
    "base::message(\"\\n--- Stacked Ensemble Performance (Meta-Model Cross-Validation Results) ---\")\n",
    "base::message(\"Stacked Ensemble MAE (CV): \", best_meta_tune$MAE)\n",
    "base::message(\"Stacked Ensemble RMSE (CV): \", best_meta_tune$RMSE)\n",
    "base::message(\"Stacked Ensemble R-squared (CV): \", best_meta_tune$Rsquared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x-H3d66g7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Build your final model here, use additional coding blocks if you need to\n",
    "fin.mod <- meta_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0SYx-zYg7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the test data.\n",
    "test <- read.csv(\"regression_test.csv\")\n",
    "\n",
    "# Test data preprocessing\n",
    "test <- prepare_data_for_model(test, is_training = FALSE, target_variable_name = \"happiness\")\n",
    "\n",
    "# Make predictions from each base model on the preprocessed test data\n",
    "svr_test_preds <- predict(svr_model, newdata = test)\n",
    "rf_test_preds <- predict(rf_model, newdata = test)\n",
    "xgb_test_preds <- predict(xgb_model, newdata = test)\n",
    "\n",
    "# Create a dataframe of these test predictions for the meta-model\n",
    "meta_test_data <- data.frame(\n",
    "  SVR_Prediction = svr_test_preds,\n",
    "  RF_Prediction = rf_test_preds,\n",
    "  XGB_Prediction = xgb_test_preds\n",
    ")\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, meta_test_data)\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "write.csv(\n",
    "    data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "    \"RegressionPredictLabel.csv\", \n",
    "    row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gross-disaster",
    "outputId": "ac8b1b5a-ad80-42f7-824a-61432e4d64cf",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "tryCatch(\n",
    "    {\n",
    "        source(\"../supplimentary.R\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        source(\"supplimentary.R\")\n",
    "    }\n",
    ")\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../regression_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"regression_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "RMSE.fin <- rmse(pred.label, truths$x)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_u6Dyj1xehk"
   },
   "source": [
    "# Part 2 Classification (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual, but this time we want to predict a categorical score for perfect mental health, rather than a continuous score. You need to build 5-class classification models to optimally predict the variable in the survey dataset called 'perfectMentalHealth' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```classification_train.csv``` and ```classification_test.csv```. Using these datasets, you hope to build a model that can predict 'perfectMentalHealth' using the other variables. ```classification_train.csv``` comes with the ground-truth target label (i.e. 'perfectMentalHealth' happiness classes) whereas `classification_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict 'perfectMentalHealth'. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. E.g. the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'\n",
    "\n",
    "This question will also be in the form of a [Kaggle competition](https://www.kaggle.com/t/15989e8eefe246b89193a219173909ba) among students to find out who has the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8x-H3d66g7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Training Data ---\n",
      "\n",
      "\n",
      "Performing One-Hot Encoding...\n",
      "\n",
      "\n",
      "Using all 74 features (no feature selection)\n",
      "\n",
      "\n",
      "Subsetting data to include all features...\n",
      "\n",
      "\n",
      "--- Training Support Vector Classifier (SVC) Model ---\n",
      "\n",
      "SVC Model Training Complete.\n",
      "\n",
      "\n",
      "SVC Best Tune:\n",
      "\n",
      "  sigma = 0.003737714 C = 4.497931\n",
      "\n",
      "SVC CV Macro-F1: 0.365657456291259\n",
      "\n",
      "\n",
      "--- Training Random Forest Model ---\n",
      "\n",
      "Random Forest Model Training Complete.\n",
      "\n",
      "\n",
      "RF Best Tune:\n",
      "\n",
      "  mtry = 41\n",
      "\n",
      "RF CV Macro-F1: 0.391326370782509\n",
      "\n",
      "\n",
      "--- Training XGBoost Model ---\n",
      "\n",
      "XGBoost Model Training Complete.\n",
      "\n",
      "\n",
      "XGB Best Tune:\n",
      "\n",
      "  nrounds = 634 max_depth = 8 eta = 0.2140439 gamma = 3.170534 colsample_bytree = 0.4413402 min_child_weight = 4 subsample = 0.8548148\n",
      "\n",
      "XGB CV Macro-F1: 0.379001875778174\n",
      "\n",
      "\n",
      "--- Setting up Stacking Ensemble ---\n",
      "\n",
      "\n",
      "--- Training Meta-Model (SVM Radial) ---\n",
      "\n",
      "Meta-Model Training Complete.\n",
      "\n",
      "\n",
      "Meta-Model Best Tune:\n",
      "\n",
      "  sigma = 0.08300235 C = 1\n",
      "\n",
      "Meta-Model CV Macro-F1: 0.4108672696126\n",
      "\n",
      "\n",
      "--- Model Performance Comparison ---\n",
      "\n",
      "SVC CV Macro-F1: 0.3657\n",
      "\n",
      "RF CV Macro-F1: 0.3913\n",
      "\n",
      "XGB CV Macro-F1: 0.379\n",
      "\n",
      "Stacked Ensemble CV Macro-F1: 0.4109\n",
      "\n",
      "\n",
      "Performing One-Hot Encoding...\n",
      "\n",
      "\n",
      "Subsetting data to include all features...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in the train and test classification data.\n",
    "train <- read.csv(\"classification_train.csv\")\n",
    "test <- read.csv(\"classification_test.csv\")\n",
    "\n",
    "# Essentially the same model as regression, just adapted to classification\n",
    "# Load necessary libraries\n",
    "library(e1071)\n",
    "library(caret)\n",
    "library(dplyr)\n",
    "library(forcats)\n",
    "library(Matrix)\n",
    "library(randomForest)\n",
    "library(xgboost)\n",
    "library(doParallel)\n",
    "stopImplicitCluster() # Clears any previously lingering clusters\n",
    "registerDoParallel(cores = detectCores() - 1)\n",
    "\n",
    "# --- Data Pre-processing Configuration ---\n",
    "ordinal_vars <- c(\n",
    "  \"whatIsYourHeightExpressItAsANumberInMetresM\",\n",
    "  \"howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\",\n",
    "  \"howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\",\n",
    "  \"doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\"\n",
    ")\n",
    "\n",
    "multi_level_categorical_vars <- c(\n",
    "  \"doYouHaveASupportSystemOfFriendsAndFamilyToTurnToWhenNeeded\",\n",
    "  \"income\"\n",
    ")\n",
    "\n",
    "custom_ordinal_orders <- list(\n",
    "  \"income\" =\n",
    "    c(\"0 - 10k\", \"10k - 15k\", \"15k - 20k\", \"20k - 50k\", \"50k - 80k\", \"80k - 120k\", \"120k - 150k\", \"150k - 200k\", \"200k above\"),\n",
    "  \"howOftenDoYouParticipateInSocialActivitiesIncludingClubsSportsV\" =\n",
    "    c(\"No\", \"Rarely\", \"At least once a month\", \"At least once a week\", \"More than three times a week\"),\n",
    "  \"howOftenDoYouFeelSociallyConnectedWithYourPeersAndFriends\" =\n",
    "    c(\"Never\", \"Rarely\", \"Sometimes\", \"Always\"),\n",
    "  \"doYouFeelComfortableEngagingInConversationsWithPeopleFromDiffer\" =\n",
    "    c(\"Never\", \"Rarely\", \"Sometimes\", \"Always\")\n",
    ")\n",
    "\n",
    "# --- Global Objects for Data Preparation ---\n",
    "dummy_spec <- NULL\n",
    "selected_features_all <- NULL\n",
    "\n",
    "# --- Data Preparation Function ---\n",
    "factor_convert_data <- function(data) {\n",
    "  for (var in ordinal_vars) {\n",
    "    if (var %in% names(custom_ordinal_orders)) {\n",
    "      data[[var]] <- factor(data[[var]],\n",
    "        levels = custom_ordinal_orders[[var]],\n",
    "        ordered = TRUE\n",
    "      )\n",
    "    } else {\n",
    "      data[[var]] <- factor(data[[var]], ordered = TRUE)\n",
    "    }\n",
    "  }\n",
    "  for (var in multi_level_categorical_vars) {\n",
    "    if (var %in% names(custom_ordinal_orders)) {\n",
    "      data[[var]] <- factor(data[[var]],\n",
    "        levels = custom_ordinal_orders[[var]],\n",
    "        ordered = FALSE\n",
    "      )\n",
    "    } else {\n",
    "      data[[var]] <- as.factor(data[[var]])\n",
    "    }\n",
    "  }\n",
    "  return(data)\n",
    "}\n",
    "\n",
    "prepare_data_for_model <- function(data, is_training = TRUE, target_variable_name = \"perfectMentalHealth\") {\n",
    "  data_fe <- factor_convert_data(data)\n",
    "\n",
    "  if (is_training) {\n",
    "    if (any(is.na(data_fe))) {\n",
    "      na_count <- sum(is.na(data_fe))\n",
    "      cols_with_na <- names(data_fe)[colSums(is.na(data_fe)) > 0]\n",
    "      stop(paste0(\n",
    "        \"Error: Found \", na_count, \" NA values in training data. \",\n",
    "        \"Columns with NAs: \", paste(cols_with_na, collapse = \", \")\n",
    "      ))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  predictors_for_ohe_current <- data_fe\n",
    "  if (target_variable_name %in% colnames(predictors_for_ohe_current)) {\n",
    "    predictors_for_ohe_current <- predictors_for_ohe_current %>% select(-all_of(target_variable_name))\n",
    "  }\n",
    "  if (\"RowIndex\" %in% colnames(predictors_for_ohe_current)) {\n",
    "    predictors_for_ohe_current <- predictors_for_ohe_current %>% select(-RowIndex)\n",
    "  }\n",
    "\n",
    "  message(\"\\nPerforming One-Hot Encoding...\")\n",
    "  local_data_processed_x <- NULL\n",
    "  if (is_training) {\n",
    "    assign(\"dummy_spec\", dummyVars(~., data = predictors_for_ohe_current, contrasts = FALSE), envir = .GlobalEnv)\n",
    "    local_data_processed_x <- as.data.frame(predict(dummy_spec, newdata = predictors_for_ohe_current))\n",
    "  } else {\n",
    "    if (is.null(dummy_spec)) stop(\"Run training data preparation first\")\n",
    "    local_data_processed_x <- as.data.frame(predict(dummy_spec, newdata = predictors_for_ohe_current))\n",
    "  }\n",
    "\n",
    "  local_data_processed <- NULL\n",
    "  if (is_training) {\n",
    "    local_data_processed <- cbind(local_data_processed_x, data_fe[[target_variable_name]])\n",
    "    colnames(local_data_processed)[ncol(local_data_processed)] <- target_variable_name\n",
    "\n",
    "    assign(\"selected_features_all\", colnames(local_data_processed_x), envir = .GlobalEnv)\n",
    "    message(paste0(\"\\nUsing all \", length(selected_features_all), \" features (no feature selection)\"))\n",
    "  } else {\n",
    "    local_data_processed <- local_data_processed_x\n",
    "    if (is.null(selected_features_all)) { # Add check for selected_features_all in test\n",
    "      stop(\"Error: 'selected_features_all' not found. Run training data preparation first to determine features.\")\n",
    "    }\n",
    "    # Ensure test data has the same columns as training data OHE features\n",
    "    missing_features_in_test <- selected_features_all[!selected_features_all %in% colnames(local_data_processed)]\n",
    "    if (length(missing_features_in_test) > 0) {\n",
    "      # Add missing columns as 0 to match training data\n",
    "      for (feature in missing_features_in_test) {\n",
    "        local_data_processed[[feature]] <- 0\n",
    "      }\n",
    "      warning(paste0(\"Warning: The following features were missing in the test data and added as zeroes: \",\n",
    "                           paste(missing_features_in_test, collapse = \", \")))\n",
    "    }\n",
    "    # Remove extra columns if any\n",
    "    extra_features_in_test <- colnames(local_data_processed)[!colnames(local_data_processed) %in% selected_features_all]\n",
    "    if (length(extra_features_in_test) > 0) {\n",
    "      local_data_processed <- local_data_processed %>% select(-all_of(extra_features_in_test))\n",
    "      warning(paste0(\"Warning: The following extra features were found in the test data and removed: \",\n",
    "                           paste(extra_features_in_test, collapse = \", \")))\n",
    "    }\n",
    "  }\n",
    "\n",
    "  message(\"\\nSubsetting data to include all features...\")\n",
    "  if (is_training) {\n",
    "    final_data <- local_data_processed %>%\n",
    "      select(all_of(selected_features_all), all_of(target_variable_name))\n",
    "  } else {\n",
    "    final_data <- local_data_processed %>%\n",
    "      select(all_of(selected_features_all))\n",
    "  }\n",
    "  return(final_data)\n",
    "}\n",
    "\n",
    "# --- Custom Macro-F1 Metric Function ---\n",
    "macroF1 <- function(data, lev = NULL, model = NULL) {\n",
    "  # Multiclass F1 with macro averaging\n",
    "  # Ensure the levels are in the correct order for confusionMatrix if 'lev' is provided\n",
    "  if (!is.null(lev)) {\n",
    "    data$pred <- factor(data$pred, levels = lev)\n",
    "    data$obs <- factor(data$obs, levels = lev)\n",
    "  }\n",
    "\n",
    "  cm <- confusionMatrix(data$pred, data$obs)\n",
    "  f1_scores <- cm$byClass[, \"F1\"]\n",
    "  macro_f1 <- mean(f1_scores, na.rm = TRUE)\n",
    "\n",
    "  # Return standard caret names\n",
    "  c(\n",
    "    F1 = macro_f1,\n",
    "    Accuracy = cm$overall[\"Accuracy\"]\n",
    "  )\n",
    "}\n",
    "\n",
    "# --- MAIN SCRIPT EXECUTION ---\n",
    "# Prepare TRAINING Data\n",
    "message(\"\\n--- Preparing Training Data ---\")\n",
    "classification_train_fe <- prepare_data_for_model(\n",
    "  train,\n",
    "  is_training = TRUE,\n",
    "  target_variable_name = \"perfectMentalHealth\"\n",
    ")\n",
    "\n",
    "# Convert target to factor with valid R variable names\n",
    "original_levels <- c(\"-2\", \"-1\", \"0\", \"1\", \"2\")\n",
    "valid_levels <- make.names(original_levels) # Converts to valid R names\n",
    "\n",
    "classification_train_fe$perfectMentalHealth <- factor(\n",
    "  classification_train_fe$perfectMentalHealth,\n",
    "  levels = -2:2, # Use original numeric order for levels\n",
    "  labels = valid_levels # Use valid R names for labels\n",
    ")\n",
    "\n",
    "# Store mapping for later conversion\n",
    "level_mapping <- data.frame(\n",
    "  valid = valid_levels,\n",
    "  original = original_levels\n",
    ")\n",
    "\n",
    "# Define common formula for base models\n",
    "quoted_features <- paste0(\"`\", selected_features_all, \"`\")\n",
    "model_formula <- as.formula(paste(\"perfectMentalHealth ~\", paste(quoted_features, collapse = \" + \")))\n",
    "\n",
    "# --- Base Model Training Controls ---\n",
    "# Increase number and repeats for more robust CV\n",
    "train_control_base <- trainControl(\n",
    "  method = \"repeatedcv\",\n",
    "  number = 10,\n",
    "  repeats = 5,\n",
    "  classProbs = TRUE, # For getting probabilities\n",
    "  summaryFunction = macroF1,\n",
    "  verboseIter = FALSE,\n",
    "  search = \"random\",\n",
    "  savePredictions = \"final\",\n",
    "  allowParallel = TRUE\n",
    ")\n",
    "tune_length <- 50 # Higher is better, but with higher repeated cross validation this comes with very long computing times\n",
    "# Calculate class weights for imbalance\n",
    "class_weights <- table(classification_train_fe$perfectMentalHealth)\n",
    "# Inverse of class frequency. Add a small value to avoid division by zero if a class has 0 observations\n",
    "class_weights_normalized <- (1 / class_weights) / sum(1 / class_weights)\n",
    "names(class_weights_normalized) <- levels(classification_train_fe$perfectMentalHealth)\n",
    "\n",
    "# --- Train SVC Model (Base Learner 1) ---\n",
    "set.seed(42)\n",
    "message(\"\\n--- Training Support Vector Classifier (SVC) Model ---\")\n",
    "invisible(capture.output( \n",
    "  svc_model <- train(\n",
    "    model_formula,\n",
    "    data = classification_train_fe,\n",
    "    method = \"svmRadial\",\n",
    "    trControl = train_control_base,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(C = 4.497931, sigma = 0.003737714), # Hard-coded values found from random search\n",
    "    metric = \"F1\",\n",
    "    maximize = TRUE,\n",
    "    weights = class_weights_normalized[classification_train_fe$perfectMentalHealth]\n",
    "  )\n",
    "))\n",
    "message(\"SVC Model Training Complete.\\n\")\n",
    "message(\"SVC Best Tune:\")\n",
    "message(paste(\"  sigma =\", svc_model$bestTune$sigma, \"C =\", svc_model$bestTune$C))\n",
    "message(\"SVC CV Macro-F1: \", max(svc_model$results$F1))\n",
    "\n",
    "\n",
    "# --- Train Random Forest Model (Base Learner 2) ---\n",
    "set.seed(42)\n",
    "message(\"\\n--- Training Random Forest Model ---\")\n",
    "invisible(capture.output(\n",
    "  rf_model <- train(\n",
    "    model_formula,\n",
    "    data = classification_train_fe,\n",
    "    method = \"rf\",\n",
    "    trControl = train_control_base,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(mtry = 41), # Hard-coded values found from random search\n",
    "    ntree = 500,\n",
    "    metric = \"F1\",\n",
    "    maximize = TRUE,\n",
    "    weights = class_weights_normalized[classification_train_fe$perfectMentalHealth]\n",
    "  )\n",
    "))\n",
    "message(\"Random Forest Model Training Complete.\\n\")\n",
    "message(\"RF Best Tune:\")\n",
    "message(paste(\"  mtry =\", rf_model$bestTune$mtry))\n",
    "message(\"RF CV Macro-F1: \", max(rf_model$results$F1))\n",
    "\n",
    "\n",
    "# --- Train XGBoost Model (Base Learner 3) ---\n",
    "set.seed(42)\n",
    "message(\"\\n--- Training XGBoost Model ---\")\n",
    "invisible(capture.output(\n",
    "  xgb_model <- train(\n",
    "    model_formula,\n",
    "    data = classification_train_fe,\n",
    "    method = \"xgbTree\",\n",
    "    trControl = train_control_base,\n",
    "    # tuneLength = tune_length, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(\n",
    "      nrounds = 634,\n",
    "      max_depth = 8,\n",
    "      eta = 0.2140439,\n",
    "      gamma = 3.170534,\n",
    "      colsample_bytree = 0.4413402,\n",
    "      min_child_weight = 4,\n",
    "      subsample = 0.8548148\n",
    "    ), # Hard-coded values found from random search\n",
    "    metric = \"F1\",\n",
    "    maximize = TRUE,\n",
    "    weights = class_weights_normalized[classification_train_fe$perfectMentalHealth]\n",
    "  )\n",
    "))\n",
    "message(\"XGBoost Model Training Complete.\\n\")\n",
    "message(\"XGB Best Tune:\")\n",
    "message(paste(\n",
    "  \"  nrounds =\", xgb_model$bestTune$nrounds,\n",
    "  \"max_depth =\", xgb_model$bestTune$max_depth,\n",
    "  \"eta =\", xgb_model$bestTune$eta,\n",
    "  \"gamma =\", xgb_model$bestTune$gamma,\n",
    "  \"colsample_bytree =\", xgb_model$bestTune$colsample_bytree,\n",
    "  \"min_child_weight =\", xgb_model$bestTune$min_child_weight,\n",
    "  \"subsample =\", xgb_model$bestTune$subsample\n",
    "))\n",
    "message(\"XGB CV Macro-F1: \", max(xgb_model$results$F1))\n",
    "\n",
    "\n",
    "# --- Stacking Ensemble Setup ---\n",
    "message(\"\\n--- Setting up Stacking Ensemble ---\")\n",
    "\n",
    "# Get the levels of the target variable from the training data\n",
    "target_levels <- levels(classification_train_fe$perfectMentalHealth)\n",
    "\n",
    "# Function to extract OOF probabilities for a given model\n",
    "# Simplified: relying on model$pred from savePredictions=\"final\" for best tune's predictions\n",
    "get_oof_probabilities <- function(model, target_levels, model_name) {\n",
    "  # model$pred already contains predictions for the best tune if savePredictions = \"final\"\n",
    "  # Group by rowIndex and average probabilities across repeated folds\n",
    "  oof_probs <- model$pred %>%\n",
    "    group_by(rowIndex) %>%\n",
    "    summarise(across(all_of(target_levels), mean, .names = paste0(model_name, \"_Prob_{.col}\")), .groups = 'drop') %>%\n",
    "    ungroup() %>%\n",
    "    arrange(rowIndex)\n",
    "\n",
    "  return(oof_probs)\n",
    "}\n",
    "\n",
    "# Extract OOF probabilities for each base model\n",
    "# Use model$pred for best tune's predictions\n",
    "svc_oof_probs <- get_oof_probabilities(svc_model, target_levels, \"SVC\")\n",
    "rf_oof_probs <- get_oof_probabilities(rf_model, target_levels, \"RF\")\n",
    "xgb_oof_probs <- get_oof_probabilities(xgb_model, target_levels, \"XGB\")\n",
    "\n",
    "# Combine the out-of-fold predictions with the true target values for the meta-model training.\n",
    "# Ensure 'rowIndex' for meta_training_data aligns with the OOF predictions.\n",
    "meta_training_data <- classification_train_fe %>%\n",
    "  select(perfectMentalHealth) %>%\n",
    "  mutate(rowIndex = row_number()) %>% # Add rowIndex for joining\n",
    "  arrange(rowIndex) %>%\n",
    "  left_join(svc_oof_probs, by = \"rowIndex\") %>%\n",
    "  left_join(rf_oof_probs, by = \"rowIndex\") %>%\n",
    "  left_join(xgb_oof_probs, by = \"rowIndex\") %>%\n",
    "  select(-rowIndex) # Remove index after joining\n",
    "\n",
    "# Check for NAs in meta_training_data (important after joining)\n",
    "if (any(is.na(meta_training_data))) {\n",
    "  na_summary <- colSums(is.na(meta_training_data))\n",
    "  warning(\"NA values found in meta_training_data after combining OOF probabilities. Columns with NAs:\", paste(names(na_summary[na_summary > 0]), collapse = \", \"))\n",
    "}\n",
    "\n",
    "# --- Meta-Model Training (SVM Radial) ---\n",
    "# Increase number and repeats for more robust CV\n",
    "train_control_meta <- trainControl(\n",
    "  method = \"repeatedcv\",\n",
    "  number = 10,\n",
    "  repeats = 5,\n",
    "  classProbs = TRUE,\n",
    "  summaryFunction = macroF1,\n",
    "  verboseIter = FALSE,\n",
    "  allowParallel = TRUE\n",
    ")\n",
    "tune_length_meta <- 20 # Meta model doesn't need as extensive tuning\n",
    "set.seed(42)\n",
    "message(\"\\n--- Training Meta-Model (SVM Radial) ---\")\n",
    "invisible(capture.output(\n",
    "  meta_model <- train(\n",
    "    perfectMentalHealth ~ ., # Use all probability columns as predictors\n",
    "    data = meta_training_data,\n",
    "    method = \"svmRadial\",\n",
    "    trControl = train_control_meta,\n",
    "    # tuneLength = tune_length_meta, # Random search for best parameters\n",
    "    tuneGrid = expand.grid(sigma = 0.08300235, C = 1), # Hard-coded values found from random search, # Hard-coded values found from random search\n",
    "    metric = \"F1\",\n",
    "    maximize = TRUE,\n",
    "    preProcess = c(\"center\", \"scale\"),\n",
    "    weights = class_weights_normalized[meta_training_data$perfectMentalHealth]\n",
    "  )\n",
    "))\n",
    "message(\"Meta-Model Training Complete.\\n\")\n",
    "message(\"Meta-Model Best Tune:\")\n",
    "message(paste(\"  sigma =\", meta_model$bestTune$sigma, \"C =\", meta_model$bestTune$C))\n",
    "message(\"Meta-Model CV Macro-F1: \", max(meta_model$results$F1))\n",
    "\n",
    "\n",
    "# --- Performance Comparison ---\n",
    "message(\"\\n--- Model Performance Comparison ---\")\n",
    "message(\"SVC CV Macro-F1: \", round(max(svc_model$results$F1), 4))\n",
    "message(\"RF CV Macro-F1: \", round(max(rf_model$results$F1), 4))\n",
    "message(\"XGB CV Macro-F1: \", round(max(xgb_model$results$F1), 4))\n",
    "message(\"Stacked Ensemble CV Macro-F1: \", round(max(meta_model$results$F1), 4))\n",
    "\n",
    "# Build your final model here, use additional coding blocks if you need to\n",
    "fin.mod <- meta_model\n",
    "\n",
    "# Prepare test data\n",
    "test <- prepare_data_for_model(\n",
    "  test,\n",
    "  is_training = FALSE,\n",
    "  target_variable_name = \"perfectMentalHealth\" # Even though it's test, function needs it\n",
    ")\n",
    "\n",
    "# Get levels for prediction type=\"prob\"\n",
    "target_levels_valid <- levels(classification_train_fe$perfectMentalHealth)\n",
    "\n",
    "# Make predictions (probabilities) from each base model on the preprocessed test data\n",
    "# Using type=\"prob\" to get class probabilities\n",
    "svc_test_probs <- predict(svc_model, newdata = test, type = \"prob\")\n",
    "rf_test_probs <- predict(rf_model, newdata = test, type = \"prob\")\n",
    "xgb_test_probs <- predict(xgb_model, newdata = test, type = \"prob\")\n",
    "\n",
    "# Rename columns to match meta-model training data\n",
    "# Note: The column names in model$pred will be the valid_levels themselves (e.g., X.2, X.1, X0, X1, X2)\n",
    "# We need to ensure consistency.\n",
    "colnames(svc_test_probs) <- paste0(\"SVC_Prob_\", target_levels_valid)\n",
    "colnames(rf_test_probs) <- paste0(\"RF_Prob_\", target_levels_valid)\n",
    "colnames(xgb_test_probs) <- paste0(\"XGB_Prob_\", target_levels_valid)\n",
    "\n",
    "# Create a dataframe of these test probabilities for the meta-model\n",
    "meta_test_data <- cbind(\n",
    "  svc_test_probs,\n",
    "  rf_test_probs,\n",
    "  xgb_test_probs\n",
    ")\n",
    "\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mod, meta_test_data)\n",
    "\n",
    "# Convert predictions back to original labels\n",
    "pred.label <- factor(\n",
    "  as.character(pred.label),\n",
    "  levels = valid_levels,\n",
    "  labels = original_levels\n",
    ")\n",
    "\n",
    "# Convert to integer values\n",
    "pred.label <- as.integer(as.character(pred.label))\n",
    "\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "write.csv(\n",
    "    data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "    \"ClassificationPredictLabel.csv\",\n",
    "    row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../classification_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"classification_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "f1_score <- F1_Score(truths$x, pred.label)\n",
    "cat(paste(\"f1_score is\", f1_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5197_Assignment_Marking_Guidelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
