{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 5 - Statistical Interference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference\n",
    "\n",
    "## Introduction\n",
    "\n",
    "* We often want to understand characteristics of a **population** based on a **sample** drawn from it. This process is known as **statistical inference**.\n",
    "* Recall from previous discussions (Slides 2 & 5) that population parameters (like $\\mu$ for mean and $\\sigma^2$ for variance) are often unknown.\n",
    "* Statistical inference provides tools to estimate these unknown parameters and make decisions about the population based on sample data.\n",
    "\n",
    "## Parameters Estimation (Slide 4)\n",
    "\n",
    "* The core of statistical inference often involves **parameter estimation**.\n",
    "* We use **estimators** (functions of the sample data) to estimate the unknown population parameters.\n",
    "\n",
    "### Example (Slide 2 & 5)\n",
    "\n",
    "* Consider the example of weights of individuals. Initially, we might assume a distribution with known parameters: $Weight \\sim \\mathcal{N}(\\theta \\equiv \\{\\mu = 68, \\sigma = 16\\})$.\n",
    "* However, in reality, $\\mu$ and $\\sigma$ are likely unknown.\n",
    "* We collect sample data, e.g., heights of people in a classroom: $y = (y_1, y_2, ..., y_n) = (1.75, 1.64, 1.81, 1.55, 1.51, 1.67, 1.83, 1.63, 1.72, ...)$.\n",
    "* We can use this sample data to estimate the population mean and variance of heights.\n",
    "\n",
    "## Bias and Estimator Quality (Slide 3)\n",
    "\n",
    "* When we use an estimator, we want it to be \"good\". Key properties of an estimator include:\n",
    "\n",
    "    * **Bias**: The difference between the expected value of the estimator and the true value of the parameter.\n",
    "        * $Bias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$\n",
    "        * An estimator is **unbiased** if $E[\\hat{\\theta}] = \\theta$, meaning its average value over many samples equals the true parameter.\n",
    "\n",
    "    * **Variance**: A measure of the estimator's variability or spread around its expected value.\n",
    "        * $Var(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]$\n",
    "        * Lower variance indicates more precise estimates.\n",
    "\n",
    "    * **Squared Error**: A measure that combines both bias and variance.\n",
    "        * $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = Var(\\hat{\\theta}) + [Bias(\\hat{\\theta})]^2$\n",
    "        * We often aim for estimators with low MSE.\n",
    "\n",
    "## Maximum Likelihood Estimation (MLE) (Slide 53)\n",
    "\n",
    "* **Maximum Likelihood Estimation (MLE)** is a common method for estimating parameters.\n",
    "* The idea is to find the parameter values that maximize the **likelihood function**, which represents the probability of observing the given sample data under different parameter values.\n",
    "\n",
    "    * Given a sample $y = (y_1, ..., y_n)$ from a distribution with parameter(s) $\\theta$, the likelihood function is:\n",
    "        $$L(\\theta | y) = P(y_1, ..., y_n | \\theta) = \\prod_{i=1}^{n} P(y_i | \\theta) \\quad \\text{(for i.i.d. samples)}$$\n",
    "\n",
    "* We find the value of $\\theta$ that maximizes $L(\\theta | y)$. Often, it's easier to maximize the **log-likelihood function**:\n",
    "    $$l(\\theta | y) = \\log L(\\theta | y) = \\sum_{i=1}^{n} \\log P(y_i | \\theta)$$\n",
    "\n",
    "* **Consistency of MLE**: For many distributions, the MLE is **consistent**, meaning that as the sample size $n$ increases, the estimator $\\hat{\\theta}_{MLE}$ converges in probability to the true parameter $\\theta$ ($\\hat{\\theta}_{MLE} \\xrightarrow{p} \\theta$). However, there are cases (e.g., high-dimensional problems) where this might not hold.\n",
    "\n",
    "## Sampling Distribution (Slides 54 & 55)\n",
    "\n",
    "* The **sampling distribution** of an estimator is the probability distribution of that estimator when computed from multiple independent random samples of the same size from the same population.\n",
    "\n",
    "### Example (Slide 54)\n",
    "\n",
    "* Consider a random sample $Y_1, ..., Y_n$ from a normal distribution $N(\\mu, \\sigma^2)$.\n",
    "* The **sample mean** $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ is an estimator of the population mean $\\mu$.\n",
    "* The sampling distribution of the sample mean is:\n",
    "    * $E[\\bar{Y}] = \\mu$ (unbiased estimator)\n",
    "    * $Var(\\bar{Y}) = \\frac{\\sigma^2}{n}$\n",
    "* Furthermore, if the population is normally distributed, the sampling distribution of the sample mean is also normal:\n",
    "    * If $Y_i \\sim N(\\mu, \\sigma^2)$, then $\\bar{Y} \\sim N(\\mu, \\frac{\\sigma^2}{n})$.\n",
    "* Knowing the sampling distribution provides more information than just the mean and variance of the estimator; it tells us the entire probabilistic behavior of the estimator.\n",
    "\n",
    "## Reading/Terms to Revise (Slide 55)\n",
    "\n",
    "* **Estimator, parameter estimation**\n",
    "* **Sample mean**\n",
    "* **Maximum likelihood**\n",
    "* **Sampling distribution**\n",
    "* **Bias, variance, and squared error of an estimator**\n",
    "\n",
    "## Next Steps (Slide 55)\n",
    "\n",
    "* Next week, we will cover the **Central Limit Theorem** and **confidence intervals**, which build upon the concepts of sampling distributions and parameter estimators to quantify the uncertainty in our estimates.\n",
    "* **Reading for next week:** Chapters 6 (Section 6.3) and 7 (primarily Sections 7.3, 7.4, also 7.5) of Ross."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum Square Error  \n",
    "To estimate mean, compute derivative of $\\sum_{i=1}^{n}(y_i - μ)^2 $  \n",
    "Calculate that to equal 0 (turning point aka min)  \n",
    "This equates to 1/n $\\sum_{i=1}^{n} y_i$ which is just the sample mean  \n",
    "\n",
    "\n",
    "Maximum likelihood estimation  \n",
    "$ p(y | θ) =  \\prod_{i=1}^{n}p(y_i | θ)$  \n",
    "this is each item multiplied by each other  \n",
    "\n",
    "As this function is normally small, and for easier calculation,\n",
    "we usually take the logarithm of the function to more easily compute the derivative  \n",
    "(this uses the properties of ln(a*b) = ln(a)+ln(b), ln(1/a) = -ln (a) and ln (a^b) = b ln (a))  \n",
    "(don't forget exponent rule: e^a*e^b = e^(a+b))\n",
    "\n",
    "(alternatives: Maximum a Posteriori (MAP), Bayesian Estimate, out of scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE will be in assignment (not mid-term)\n",
    "Mid term will be week 1-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ML estimator of the Bernouli Distribution\n",
    "In the bernouli distribution  \n",
    "$p(y|θ) = θ^y(1-θ)^{1-y}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Likelihood  \n",
    "What is :  \n",
    "$p(y|θ) = \\prod_{i=1}^{n} p(y_i|θ)$\n",
    "equal to?  \n",
    "\n",
    "$p(y|θ) = p(y_1|θ) . p(y_2|θ) ... p(y_n|θ) $  \n",
    "        = $θ^{y1}(1-θ)^{1-y_1} . θ^{y2}(1-θ)^{1-y_2} ... θ^{yn}(1-θ)^{1-y_n} $   \n",
    "        = $θ^{\\sum_{y1}^{yn} y_i} (1-θ)^{\\sum_{y1}^{yn} 1-y_i}$  \n",
    "  \n",
    "\n",
    "Let $\\sum_{y1}^{yn} y_i $ = m  \n",
    "    $p(y|θ) = θ^m(1-θ)^{n-m}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "2.2 Negative log-likelihood  \n",
    "$p(y|θ) = θ^m(1-θ)^{n-m}$  \n",
    "$L(y|θ) = -log[p(y|θ)]$  \n",
    "        = $ - log θ^m + (- log (1-θ)^{n-m})$  \n",
    "        = $ -m log θ - (n-m) log (1-θ) $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2.3 Find the MLE  \n",
    "$\\frac{d[L(y|θ)]}{dθ}$  \n",
    "    = $-m/θ - (n-m) ((\\frac{1}{1-θ}) . (-1))$  \n",
    "    = -m/θ + (n-m)/(1-θ)  \n",
    "\n",
    "Let $\\frac{d[L(y|θ)]}{dθ} = 0$  \n",
    "-m/θ + (n-m)/(1-θ) = 0  \n",
    "$\\frac{-m(1-θ) + (n-m)θ}{(1-θ)θ} = 0$  \n",
    "-m + mθ + nθ - mθ = 0  \n",
    "-m + nθ = 0  \n",
    "θ = m/n -> MLE estimator of parameter θ  \n",
    "where n is number of trials and m is number of successes  \n",
    "this requires n to be large enough  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hitchhiker's estimator\n",
    "Hitchhiker's estimator always says the answer is 42, no matter what  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Quality of the hitchhiker's estimator  \n",
    "Bias = Expected value of parameter - parameter  \n",
    "1. Bias of hitchhiker's estimator:  \n",
    "$B_μ(μ) = E[μ(Y)] - μ  \n",
    "        = 42 - μ  \n",
    "\n",
    "2. Variance of hitchhiker's estimator:  \n",
    "    = 0 \n",
    "\n",
    "3. Squared-error of hitchhiker's estimator:  \n",
    "MSE = E[(μ(y) - μ)^2]   \n",
    "    = (42 - μ)^2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 When does the hitchhiker's estimator work well? \n",
    "When is it better than the sample mean?  \n",
    "$(42 - μ)^2 = σ^2/n$  \n",
    "$42 - μ = \\sqrt{\\frac{σ^2}{n}}$  \n",
    "$42 - μ = -\\frac{σ}{\\sqrt{n}}$      |      $42 - μ = \\frac{σ}{\\sqrt{n}}$  \n",
    "$μ_2 = 42 + \\frac{σ}{\\sqrt{n}}$      |       $μ_1 = 42 - \\frac{σ}{\\sqrt{n}}$  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
