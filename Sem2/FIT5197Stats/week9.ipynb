{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7d21f6",
   "metadata": {},
   "source": [
    "# Week 9 - Machine Learning and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e6d63",
   "metadata": {},
   "source": [
    "## Recap/Precap\n",
    "Sample statistics: use to help us compute parameter estimates of models  \n",
    "Probability: helps formalise the model problem to deal with noisy data  \n",
    "Expectation: defines average values a model takes on.  \n",
    "Distributions: define the random processes in a model.  \n",
    "Inference: helps to estimate parameters of a model.  \n",
    "CLT: implies the variance of our parameter estimates will\n",
    "shrink with more observations.  \n",
    "Confidence Intervals: give bounds on the value of the true\n",
    "parameters of the model.  \n",
    "Hypothesis testing: helps tell us if model components are\n",
    "useful for prediction, and to compare models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610e6c3",
   "metadata": {},
   "source": [
    "# Statistical Data Modelling, Machine Learning, and Regression\n",
    "\n",
    "## Introduction (Slides w9stats 2 & 3)\n",
    "\n",
    "* **Statistical Data Modelling:** Using models to make inferences and predictions about data.\n",
    "* **Machine Learning (ML):** The study of computer algorithms that improve automatically through experience.\n",
    "* Historically, many ML methods were considered statistical methods.\n",
    "* ML often helps us train the models used in statistical data modelling for predictions.\n",
    "\n",
    "## The Goal: Prediction (Slide w9stats 3)\n",
    "\n",
    "* We observe data as pairs of $(y_i, \\mathbf{x}_i)$, where:\n",
    "    * $y_i$ is the observation of the random variable $Y$ (the target or dependent variable).\n",
    "    * $\\mathbf{x}_i = (x_{i,1}, x_{i,2}, ..., x_{i,p})$ are observations of $p$ random variables $X_1, ..., X_p$ (the predictors or independent variables).\n",
    "    * Each data point is $(y_i, x_{i,1}, ..., x_{i,p}) \\in \\mathbb{R}^{p+1}$, indexed by $i = 1, ..., n$.\n",
    "* Our primary goal is to make predictions, $\\hat{y}_i$, based on the predictor variables $\\mathbf{x}_i$.\n",
    "* This is particularly useful when we know $\\mathbf{x}_i$ but don't know or cannot measure $y_i$.\n",
    "\n",
    "## Regression: Predicting Continuous Outcomes (Slide w9stats 5)\n",
    "\n",
    "* **Regression** is a statistical method used for predictive modelling where the target variable $Y$ is continuous.\n",
    "* We aim to find a relationship between the predictor variables $\\mathbf{X}$ and the expected value of the target variable $Y$.\n",
    "* This relationship is often expressed through a model with unknown parameters that we need to estimate from the data.\n",
    "\n",
    "## Simple Linear Regression\n",
    "\n",
    "* A basic form of regression with a single predictor variable ($p=1$).\n",
    "* The model assumes a linear relationship:\n",
    "    $$\\hat{y}_i = \\beta_0 + \\beta_1 x_{i,1}$$\n",
    "    * $\\beta_0$ is the intercept.\n",
    "    * $\\beta_1$ is the slope.\n",
    "* Our goal is to estimate $\\beta_0$ and $\\beta_1$ from the observed data.\n",
    "\n",
    "## Least Squares Fitting (Slide w9stats 6)\n",
    "\n",
    "* A common method to estimate the parameters in a regression model.\n",
    "* The idea is to find the line (or hyperplane in multiple regression) that minimizes the sum of the squared differences between the observed target values ($y_i$) and the predicted values ($\\hat{y}_i$).\n",
    "* The **residual** for the $i$-th observation is $e_i = y_i - \\hat{y}_i$.\n",
    "* The **residual sum of squares (RSS)** is:\n",
    "    $$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_{i,1}))^2 $$ \n",
    "    (for simple linear regression)\n",
    "* We find the values of $\\beta_0$ and $\\beta_1$ that minimize this RSS. This is typically done by taking partial derivatives with respect to $\\beta_0$ and $\\beta_1$, setting them to zero, and solving the resulting system of equations.\n",
    "\n",
    "## Multiple Linear Regression (Slide w9stats 7)\n",
    "\n",
    "* Extends simple linear regression to include multiple predictor variables ($p > 1$).\n",
    "* The model is:\n",
    "    $$\\hat{y}_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + ... + \\beta_p x_{i,p} = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{i,j}$$\n",
    "* Here, $\\beta_j$ represents the change in the expected value of $Y$ for a one-unit increase in $X_j$, holding all other predictors constant.\n",
    "* The least squares approach is also used to estimate the coefficients $\\beta_0, \\beta_1, ..., \\beta_p$ by minimizing the RSS:\n",
    "    $$RSS = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{i,j}))^2$$\n",
    "\n",
    "## Nonlinear Effects and Transformations (Slides w9stats 60 & 61)\n",
    "\n",
    "* Sometimes the relationship between predictors and the target is not linear.\n",
    "* We can still use linear regression techniques to model nonlinear relationships by transforming the predictor variables.\n",
    "* **Logarithmic Transformation:** Useful when a percentage increase in $X$ is associated with a constant increase in $Y$.\n",
    "    * $x_{i,j} \\Rightarrow \\log(x_{i,j})$ (only if all $x_{i,j} > 0$)\n",
    "* **Polynomial Transformations:** Allow for more general nonlinear fits.\n",
    "    * $x_{i,j} \\Rightarrow x_{i,j}, x_{i,j}^2, x_{i,j}^3, ..., x_{i,j}^q$\n",
    "    * Increasing $q$ can lead to more nonlinear fits but also increases the risk of **overfitting** (modeling noise in the data).\n",
    "* **Example (Quadratic):** $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$\n",
    "\n",
    "## Connection to Maximum Likelihood Estimation (MLE) (Recall w5stats)\n",
    "\n",
    "* While Least Squares focuses on minimizing the sum of squared errors, it has a close connection to MLE under certain assumptions.\n",
    "* If we assume that the errors (residuals) in our linear regression model are independently and identically distributed (i.i.d.) from a normal distribution with mean 0 and constant variance $\\sigma^2$ ($e_i \\sim \\mathcal{N}(0, \\sigma^2)$), then the least squares estimators for the regression coefficients are also the maximum likelihood estimators.\n",
    "* Maximizing the likelihood of the data under this normality assumption is equivalent to minimizing the sum of squared errors.\n",
    "\n",
    "## Reading/Terms to Revise (Slide w9stats 62 & w5stats 55)\n",
    "\n",
    "* **Machine learning**\n",
    "* **Supervised/unsupervised learning**\n",
    "* **Predictive modelling in R**\n",
    "* **Regression**\n",
    "* **Simple linear regression**\n",
    "* **Multiple linear regression**\n",
    "* **Least squares fitting**\n",
    "* **Overfitting**\n",
    "* **Estimator, parameter estimation** (from previous notes)\n",
    "\n",
    "## Next Steps (Slide w9stats 62)\n",
    "\n",
    "* Continue exploring more advanced regression techniques and R modelling.\n",
    "* Finish reading Chapter 9 of Ross on regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e035731",
   "metadata": {},
   "source": [
    "Supervised learning  \n",
    "- observations with labels  \n",
    "    - Regression  \n",
    "        - Numerical variable  \n",
    "    - Classification  \n",
    "        - Categorical variable  \n",
    "\n",
    "Unsupervised learning  \n",
    "- observations without labels  \n",
    "    - Clustering  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500907f7",
   "metadata": {},
   "source": [
    "Least Squares Fitting  \n",
    "$$\n",
    "RSS (\\beta_0, \\beta_1) = argmin \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1x_i)^2\n",
    "$$\n",
    "$$\n",
    "\\frac{d RSS (\\beta_0, \\beta_1)}{d\\beta0} = \\sum_{i=1}^{n} \\frac{d (y_i - \\beta_0 - \\beta_1x_i)^2}{d\\beta0}\n",
    "$$\n",
    "$$\n",
    "2\\sum_{i=1}^{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc4ee3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ X, data = df)\n",
       "\n",
       "Coefficients:\n",
       "(Intercept)            X  \n",
       "    -0.1883       1.1035  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ X, data = df)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.6791 -0.9811  0.3578  1.6008  2.0306 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)   \n",
       "(Intercept)  -0.1883     1.5123  -0.125  0.90397   \n",
       "X             1.1035     0.2437   4.528  0.00193 **\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 2.214 on 8 degrees of freedom\n",
       "Multiple R-squared:  0.7193,\tAdjusted R-squared:  0.6842 \n",
       "F-statistic:  20.5 on 1 and 8 DF,  p-value: 0.00193\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ X, data = df, subset = 1:9)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-0.9506 -0.4855 -0.2701  0.3570  1.8275 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  -1.6180     0.6588  -2.456   0.0437 *  \n",
       "X             1.4934     0.1171  12.756 4.22e-06 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.9069 on 7 degrees of freedom\n",
       "Multiple R-squared:  0.9588,\tAdjusted R-squared:  0.9529 \n",
       "F-statistic: 162.7 on 1 and 7 DF,  p-value: 4.215e-06\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tutorial\n",
    "library(ggplot2)\n",
    "df <- read.csv(\"data/toydata.csv\")\n",
    "ggplot(data = df, aes(x = X, y = y)) +\n",
    "    geom_point()\n",
    "\n",
    "lm (y ~X, data = df)\n",
    "fit <- lm(y ~ X, data=df)\n",
    "summary(fit)\n",
    "\n",
    "yhat = fit$coefficients[[1]] + fit$coefficients[[2]]*df$X\n",
    "yhat_pred = predict(fit, df)\n",
    "\n",
    "# Outliers skew the data significantly, because least-squares fit\n",
    "# overcompensates for outliers\n",
    "fit2 <- lm(y ~ X, data=df, subset=1:9)\n",
    "summary(fit2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
