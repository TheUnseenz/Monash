{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316f80ad",
   "metadata": {},
   "source": [
    "# Week 9 - Unsupervised Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47366c9",
   "metadata": {},
   "source": [
    "## 1. Supervised vs. unsupervised machine learning\n",
    "- Uses labelled datasets  \n",
    "- Commonly used in classification and regression  \n",
    "\n",
    "## Unsupervised learning\n",
    "- Unsupervised learning employs machine learning algorithms to\n",
    "  examine and group unlabeled datasets.  \n",
    "- These algorithms unveil concealed patterns within data\n",
    "  autonomously, eliminating the necessity for human guidance,\n",
    "  hence termed \"unsupervised.\"  \n",
    "- Common tasks:  \n",
    "    - Clustering, i.e., assign similar data points into groups  \n",
    "    - Association mining, i.e., use different rules to find relationships\n",
    "    between variables in a given dataset, often used for market\n",
    "    basket analysis and recommendation engines, along the lines of\n",
    "    “Customers Who Bought This Item Also Bought” recommendations.  \n",
    "    - Dimensionality reduction, i.e., reduces the number of data\n",
    "    inputs to a smaller size while also preserving the data integrity\n",
    "\n",
    "### Differences\n",
    "- The main difference: Labeled data  \n",
    "- Other differences:  \n",
    "    - Goals: In supervised learning, the goal is to predict outcomes for\n",
    "    new data. With an unsupervised learning algorithm, the goal is\n",
    "    to get insights from large volumes of new data.  \n",
    "    - Applications: Supervised learning models are ideal for spam\n",
    "    detection, sentiment analysis, weather forecasting and pricing\n",
    "    predictions, among other things.   \n",
    "    In contrast, unsupervised learning is a great fit for anomaly  \n",
    "    detection and customer personas.  \n",
    "    - Drawbacks: Training supervised learning models can be time-\n",
    "    consuming, as it demands expertise to label input and output\n",
    "    variables accurately. Conversely, unsupervised learning\n",
    "    approaches may yield highly inaccurate results without human\n",
    "    intervention to validate the output variables.  \n",
    "\n",
    "Semi-supervised learning\n",
    "- Semi-supervised learning combines aspects of both supervised\n",
    "  learning and unsupervised learning.  \n",
    "- Machine learning techniques that fall under this category\n",
    "  utilize both labeled and unlabeled data to train a predictive\n",
    "  model.  \n",
    "- Typically, it works in the following way:  \n",
    "(1) Semi-supervised learning uses a small amount of labeled data to train\n",
    "an initial model, which can be used to predict labels on a larger amount\n",
    "of unlabeled data.  \n",
    "(2) The model is then applied iteratively to both originally labeled data and\n",
    "data with predicted labels (pseudo-labels).  \n",
    "(3) After, you will add your most accurate predictions to the labeled\n",
    "dataset and repeat the process again to continue improving the\n",
    "performance of your model.  \n",
    "\n",
    "Semi-supervised learning  \n",
    "- Self-supervised learning is a machine learning process where\n",
    "  the model trains itself to learn one part of the input from\n",
    "  another part of the input.  \n",
    "- In this process, the unsupervised problem is transformed into\n",
    "  a supervised problem by In this process, the unsupervised\n",
    "  problem is transformed into a supervised problem by auto-\n",
    "  generating the labels.  \n",
    "- The process of the self-supervised learning method is to\n",
    "  identify any hidden part of the input from any unhidden part\n",
    "  of the input, e.g.:  \n",
    "    - In natural language processing, if we have a few words, using self-\n",
    "      supervised learning we can complete the rest of the sentence.  \n",
    "    - In a video, we can predict past or future frames based on available\n",
    "      video data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac43bb",
   "metadata": {},
   "source": [
    "## 2. Segmenting data  \n",
    "- Sometimes the segmenting of data is because of the\n",
    "  context of the data  \n",
    "  - Separate sources  \n",
    "  - Separate collection circumstances  \n",
    "  - Social or physical distinctions  \n",
    "- Sometimes we don’t have pre-determined segments, but\n",
    "  we want segmentation  \n",
    "  - Some of the data may be similar  \n",
    "  - Some of the modelling would be better if it didn’t need to\n",
    "    represent all of the data  \n",
    "  - Better decision-making if we consider each segment\n",
    "    independently  \n",
    "\n",
    "### Identifying Customer Segments  \n",
    "- Customers are grouped into segments  \n",
    "- Marketing is then specialised to each segment  \n",
    "  - leads to better marketing  \n",
    "- in healthcare, segments are called cohorts  \n",
    "  - used for patient management and staff organisation  \n",
    "\n",
    "Segmentation can be done by various attributes e.g. Geographic, Demographic, Psychographic, Behavioural  \n",
    "A segmentation model is a graphical model where  \n",
    "- the cluster variable is unknown, called “latent”  \n",
    "- the cluster variable identifies the segments  \n",
    "- latent means the variable is never observed in the data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbc412b",
   "metadata": {},
   "source": [
    "## 3. Clustering data\n",
    "A cluster is  \n",
    "- segmented data for analysis  \n",
    "- segmented network nodes  \n",
    "- segmented data storage  \n",
    "- a group of associated computers  \n",
    "So clustering = segmentation… sometimes  \n",
    "\n",
    "Clustering tends to be associated with segmentation that\n",
    "allows us to recognize similar combinations of attribute\n",
    "values when we don’t have predefined categories.  \n",
    "\n",
    "### Uses of clustering\n",
    "- Text documents, e.g., patents, legal cases,\n",
    "  webpages, questions and feedback  \n",
    "    - Topic modelling  \n",
    "- Clients, e.g., recommendation systems  \n",
    "- Fault detection, e.g., fraud, network security  \n",
    "- Missing data  \n",
    "- A clustering task may require a number of different\n",
    "  algorithms/approaches.  \n",
    "\n",
    "Elements in a cluster\n",
    "- Are similar in some attributes  \n",
    "- May consider some attributes to weigh more than others  \n",
    "    - Not all attributes are as important as others  \n",
    "    - Needs feature selection  \n",
    "- May be considered to be close to each other  \n",
    "    - Needs distance measurements  \n",
    "\n",
    "Clustering terms  \n",
    "- Distance  \n",
    "- Centroid  \n",
    "- Nearest neighbour  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9892bd4",
   "metadata": {},
   "source": [
    "### K-means  \n",
    "1. Randomly select centroids for K clusters\n",
    "2. Select nearest data points as cluster population\n",
    "3. Find mean values in each cluster and use that as new centroid\n",
    "4. Re-evaluate populations and centroids until stable/convergance\n",
    "Stopping criteria for K-means clustering  \n",
    "(1) Centroids of newly formed clusters do not change  \n",
    "(2) Points remain in the same cluster  \n",
    "(3) Maximum number of iterations is reached  \n",
    "\n",
    "Downsides\n",
    "- Does not work with categorical data and it is susceptible to outliers  \n",
    "- Have to predefine a value for K  \n",
    "- No guarantee there are actually clusters to find  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40a007",
   "metadata": {},
   "source": [
    "### Evaluating clustering algorithms  \n",
    "Clustering is easier to evaluate by visualization  \n",
    "High dimensional data can be projected to lower dimensions via dimentionality reduction e.g. PCA/t-SNE  \n",
    "\n",
    "#### Internal metrics  \n",
    "- Internal metrics use only the data and the clustering  \n",
    "  output to measure how well the clusters are formed.  \n",
    "- Some well-known internal metrics include:  \n",
    "    - Silhouette Score  \n",
    "    - Dunn’s Index  \n",
    "\n",
    "#### Silhouette Score  \n",
    "- Measures separation distance between clusters  \n",
    "- Value range [-1, 1]  \n",
    "    - Closer to +1: Cluster samples further away from neighbouring clusters  \n",
    "    - Close to 0: Sample very close to decision boundary between neighbouring clusters  \n",
    "    - Negative values: Samples assigned to wrong cluster  \n",
    "![image.png](attachment:image.png)\n",
    "<style type=\"text/css\">\n",
    "    img {\n",
    "        width: 400px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "#### Elbow method  \n",
    "- The Elbow Method is a graphical representation of finding the optimal ‘K’ in a \n",
    "  K-means clustering.  \n",
    "- It works by finding WCSS (Within-Cluster Sum of Square) i.e. the sum of the square\n",
    "  distance between points in a cluster and the cluster centroid.  \n",
    "- It involves plotting the variance (i.e., WCSS) explained by different numbers of\n",
    "  clusters and identifying the “elbow” point, where the rate of variance decreases\n",
    "  sharply levels off, suggesting an appropriate cluster count for analysis or model\n",
    "  training.  \n",
    "![image-2.png](attachment:image-2.png)  \n",
    "\n",
    "#### External Metrics  \n",
    "- External metrics use some external information, such as\n",
    "  labels, classes, or ground truth, to measure how well the\n",
    "  clusters match the expected or desired outcomes.  \n",
    "- Some well-known internal metrics include:  \n",
    "  - Accuracy, i.e., how many data points are correctly\n",
    "    grouped?  \n",
    "  - Rand index  \n",
    "    - Similarity measure between two clusters by considering all pairs of samples \n",
    "    and counting pairs that are assigned in the same or different clusters in\n",
    "    the predicted and true clusterings.  \n",
    "    - RI = Number of agreeing pairs/number of pairs  \n",
    "    - Value range of [0,1], with 1 representing perfect match  \n",
    "  - Mutual information  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02214357",
   "metadata": {},
   "source": [
    "### Hierarchical clustering  \n",
    "- Clusters within clusters\n",
    "- Greedy, costly\n",
    "- No randomness (reproducible), can cut the tree at any level\n",
    "\n",
    "#### Agglomerative (bottom-up)\n",
    "- Common steps:\n",
    "    - Treat each data point as a centroid in a cluster of\n",
    "    population 1\n",
    "    - Form new clusters by merging nearby clusters\n",
    "    - Continue until only one cluster\n",
    "- Various ways to calculate which clusters should be\n",
    "  merged, often looking at (min or max) distances of the\n",
    "  cluster population to each other\n",
    "- The results of hierarchical clustering are usually presented\n",
    "  in a dendrogram\n",
    "\n",
    "#### Divisive (top-down)\n",
    "- Consider all data points as a single cluster\n",
    "- In each iteration, separate data points from cluster which are not similar\n",
    "- Each data point which is separated is considered as an individual cluster, \n",
    "  and we will end up with n clusters in the end (assuming there are n data points).\n",
    "- Less commonly used compared to agglomerative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041a544",
   "metadata": {},
   "source": [
    "## 4. Integrate data science findings\n",
    "Model Uncertainty  \n",
    "- Machine learning models can display confidence degree in predictions  \n",
    "- Provides transparency and tells you room for improvement  \n",
    "\n",
    "Cross-Validation  \n",
    "- Used in statistical analysis and machine learning to evaluate model performance and reliability  \n",
    "    - Reduce overfitting  \n",
    "    - Provide reliable performance metric  \n",
    "    - Potentially alleviate model uncertainty  \n",
    "- Process:  \n",
    "    1. Splitting the Data: The dataset is divided into “folds” or\n",
    "    subsets. The typical structure involves k subsets, hence the\n",
    "    term “k-fold cross-validation.”  \n",
    "    2. Iterative Training and Testing: In k-fold cross-validation, the\n",
    "    model is trained k times, each time using a different fold as\n",
    "    the test set while using the remaining k-1 folds for training.  \n",
    "    3. Performance Metrics: The results from each iteration are\n",
    "    averaged to provide a more reliable estimate of the model’s\n",
    "    performance.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
