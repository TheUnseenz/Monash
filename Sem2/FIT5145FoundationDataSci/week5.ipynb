{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterising data and \"big\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 5 Leanring Outcomes:  \n",
    "- Explain what Big data is  \n",
    "- Understand the V’s in Big data  \n",
    "- Characterise data sets used to assess a data science project  \n",
    "- Analyse a given use case based on a set of criteria used by NIST  \n",
    "- Work with temporal data and textual data  \n",
    "- Perform network and graph analysis  \n",
    "- Wrangle missing and NaN data  \n",
    "\n",
    "Topics for Lecture 5  \n",
    "1. Big Data characteristics  \n",
    "2. Case studies of Data Science projects  \n",
    "3. Working with complex data  \n",
    "4. Missing data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Data\n",
    "Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture,\n",
    "curate, manage, and process data within a tolerable elapsed time. Big data \"size\" is a constantly moving target.  \n",
    "\n",
    "40 MB of text, for us, can be argued to be big data for us - big data size depends on the tools you use for it.  \n",
    "\n",
    "### The V's in Big Data  \n",
    "1. Volume - bigness  \n",
    "2. Velocity - bigness  \n",
    "3. Variety - problems with analysis - change in meaning over time  \n",
    "4. Veracity - problems with analysis - correctness  \n",
    "BIG DATA is ANY attribute that challenges CONSTRAINTS of a system’s CAPABILITY or BUSINESS NEED."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "Quality and Complexity  \n",
    "- Managing data is hard  \n",
    "    - Having the required data and technologies  \n",
    "- Managing Big Data is harder  \n",
    "    - Determining/creating the required data and technologies  \n",
    "- Managing data quality is hardest  \n",
    "    - Making sure the data (and technologies) meet the needs  \n",
    "\n",
    "NIST - National Institute of Standards and Technology is widely used in characterising data science projects  \n",
    "### Nist Analysis\n",
    "- Data sources: where does the data comes from?  \n",
    "- Data volume: how much data is there?  \n",
    "- Data velocity: how does the data change over time?  \n",
    "- Data variety: what different kinds of data is there?  \n",
    "- Data veracity: is the data correct? what problems might it have?  \n",
    "- Software: what software needed to do the work?  \n",
    "- Analytics: what statistical analysis & visualisation is needed?  \n",
    "- Processing: what are the computational requirements?  \n",
    "- Capabilities: what are key requirements of the operational system?  \n",
    "- Security/privacy: what security/privacy requirements are there?  \n",
    "- Lifecycle: what ongoing requirements are there?  \n",
    "- Other: are there other notable factors?  \n",
    "\n",
    "Every company will use different ways and tools to analyse and handle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with complex data  \n",
    "- Complexities of temporal data  \n",
    "    - Temporal context can be specific/absolute or relative  \n",
    "    - Dates often don't have a consistent syntax e.g. ddmmyy vs yymmdd vs mmddyy   \n",
    "    - Convert/Decompose once, use often  \n",
    "    - Be careful comparing time  \n",
    "        - Months and years have different number of days  \n",
    "        - Time periods are not socially equal - Weekends, holidays, pay periods  \n",
    "\n",
    "- Complexities of textual data  \n",
    "    - Text data is everywhere  \n",
    "    - Syntax: structure of language e.g. grammar rules  \n",
    "    - Semantics: meaning of individual words within the surrounding context, understanding the theme  \n",
    "    - To process text, first do:  \n",
    "        - Tokenisation  \n",
    "            - The process of breaking a stream of text into tokens.  \n",
    "            - Acronyms might use periods and not just end of sentences.  \n",
    "            - Collocations: multi-word expressions that occur frequently and\n",
    "            correspond to some conventional way of saying things.  \n",
    "                - Noun phrases: “strong tea” and “weapons of mass destruction”  \n",
    "                - Verb phrases: “make up”, “wind up  \n",
    "            - Collocations often cannot be split up or their meaning changes  \n",
    "        - Case Normalisation  \n",
    "            - Capitalisation can help readers but aren't always helpful  \n",
    "        - Stopping/Stop Words  \n",
    "            - Many words only carry the grammar e.g. \"a\", \"the\", \"well\", \"however\", \"he\", \"him\"  \n",
    "            - They appear very commonly and carry little meaning, skewing analysis results  \n",
    "        - Stemming & Lemmatisation  \n",
    "            - Should we shorten words to their base form? E.g. \"educates\", \"educator\", \"education\" to \"educate\"  \n",
    "        - Sentence segmentation  \n",
    "            - Where do sentences begin and end?  \n",
    "            - Punctuation marks can be ambiguous e.g. part of abbreviations, decimal points, just absent in favour of new line etc  \n",
    "\n",
    "- Network and graph analysis  \n",
    "    - Network data is about different types of relationships between data, compared to tabular data.  \n",
    "    - A network is made up of nodes and arcs.  \n",
    "        - Node (vertices) – entities in the data  \n",
    "        - Edges (arcs) – relationships between the entities  \n",
    "    - Network data terms:  \n",
    "        - Directed graphs – direction of connections, visualised as arrows, e.g., retweets, power relationships, dispersion of\n",
    "        resources  \n",
    "        - Weight – the strength of a connection, e.g., number of instances  \n",
    "        - Degree – How many connections a node has  \n",
    "            - Can incorporate the weight of the connections  \n",
    "            - Can distinguish between in-degree and out-degree  \n",
    "    - Evaluating nodes  \n",
    "        - Significant nodes  \n",
    "            - The closeness of nodes is not an Euclidean distance  \n",
    "            - The centrality of a node can be measured in various ways.  \n",
    "            - Degree  \n",
    "            - Betweenness  \n",
    "            - Closeness  \n",
    "        - Clustering nodes  \n",
    "            - You can still identify clusters of nodes  \n",
    "        - Centrality  \n",
    "            - Degree centrality: most connections\n",
    "            - Betweenness centrality: most used for shortest trip between nodes\n",
    "            - Closeness centrality: least hops to travel to other nodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data\n",
    "- You want the data you are using to be of sufficient quality for your purpose  \n",
    "    - Accuracy  \n",
    "    - Completeness  \n",
    "    - Consistency  \n",
    "    - Integrity  \n",
    "    - Reasonability  \n",
    "    - Timeliness  \n",
    "    - Uniqueness/deduplication  \n",
    "    - Validity  \n",
    "        - Data Management Association (DAMA)  \n",
    "- Much of this is a data management issue  \n",
    "- Some of this is the fault of the data itself  \n",
    "\n",
    "#### Data Wrangling\n",
    "Data needs to be cleaned, so it can be (re)used.  \n",
    "Sometimes the quality of data is questionable  \n",
    "• Volume: with a lot of data, irregularities creep in  \n",
    "• Velocity: data can be out-of-date very quickly  \n",
    "• Variety: data can be in a different formats and types that don’t work well together  \n",
    "• Veracity: the accuracy or consistency of data from different sources or sets or circumstances  \n",
    "\n",
    "Data can also be incomplete\n",
    "- Sensors fail\n",
    "- Data collection failed\n",
    "- Data sharing fails\n",
    "- Data isn't significant enough (too small a sample)\n",
    "\n",
    "#### Consequences of missing data\n",
    "- Learning algorithms need the values.  \n",
    "- Not all statistical computing and graphics software ignore missing values  \n",
    "    - Bias results  \n",
    "    - Incorrect calculations   \n",
    "- NaN is not NA!\n",
    "\n",
    "- The choice for missing values like NaNs is often whether to  \n",
    "    - omit them from the data, or  \n",
    "    - give them a value  \n",
    "- If a small fraction of cases have several missings, drop the cases.  \n",
    "- If a variable or two, out of many, have a lot of missings, drop the variables.  \n",
    "- If missings are small in number, but located in many cases and variables, you need to impute these values\n",
    "    (replace with substituted values) to do most analyses.  \n",
    "    \n",
    "#### Methods of imputation\n",
    "- Simple parametric: Use the mean or median of the complete cases for each variable.  \n",
    "- Simple non-parametric: Find the k nearest neighbours with a complete value and then average these.  \n",
    "- Multiple imputation: Use a statistical distribution, e.g., normal model, and simulate a value \n",
    "    (or a set of values, for a hot deck imputation) for the missings.  \n",
    "- Designing the imputation should take into account dependencies that you have seen between missingness and existing variables.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
