{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb70b8e",
   "metadata": {},
   "source": [
    "# Task D:  Predictive Data Analysis using R\n",
    "I need to train a machine learning model on dialogue utterance vs dialogue usefulness. \n",
    "\n",
    "The data files I have, with the descriptions of their columns are:\n",
    "dialogue_utterance_train/validation/test.csv \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Usefulness_score - This score is given by a student to indicate their perceived \n",
    "usefulness of the FLoRA chatbot when answering the post-task \n",
    "questionnaire Question 3 (i.e., “To what extent do you think the \n",
    "GPT-powered chatbot on FLoRA is useful for you to accomplish \n",
    "the assignment?”). The value range of this feature is [1,5], with 1 \n",
    "representing “very unuseful”, 2 representing “unuseful”, 3 \n",
    "representing “neutral”, 4 representing “useful”, and 5 \n",
    "representing “very useful”.\n",
    "\n",
    "dialogue_usefulness_train/validation/test.csv\n",
    "Column Name - Description \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Timestamp - When an utterance contained in the dialogue was made \n",
    "Interlocutor - Whether the utterance was made by the student or the chatbot (\"Student\"/\"Chatbot\")\n",
    "Utterance_text - The text of the utterance\n",
    "\n",
    "dialogue_utterance_train has 117k lines, split over 303 unique dialogue IDs.\n",
    "1.  What  features  can  you  engineer  to  empower  the  training  of  a  machine  learning model? You may propose as many as you believe are useful. Please note that the number of the features should not exceed the number of the dialogues contained in the training set. Otherwise, the constructed machine learning models are prone to have overfitting issues. Select two features that you propose and try to use boxplots to visualise  the  feature  value  between  the following two groups of dialogues in the training  set:  (i)  those  with  Usefulness_score  of  1  or  2;  and  (ii)  those  with Usefulness_score  of  4  or  5.  Show if there  any  difference  between  the  two  groups  of dialogues? How can you tell whether the difference is statistically significant? Ideally, identify features that display statistically significant differences. \n",
    " \n",
    "2.  Build a machine learning model (e.g., polynomial regressions, regression tree) based on the training set by taking all the features that you have proposed and evaluate the performance of the model on the validation set using the relevant evaluation metrics you learned in class. Aim to include at least 5 features in this model. The best-performing model here is denoted as Model 1. \n",
    "3.  Now we want to improve the performance of Model 1 (i.e., to get a more accurate model).  For  example,  you  may  try  some  of the following methods to improve a model: \n",
    "●  Select  a  subset  of  the  features  (especially  the  important  ones  in  your opinions) as input to empower a machine learning model or a subset of the \n",
    "data in a dialogue (given that some questions asked by students might not be \n",
    "directly relevant to solving the assignment). \n",
    "●  Deal with errors (e.g.: filtering out data outliers). \n",
    "●  Rescale  data  (i.e.,  bringing  different  variables  with  different  scales  to  a common scale). \n",
    "●  Transform data (i.e., transforming the distribution of variables). \n",
    "●  Try other machine learning algorithms that you know. \n",
    " \n",
    "Please build the predictive models by trying some of the above methods or some other methods you can think of and evaluate the performance of the models and report whether Model 1 can be improved. \n",
    "Explain how you have improved your model by including code, output, \n",
    "and  explanations  (explaining  the  code  or the process) and justify why you have chosen some of the above methods or some other methods to improve a model \n",
    "(e.g., why this subset of the variables are chosen to build a model). \n",
    "4.  What is the Dialogue_ID of the dialogue you generated? Please copy and paste the whole  dialogue  text  that  you  generated  with  the  chatbot  here.  With  the best-performing model constructed from Question 2&3, what is the prediction value for the dialogue you generated? Is the prediction value close to the groundtruth value? \n",
    "If yes, what features do you think play important roles here to enable the model to successfully make the prediction? How can you determine the importance of features quantitatively? If not, what might be the reasons? For students whose dialogues are included in the test set, you may randomly select a dialogue from the validation set to analyse and answer this question. \n",
    "5.  The  groundtruth  Usefulness_score  values  in  the  file “dialogue_usefulness_test.csv” are unavailable now. Here, your task is to use the best-performing model constructed from Question 2&3 to predict the usefulness of the dialogues contained in the  test  set.  You  need  to  populate  your  prediction  results  (i.e.,  the  predicted Usefulness_score values) into the file “dialogue_usefulness_test.csv” and upload it to Moodle to measure the overall performance of your model. Please ensure the number of  columns  and  rows  remains  the  same  as  in  the  original  file (dialogue_usefulness_test.csv),  and  only  fill  in  the  prediction  results  in  the 'Usefulness_score'  column.  Please  name  the  submission  file  using  the  following format: LastName_StudentNumber_dialogue_usefulness_test.csv. \n",
    "\n",
    "The performance level of the model will be measured by RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5e4c2bb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Statistical Significance Tests ---\n",
      "Feature: total_dialogue_length_words - T-test p-value: 7e-04 \n",
      "Feature: avg_readability_score_student - T-test p-value: 0.1623 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAP1BMVEUAAAAAv8QaGhozMzNNTU1oaGh8fHyMjIyampqnp6eysrK9vb3Hx8fQ0NDZ2dnh4eHp6enr6+vw8PD4dm3///8AY8WWAAAACXBIWXMAABJ0AAASdAHeZh94AAAgAElEQVR4nO3di3qquhpGYXaLVnt0urj/a91yNIlg1SZ8ye94n7Va2yJgcEwQra0aAH9WqVcAsICQgAgICYiAkIAICAmIgJCACAgJiICQgAgICYiAkIAI/hBSNVme5ri7d3b17mf48sqcuh8uLLef8NpKuTN5dHVuWcDvC746k+OuqjaRFonUEod0x7Y/z24/d03v62sh9d+OFdLS6qwQ0ttp2dtIi0RqfwopzjT+pIePqnq/aU5XQ7p9gY+tzgohVdVPtEUitexCapqfqjreMqdVQlpanVVCirdIpBYtpO/t6UDkq7+835wufzfD8dE0ZX/xuKnegumD2e3bfUD/5c+ufZjy7cypu/p4aPdeD/M4L8Ff5M+uHh/mnL6zr6vNl7vA4frH8bHI8fygZHZ1zrdsWr9p/k3zta3qjya4wVdvaXAj3YmrubELboUzZ2ce7uywllghfTqPJ+r+8td8SG/dVO704ex+2tK6L7+rcVbTnLqrDyHtp3kshPQ1Xb/9zna6PEw8XX8/fPfzfBg3tzrOLeu/4c6/ee/nFoZ07ZYGN9KdeDGk7czE7jzc2WE1kUL66f6d/tm2m++9uze+dw+UvUf+wx3hGEx/ObvpDrSpPpv2DrM5z6m7+hBSffrpV90+lAjubsPH01HZ+7E5nu7bh27y7+b4dn787lz/MOyJtu2Ey6sT3jJv/qcv+tn59/6rtzS8kd7EM2Pn3Qp3YmcezkWsJ8ZZu6b9J717HHFs//He9JedM2venaE75nCnn2bnXQxPzI1zOh9XdXfc9v60XwppP+wHdv0kX90SnXvx+fpv3QN758huoWv/lnnz31cf7eXPIKSrtzS8kd7ECyFNt8Kd+GKgsLJIIW3cL053zq/37VJI3cVgemcib8K308H+58H5sXvMNJ4EOO+vwpA2ww7mEEwyLuV8/e7wzT2ym12d8JZ589+M92s/pKu3NLyR3sQLIc3O2ZmHcxHriXRoV7n3gI/6+p0hnP5idlU9fHno5rT5mJvTdIWZRxLhv/Xh4xbv0nBv/vGO7GZXJ7xlc/O/XJ3fQvJu5B0huRM783AuYj3RQjpf/jg9kNl/Hn4J6crsvp2Dla9de7d4n5lT3JDaXZJ7ZDe7OuEtuy2kK7d0/kZ6P7wWkjvTaR7eRawlUki18+Thxnsc40zpbP06fLLRnd2+ffBy/vJnd94lzB7aLYZ046Fd951tdXSP7GZXJ7xltxzazdzS7fitn/OZj/5GehP/FtLFnPt5hBexikgh7aruFW7dPaP//pcX0nH8znAld/pwdj/dfSDc382F1B2+fPfLDJbQfdz3SxlPNgRr7V6/ve77xjmym12d8JZ58x9ONnz06zCtzswtHa82nayYFuhNPDt2zq24Noaze0IkFCmkn+7pjJ+6PxX7MZ4GbroTw6d/gt+O/olhd3p/du1rcqbHRP2p3H1/bHVowpDan37W7W7CW8I04ekgbN+fnv6ZDel8/XZZtXfGeH513FsWzL8//f3ZPWRxVmfmlp52WrvTOh72fSTOjfQmHveU3tg5t8If82kezkWsJ1JI41OT7b+wH+Nj4O/+zNL4HKH7VKU7/TS70flVosOTi/VhnJMf0vs0ubMEd0L/Cdlgrd3r99O6DyvmVse9Zd03Zp6QDW/wzC2drtZfz7mR3sT9mgZj594KZ2JnHu7ssJpYIZ3+fa3HU0Uf7ctmvr+6f2I33YHR9+nTu/8I2Zl+nF1ns3d/b+G7e7lLe5/o5+SH1L3EZ3hpzbQEb0L/JUL+WnvX7/YT7n1vdnWcW9Z/I3yJUPX23f8LMK3OzC09Xa19qdFmPyzvfCO9iYc19cfOuxXOxM483NlhLRxJjz7ivBbgyDHVUyKkwc/4WOlR/QslvrcVz+A8I0LqhI9iHjA+RNr+PinsIaTOJsJLAb66X2n9jLE6KA4hAREQEhABIQEREBIQASEBERASEAEhAREQEhABIQEREBIQASEBERASEAEhAREQEhABIQEREBIQASEBERASEAEhARHkGtLL5cWX8b81FlqacNVfZofwgRnd9+M/TFy4gkKaLsbaPhfzmZlxbveFlxsuX34dJ6SX++aU3+ClREjXvpHffYGQMpU6pJcTJ4D2Ky+Ml5dxGu9zv9H6r1+a8XP/38s4nTeny3m8XCwgWKlpZV4WFxpeU81Zc++jH1V/U7xbczG67g0PiwwGfrjKy/TJn3r6EI753OzNShzSMMbj/b4fWefHL42zHZzPL+7XbkXeI6XznObm4V73cr82Luj3hWblYp2dy+4kL+FwLN7Q8Srna8+NwYvzORj3F+ejN+YvM7M3a41Du/M2X7pLn78ZTnRxb3DuQOGMws8vcwsIJlos0P2ckdl4LkNyv3UZ0tzncF4Ln2cnn19CluOXTPKQzsd2iyG9OEcd/oZ4Cf7tXAxp3M/Nh/QSHr6cJ764s7xcLDQrlyG5B8/TJN5tv15HMDhLY/BgSM1zHNit8Bhp+n9hM/j/sHobIvjaC6nxHmt133DbWvr39mLi2ftX+P2MzK1zsJ7h/f6XkC4XsDSI81tw2rrzIT1LSus8RroppKub+veQrt4HZtfMQkgz6+mPlz6k/AYxhRVCevFHdvmkz/DDi8/eP6vzIf0W4dI9bXah1+5nanMhzRzaOcdzM4d2czfYufZsQAshDZfn/k3iZENM3ilm58vhp+M3X9wfBp/D09/nL/3lvMx89hcQTDyePg4X6t0rMjsw8U55O+v/Ekwz3f/DM+FLN3i6cnMxBtNc/G15nn6cNBjzudmbJXhCNtK4PsXmSeSBsWO4r1s3pJg7erbsQx7YBE9zePYXK++Rzs/G/3GXfz6Au3lGEZaas1tv3vw0V699+QPjY/mAXF9rBxSFkIAICAmIgJCACDII6d9TLjoFRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1njqk19dX2bJTYCR1njmk19cstv+/8nUjGWle6s3xoCcO6fU1k5KiYSR1CEm09BQYSR1CEi09BUZS54lDyuUxUjyMpM4zh2TuXBMjqfPUIVl79oM9kg4hGcJjJB1CMoSQdAjJEELSISRDeIykQ0iGcNZOh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAzhLYt1CMkQ3kRfh5AM4c+66BCSIYSkQ0iGEJLODSHVJ+7n2AgpFh4j6fweUj18qKcvIiOkWDhrp0NIhjCSOjc+RiKkEjCSOg+H9A+uJBvnXoSkc1tIdcMeqQCMpA4hGcJI6twUUu1/iIzNHwsjqXNLSPX5IyHljJHUueUJWecTIeWMkdS54XmkenhJA69syB0jqcNr7QxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdQgpgfC9aZc+x2ZvJMtBSPGFb6m59Dk6cyNZEEKKj5CeECElQkjPhZASuT0k9ZvEZibdJkmKkNJw31KTPdITIKQ0COnJEFIS3nvTEtITIKQU/PemJaQnQEgJBO9NS0hPgJDiu3hvWvuvbOBPXxKSIbKbwx9jJiRLVDfn9ZWSCMkQQtIhJEMISYeQDOExkg4hGcJZOx1CMoSR1CEkQxhJHUIyhJHUISRDGEkdQjKEkdQhJEMYSR1CMoSR1CEkQxhJHUIyhJHUISRDGEkdQjKEkdQhJEMYSR1CMoSR1CEkQxhJHUIyhF+j0CEkQ/jFPh1CMoRfNdchJEMISYeQDCEkHUIyhMdIOoRkCGftdAjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQDGEkdfQhKc+cGtv8nP7WkYckfS6PkOLgCVl5SNpXlxBSFLxEiJBMISQdQjKEkHTUIfEYKSIeI+nIQ+KsXTyctdPRh8SzH9EwkjqEZAgjqUNIhjCSOoRkCCOpQ0iGMJI6hGQII6lDSIYwkjqEZAgjqUNIhjCSOoRkCCOpQ0iGMJI6hGQII6lDSIYwkjqEZAgjqUNIhjCSOoRkCCOpQ0iGMJI6hGQII6lDSIYwkjoPh/QPrpjb5GGEpMMeyRBGUoeQDGEkdQjJEEZSh5AMYSR1CMkQRlKHkAxhJHUIyRBGUoeQ5NRPgWVGvTkeREiGMJI6hGQII6lDSIYwkjqEZAgjqUNIhjCSOoRkCCOpQ0iGMJI6hGQII6lDSIYwkjqEZAgjqUNIhjCSOoRkiO7mvL6+ypadB0IyRHZzXl+fviRCMkR1c15fKYmQDCEkHUIyhJB0CMkQHiPpEJIhnLXTISRDCEmHkAzh0E6HkAzhZIMOIRlCSDqEZAgh6RCSITxG0iEkQzhrp0NIhjCSOs8dkrF/RglJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQjKEkHQIyRBC0iEkQwhJh5AMISQdQkqi7j+2hs/NzOfYCEmHkFIYOhlqqYcP4efoCEmHkBKom2cL6fXV2FHy3QgpCa8V+yG9vj59SRmEJNwCiUMaHyIN31kI6V/xXntxZpZqkyRGSEnU7gfze6QhJNHS80BISdTuJUJ6AoSUxHOFxGMkQkrkuQ7tOGtHSIlMrdxwsiEenkfSIaQkzq9suPY5NkLSuSkk/wUvsVkMSYOQdG4JyX/BS3SEFAsh6dwQUvCCl+gIKRZC0rn90C5VR4QUDSHp3BGS/xApzutBWpFeWqJddJqtcydC0rlzj8TJhpwRks4dZ+38S/EQUiyEpENIhhCSDod2hhCSzn0hJTlzR0ixEJLOna9sSLEKhBQLIenwWjtDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdC5C+nirqmb7s+IqEFIshKQThHTcVCdNVX2vtwqEFAsh6QQh7ar9qaLms9qutwqEFAsh6QQhnSKa/l8LIcVCSDqEZAgh6cwf2u2r3XqrQEixEJJOeLKhrjr1Yb1VIKRYhPdmYyN5v4tDuPdNVW32xxVXgZBiISQdnpA1hJB0CMkQQtK5OGs3Wm8VCCkWQtIhJEMISWc2mMP2fcVVIKRYCElnfs9zrFYsiZBiISSdhUM4Du1KREg688F8VvV6q0BIsRCSztLJhv16q0BIsTxBSMePt7rafjxy1c+3qqp3qX4/aD6kesWOCCka+yH9jC9hu/+lN9u0uwiekDXEfkibandszyrfn8O22p52RsfPukrz29+EZIj9kIaTYMfu8+FtOHQ67Kpqd+h+/FNvTz9uv/b3WV/VZrywm6ZzrjfO/PT/W7V94CXbbkiV6/5ZPYqQYrEf0lv1NV3uf1PhbbzQHu1V1bbtpPt6411xN12xDayfzr1e04wh7R47ciQkuX8WvEab09WxOtTVZv/Z7zDaX5r7bu+o+/aNEbqjvf4R0Hv7cV95ZyRq7w7dT+der/9ul9ixeeDIkUM7S+zvkZpj+2s+1aY997aphv3GpjqVdWh3QVV76fR1+912X3U2ptLvI8bpztcbp6nah1CHYHd2C0Iy5AlCOvnZ77bVp/OiAe8RTnM+snKvFIY0fz3/J3cJr7Hn0K5czxFSq33BwF0hnR9crRPS1BEhFch+SNVwONfeP2cP7fqvL684nbVzcgmud5gO+g4PvBtdsMy6+tlWh+OWN4gskf2Q9v2zQfv2AdC+2jc/4cmGfqJ9c/HWjNtq0+6Tvt7OIZ2vV58OFU93+v5kQ3vp/tdsX74d1/tpJ3jkDSJLZD+kZnN+c57DeJY7PI09vINP8MTr+MqG+tufrr1edyD2PoTUfu/+FbsM6as9b8ihXYmeIKTmY9u+gq07qDsdPPXPpwZPrHZfby+Oqb52p3DePht/uu56zb4+7YSGQ7vt8L37BMG8VZ/tQeM3IZXoGUJK7OE7fnC9tqBuH8gbRBaIkP4sVkjN16Z9OcWav0VBSNEQku+B1+lEC0mAkGIhJJ8qpDXfXtVBSLEQko73otXtp6IlQoqFkHTckNpz9NuvxUlTIaRYCEnHOyQ8dC+t3a3cEiHFQkg64WMrQUuEFAsh6cycpOha4u24CkRIOvNn+754ZUOJCEmHPZIhhKTDYyRDCEmHs3aGPFlIWW298HmkN55HKhch6fDKBkMISYfX2hlCSDq8+tuQZwjpdd5ai19ESIY8RUj/zdFvSEIyhJB0CMkQQtJ5OKRob5oe8f3XhYuOuU0eRki9yvt0lfNauMq/ULkTdZNdnd/FDz/ad9DbpvljTPPYI8VCSL3bQ6pmLg8dBYX9VlLws2P3BnynufBOqwUipF6EkKrmbyHtqn3bYvh+r0kRUiyE1DuH1O4TpgS6C9X5jfaHj9U0rXvli7dOnfnezDKnLyv3XcbXQUiRKJ9Oyep5JCekoSIvluAzIRW96PikT0xmu0ea+dw8ENLFCYjQ/KHdnndaLY72Kf5CQhr3ENNfHGsShTS8kX/3bv9rIaQoCGlyJaTz0d405ZWQpveW/P30xcWP2l9J2uzXfP0qIUVBSJOrIV0c4t2wR1p43DSzTCVCioPHSBP/hPV4CHflZEN1NaTq4sKl4EfbFR8bjQgpkuc4a3dLSOez3OOH8Ty3f/q7GS+7e6SLw7jp7cOvnYILflQL9lCEFMtTPI+U/NcolhO4Fkfws5/tfsXTDD1CiuUZQkq/zCghPfCHMP6MkGIhpCiW7vtXmyAkQwhJh7N2hjxZSFkhJEMISYdDO0MISYeQDCEkndlgDtv3FVeBkGIhJJ35Pc+xWrEkQoqFkHQWDuE4tCvRk4WU1dabD+aTv49UIkLSWTrZsF9vFQgpFkLSmQ+pXrEjQoqGkHSsPSG78OLgOGKuaArPEFKum8ZcSP9LR7+1fvEUIWW6aS4O7frPdaknGwhJg5Ccy3VVFf/KBkLSyCqkK29Vsvxe3+GvxN55tOZO/OF09HHPTP6GkGIhpN5ySFfe0WQmsXtKWji0WxMhxUJIvUdCuniz7/kZLOJkAyHFkGtIt77Xt3skN3P097tw0j2PkQjpAVmd/l5+X7vF97HzQgrfF/IWwaRTR4RESPfIdI/kHaRdCalqwj1S1TR/CamufrbV4bgt9u8jEZJGpiHd+F7fM9X8MaTTwt6rr+ZY7N9HIiSNXENqbnqv7/EozDkU+3tIX+2pbw7tCOku+YZ0cYg3+xjp4sIfQ3qrPg/VpvkmJEK6S1YhBY97Zk42zLzXt3/hrycb2oK27Q6u1L+PREgaeYXk/D2Wm9/rO3hlg/ejG4STfm3avza25q8jEVI0hPSI5Vp4QlYeUv+a37ruX/y79Dm2pwgp+q9RZBxShF/9WfbLorMIaehl+LD0ObpnCCnBMpcauKuNi4k/3tqHST8PrdFo/m/YxFFCSHVDSEaXuSgI6bgZzqf/6QnZZw+pISSry1wUhDT8VfPPvz0hS0j3hPQvltNxb7R53b3saHO6656Wj8snZKf/H0dIij3SXx90/23hqgXngpCshPT301d/WrpoudmYP7Tb/+0JWUIipGcTnmwY3reh/tNfkiUkQno2F4dw75uq2uyPf5opIfEY6dmkeUKWkPqP676yQdgRIRFSkpA0nux5pKy4IUX73QlC0iAknYuQYtRESBqEpENIhBRD9mOTGiERUgzZj01qhERIMWQ/NqkREiHFkP3YpEZIhBRD9mOTmh9SpD/rQkgahKRDSIQUQ/ZjkxqvbCCkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjZAIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJ7aaQ/L/R/TtC0iAknVtCGv6m/fThV8qQUrrpxgsRks4NIdUNIRHSL7Ifm9RuP7QjpOzvLISk83BI/65IGtK1BZ8WnfIx0pXlJtk49yIkHXN7pJQh3XTjhQhJh5AIKYbsxyY1QiKkGLIfm9QIiZBiyH5sUiMkQooh+7FJjVc2GApJuILZj01qvNaOkApfdB4IiZAKX3QeCImQCl90HgiJkApfdB4IiZAKX3QeCImQCl90HgiJkApfdB4IiZAKX3QeCImQCl90HgiJkApfdB4IiZAKX3QeCImQCl90HgiJkApfdB4IiZAKX3QeCImQCl90HgiJkApfdB4IiZAKX3QeCImQCl90HghJHtL1d7y8xy9vnplSvEWnGeTkCEkeUjzskXQIiZAKX3QeCImQCl90HgiJkApfdB7ShKT72yqE9HSLzgMhEVLhi84DIRFS4YvOA4+RCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXngZAIqfBF54GQCKnwReeBkAip8EXnwVxIuvdd0SMkHWsh3bmiMWemR0g6hGQIIekQkiFxx+apj5LvRkiGxB2bhKdt8j9vczdCMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdNKEVMofTTS2PQlJ5+GQ/kXzGm9WukXH3CYPIySdJHuk+wgH1dj2JCQdQjKEkHQIKaG6NXxuZj7HRkg6hJRQ7XyqLz9HR0g6hJQQIRHSisyGVLufCYmQErMb0vgQqWmuhRTtFHzcJxLShrS83KSbJB1CSmcpIPZI7JFSMBtSh5AIaSWEFAsh6RBSOhzaEdKaTId0w8mGeAhJh5ASWnpFA69sIKQE7Ia0NkLSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1CMoSQdAjJEELSISRDCEmHkAwhJB1Cyprwbx8S0l0IKWvCv8ZLSHchpKwRUikIKWuEVApCyhohlYKQskZIpSCkrBFSKQgpa4RUCkLKGiGVgpCyRkilIKSsEVIpCClrhFQKQsoaIZWCkLJGSKUgpKwRUikIKWuEVApCyhohlYKQskZIpSCkrBFSKQgpa4RUCkLKGiGVgpCypgxJ+MYrBSKkrBFSKQgpa4RUCkLKGo+RSkFIWSOkUhBS1gipFISUNUIqBSFljZBKQUhZI6RSEFLWCKkUhJQ1QioFIWWNkEpBSFkjpFIQUtYIqRSElDVCKgUhZY2QSkFIWSOkUhBS1gipFISUNUIqBSFljZBKQUhZI6RSEFLWhL/uTUh3IaSsEVIpCClrhFQKQsoaj5FKQUhZI6RSEJLcvyuShnRtwadFJw1pebnqzfEgQsoae6RSEFLWCKkUhJQ1QioFIWWNkEpBSFkjpFIQUtYIqRSElDVCKgUhZY2QSnFHSHUrwSoQ0jJCKsU9ISVaBUJaRkilIKSsEVIpbg8pVUeEdAUhleKOkPyHSNdf8XiPX148mVK8RSfYNC1CKsWdeyRjJxuyf6kxIZXiztPfhLQuQioFIWWNkErBoV3WCKkU94WU5MwdIS0jpFLc+cqGFKtASMuUIQnfwahAz/1aO0KKtqIxZ1YiQsoaIZWCkLJGSKUgpKwRUikIKWuEVApCyhohlYKQskZIpSCkrBFSKQgpa4RUCkLKGiGVgpCyRkilIKSsEVIpCClrhFQKQsoaIZWCkLJGSKUgpKwRUikIKWuEVApCyhohlYKQskZIpSCkrBFSKQgpa8W8AQkhqVeAkOLhzxHoEJIhhKRDSIYQkg4hGUJIOoRkCCHpEJIhhKRDSIYQkg4hGUJIOoRkCCHpEJIhhKRDSIYQkg4hGUJIOoRkCCHpEJIhhKRDSIYQkg4hGUJIOoRkCCHpEJIhhGT3VDUAAAUOSURBVKRDSIYQkg4hGUJIOoRkCCHpEJIhhKRDSIYQkg4hGUJIOoRkCCHpEJIhhKSTQUjCe7OxkBhJHUIyhJHUISRDGEkdQhKoT1LM9/lGMh+EtL56+hDZ041kRghpfYRkECGtj5AMIqT1BSH9g0uySf6OkNbHHskgQlofIRlESOsjJIMIaX2EZBAhrY+QDCIkAV7ZYA8hGcJI6hCSIYykDiEZwkjqEJIhjKQOIRnCSOoQkiGMpA4hGcJI6hCSIYykDiEZwkjqEJIhjKQOIRnCSOoQkiGMpA4hGcJI6hCSIYykDiEZwkjqEJIhjKQOIRnCSOoQkiGMpE4GIQHlIyQgAkICIiAkIAJCAiIgJCACQgIiICQgAkICIiAkIAJCAiJIE1Ltfbo+6Xmi2r9QuxN1k93zJxyurMLyIp3J636ZSf5qxB0YyUKoQ6pnLg9bP9hId27/5VVYXuTcHUO8/RnJQmQbUt2sv/kvlnnfIlNgJAuROqS6cY4lugvDH9ka9vftx3qa1r1yOPB3/p07Z2a3LtI9/pg5ZpFgJAuRPKRh23vjHXxOvPlvXqS3+ae/qpdNSIxkztLvkWY+Nw9s/nu3RbAKNyyybtzN73+QYSQLsXZI46PQ2jmXk3jz37jImW2dweZnJAuxekhNcz7Sb37d/OFhwUOb/7ZF1sPdw/lLyRlsfkayEIKQ3HG/9d/R+X9bb1uFOxYZXMhg8zOShUj0yoZp4Nxxruvm4u4wTlNf3fyXm+fBVfhlkf6F2v+WBiNZhlQvEZpOlI4fxlOl/hnUZrzs/qMWbpPpmNx9mu/2VbhjkcEyvR+pMJJFyPG1dssDnmxTCBa5BkZyNYSkWuQaGMnV5BjS4pAn3BSCRa6BkVxLliEBpSEkIAJCAiIgJCACQrqqqsILvsO2qjYzk9/s862q6t33I+uGnBDSVb+FVFeV+5O7Q9pWvf1Da4d8ENJVv4UUfPvekLbV9rQzOn7W1c8DK4eMENJVfkjvdbX5aL867qpqd2y/2+6Q+onaj/3/h7eq3ruTOdecLrS+xsPCr2rXXvGn3p6OFtsrHaZlD7N9q7aHVW4xHkNIV3kh7btu2gy6I7rNUkj1eLA2Tna+5nkWrV31NVw6djPYnno6dleqj35Iu+F7yBUhXeWFdNrXNN9VfdqttJns2x66n4chbY/NRzjZcM3pQqeu/GW18e2rbXvIt/dne5rllgdSOSOkq7yQ6mrX70E2/V38bSGk8bjsPNl0zemCM/f+dMN4xU378dDv7s6z/Rm+h1wR0lVeSF+no65NX8n5zt9chnT+apxsuuZ0wZm7N69gZuEl5IqNc1Vw1u5nU9XfD4Q0XdO5cPI2PUYipNKxca7aDnf1r/aRS+vjfMzWOod0uAxp4w3uxxjCdOF81s65YnBod5gO+g7jKiBHhHTVR1W3JZ2OyD7aBzjfzU97pmDfPu7/bO/Y3Z29rj6b4/YypPNk0zWnC71ttelm/3a+4vlkgzvbbXvpXTIEuAkhXTe+9KDdG/Tnrk935/4UdXsGoLvzd99/vwzpPNl0zelCMPv+gLH9zvn0tzvbdjrjv9BTOEL6RftiuOrts7u8r6u6i6B90rR9TcK4F6lPbVyGdJ7sfM3pwuBrV4+zH474pidk3dketsP3kClCKgCnGfLHJioAIeWPTVQAQsofm6gAhJQ/NhEQASEBERASEAEhAREQEhABIQEREBIQASEBERASEMH/AQ2yIdKAgMQ8AAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Load All Necessary Packages ---\n",
    "if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
    "if (!requireNamespace(\"ggplot2\", quietly = TRUE)) install.packages(\"ggplot2\")\n",
    "if (!requireNamespace(\"lubridate\", quietly = TRUE)) install.packages(\"lubridate\")\n",
    "if (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\n",
    "if (!requireNamespace(\"quanteda\", quietly = TRUE)) install.packages(\"quanteda\")\n",
    "if (!requireNamespace(\"quanteda.textstats\", quietly = TRUE)) install.packages(\"quanteda.textstats\")\n",
    "\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "\n",
    "# --- Load Training Data ---\n",
    "df_utterance_train <- read.csv(\"git_ignore/dialogue_utterance_train.csv\")\n",
    "df_usefulness_train <- read.csv(\"git_ignore/dialogue_usefulness_train.csv\")\n",
    "\n",
    "# Merge dataframes and sort\n",
    "df_merged_train <- left_join(df_utterance_train, df_usefulness_train, by = \"Dialogue_ID\") %>%\n",
    "    mutate(Timestamp = ymd_hms(Timestamp)) %>%\n",
    "    arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "# --- Feature Engineering Functions ---\n",
    "# Function to calculate readability per utterance\n",
    "calculate_readability <- function(df) {\n",
    "    df$utterance_id <- paste0(\"utt_\", 1:nrow(df))\n",
    "    utterance_corpus <- corpus(df, text_field = \"Utterance_text\", docid_field = \"utterance_id\")\n",
    "    if (ndoc(utterance_corpus) > 0) {\n",
    "        readability_scores <- textstat_readability(utterance_corpus, measure = \"Flesch.Kincaid\") %>%\n",
    "            select(document, Flesch.Kincaid) %>%\n",
    "            rename(utterance_id = document, readability_score = Flesch.Kincaid)\n",
    "        df <- left_join(df, readability_scores, by = \"utterance_id\")\n",
    "        df$readability_score[is.na(df$readability_score)] <- 0\n",
    "    }\n",
    "    return(df)\n",
    "}\n",
    "\n",
    "# Function to engineer all 17 features per dialogue\n",
    "engineer_features <- function(df_with_readability) {\n",
    "    dialogue_features_df <- df_with_readability %>%\n",
    "        group_by(Dialogue_ID) %>%\n",
    "        summarise(\n",
    "            num_utterances = n(),\n",
    "            total_dialogue_length_words = sum(sapply(str_split(Utterance_text, \"\\\\s+\"), length), na.rm = TRUE),\n",
    "            dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "            avg_len_student_utterance_words = mean(sapply(str_split(Utterance_text[Interlocutor == \"Student\"], \"\\\\s+\"), length), na.rm = TRUE),\n",
    "            avg_len_chatbot_utterance_words = mean(sapply(str_split(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\s+\"), length), na.rm = TRUE),\n",
    "            num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "            num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE),\n",
    "            all_student_words = list(unlist(str_split(paste(Utterance_text[Interlocutor == \"Student\"], collapse = \" \"), \"\\\\s+\"))),\n",
    "            all_chatbot_words = list(unlist(str_split(paste(Utterance_text[Interlocutor == \"Chatbot\"], collapse = \" \"), \"\\\\s+\"))),\n",
    "            avg_readability_score_student = mean(readability_score[Interlocutor == \"Student\"], na.rm = TRUE),\n",
    "            avg_readability_score_chatbot = mean(readability_score[Interlocutor == \"Chatbot\"], na.rm = TRUE),\n",
    "            time_diffs_raw = list(as.numeric(diff(Timestamp), units = \"secs\"))\n",
    "        ) %>%\n",
    "        mutate(\n",
    "            num_unique_words_student = sapply(all_student_words, function(x) length(unique(x[x != \"\" & !is.na(x)]))),\n",
    "            num_unique_words_chatbot = sapply(all_chatbot_words, function(x) length(unique(x[x != \"\" & !is.na(x)]))),\n",
    "            total_words_student = sapply(all_student_words, function(x) length(x[x != \"\" & !is.na(x)])),\n",
    "            total_words_chatbot = sapply(all_chatbot_words, function(x) length(x[x != \"\" & !is.na(x)])),\n",
    "            ttr_student = ifelse(total_words_student > 0, num_unique_words_student / total_words_student, 0),\n",
    "            ttr_chatbot = ifelse(total_words_chatbot > 0, num_unique_words_chatbot / total_words_chatbot, 0),\n",
    "            variance_time_between_utterances = sapply(time_diffs_raw, function(x) ifelse(length(x) > 1, var(x, na.rm = TRUE), 0)),\n",
    "            ratio_student_chatbot_len_words = ifelse(avg_len_chatbot_utterance_words > 0, avg_len_student_utterance_words / avg_len_chatbot_utterance_words, Inf)\n",
    "        ) %>%\n",
    "        select(-all_student_words, -all_chatbot_words, -time_diffs_raw)\n",
    "\n",
    "    df_usefulness_scores_unique <- df_with_readability %>% select(Dialogue_ID, Usefulness_score) %>% distinct()\n",
    "    dialogue_features_df <- left_join(dialogue_features_df, df_usefulness_scores_unique, by = \"Dialogue_ID\")\n",
    "    return(dialogue_features_df)\n",
    "}\n",
    "\n",
    "# --- Execute Feature Engineering ---\n",
    "df_merged_train_readable <- calculate_readability(df_merged_train)\n",
    "dialogue_features_train_raw <- engineer_features(df_merged_train_readable)\n",
    "\n",
    "# --- Visualization & Statistical Tests ---\n",
    "# We select two promising features: total words and student readability.\n",
    "selected_features_vis <- c(\"total_dialogue_length_words\", \"avg_readability_score_student\")\n",
    "\n",
    "# Create groups for plotting\n",
    "plot_data <- dialogue_features_train_raw %>%\n",
    "  filter(Usefulness_score %in% c(1, 2, 4, 5)) %>%\n",
    "  mutate(Score_Group = ifelse(Usefulness_score %in% c(1, 2), \"Unuseful (1-2)\", \"Useful (4-5)\")) %>%\n",
    "  select(all_of(selected_features_vis), Score_Group) %>%\n",
    "  tidyr::gather(key = \"Feature\", value = \"Value\", -Score_Group)\n",
    "\n",
    "# Boxplot\n",
    "ggplot(plot_data, aes(x = Score_Group, y = Value, fill = Score_Group)) +\n",
    "    geom_boxplot() +\n",
    "    facet_wrap(~Feature, scales = \"free_y\") +\n",
    "    labs(title = \"Feature Distribution by Dialogue Usefulness\", x = \"Usefulness Group\", y = \"Feature Value\") +\n",
    "    theme_minimal()\n",
    "\n",
    "# T-tests\n",
    "cat(\"\\n--- Statistical Significance Tests ---\\n\")\n",
    "for (feature in selected_features_vis) {\n",
    "    group1_data <- dialogue_features_train_raw %>% filter(Usefulness_score %in% c(1, 2)) %>% pull(!!feature)\n",
    "    group2_data <- dialogue_features_train_raw %>% filter(Usefulness_score %in% c(4, 5)) %>% pull(!!feature)\n",
    "    if (length(na.omit(group1_data)) > 1 && length(na.omit(group2_data)) > 1) {\n",
    "        ttest_result <- t.test(group1_data, group2_data)\n",
    "        cat(paste(\"Feature:\", feature, \"- T-test p-value:\", round(ttest_result$p.value, 4), \"\\n\"))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "060878de",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline Model Performance (on NA-omitted data) ---\n",
      "                              Model      RMSE    Stage\n",
      "SVR                             SVR 0.9883981 Baseline\n",
      "Random Forest         Random Forest 0.9917741 Baseline\n",
      "Linear Regression Linear Regression 1.1116204 Baseline\n",
      "Regression Tree     Regression Tree 1.1935477 Baseline\n"
     ]
    }
   ],
   "source": [
    "# --- Load Packages and Data for Modeling ---\n",
    "if (!requireNamespace(\"caret\", quietly = TRUE)) install.packages(\"caret\")\n",
    "if (!requireNamespace(\"rpart\", quietly = TRUE)) install.packages(\"rpart\")\n",
    "if (!requireNamespace(\"randomForest\", quietly = TRUE)) install.packages(\"randomForest\")\n",
    "if (!requireNamespace(\"e1071\", quietly = TRUE)) install.packages(\"e1071\")\n",
    "\n",
    "library(caret)\n",
    "library(rpart)\n",
    "library(randomForest)\n",
    "library(e1071)\n",
    "\n",
    "# --- Load and Engineer Validation Data ---\n",
    "df_utterance_validation <- read.csv(\"git_ignore/dialogue_utterance_validation.csv\")\n",
    "df_usefulness_validation <- read.csv(\"git_ignore/dialogue_usefulness_validation.csv\")\n",
    "df_merged_validation <- left_join(df_utterance_validation, df_usefulness_validation, by = \"Dialogue_ID\") %>%\n",
    "    mutate(Timestamp = ymd_hms(Timestamp)) %>%\n",
    "    arrange(Dialogue_ID, Timestamp)\n",
    "df_merged_validation_readable <- calculate_readability(df_merged_validation)\n",
    "dialogue_features_validation_raw <- engineer_features(df_merged_validation_readable)\n",
    "\n",
    "# --- Baseline Data Prep: Remove rows with NA/Inf ---\n",
    "dialogue_features_train_baseline <- na.omit(dialogue_features_train_raw)\n",
    "dialogue_features_validation_baseline <- na.omit(dialogue_features_validation_raw)\n",
    "\n",
    "# --- Train All Four Baseline Models ---\n",
    "features_to_use <- setdiff(names(dialogue_features_train_baseline), c(\"Dialogue_ID\", \"Usefulness_score\"))\n",
    "formula_all <- as.formula(paste(\"Usefulness_score ~\", paste(features_to_use, collapse = \" + \")))\n",
    "RMSE <- function(y_true, y_pred) sqrt(mean((y_true - y_pred)^2))\n",
    "\n",
    "# Train models\n",
    "lm_model_baseline <- lm(formula_all, data = dialogue_features_train_baseline)\n",
    "rt_model_baseline <- rpart(formula_all, data = dialogue_features_train_baseline, method = \"anova\")\n",
    "set.seed(123)\n",
    "rf_model_baseline <- randomForest(formula_all, data = dialogue_features_train_baseline, ntree = 500)\n",
    "svr_model_baseline <- svm(formula_all, data = dialogue_features_train_baseline)\n",
    "\n",
    "# Evaluate models\n",
    "models_baseline <- list(\"Linear Regression\" = lm_model_baseline, \"Regression Tree\" = rt_model_baseline, \"Random Forest\" = rf_model_baseline, \"SVR\" = svr_model_baseline)\n",
    "rmse_baseline <- sapply(models_baseline, function(model) {\n",
    "    preds <- predict(model, newdata = dialogue_features_validation_baseline)\n",
    "    RMSE(dialogue_features_validation_baseline$Usefulness_score, preds)\n",
    "})\n",
    "performance_df_baseline <- data.frame(Model = names(rmse_baseline), RMSE = rmse_baseline, Stage = \"Baseline\")\n",
    "\n",
    "cat(\"\\n--- Baseline Model Performance (on NA-omitted data) ---\\n\")\n",
    "print(performance_df_baseline %>% arrange(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b4b65ea",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Improvement Stage 1: Advanced Data Processing ---\n",
      "Applying mandatory NA/Inf imputation...\n",
      "\n",
      "--- Evaluating all models on Cleaned & Transformed Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Model      RMSE   Stage\n",
      "Random Forest         Random Forest 0.9882938 Cleaned\n",
      "SVR                             SVR 0.9883981 Cleaned\n",
      "Linear Regression Linear Regression 1.1116204 Cleaned\n",
      "Regression Tree     Regression Tree 1.1935477 Cleaned\n",
      "\n",
      "\n",
      "--- Improvement Stage 2: Feature Selection ---\n",
      "Selected 10 features with importance > mean importance:\n",
      " [1] \"total_dialogue_length_words\"      \"ttr_chatbot\"                     \n",
      " [3] \"total_words_chatbot\"              \"total_words_student\"             \n",
      " [5] \"ttr_student\"                      \"num_unique_words_chatbot\"        \n",
      " [7] \"num_unique_words_student\"         \"dialogue_duration\"               \n",
      " [9] \"num_utterances\"                   \"variance_time_between_utterances\"\n",
      "\n",
      "--- Evaluating all models on Selected Features ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Model     RMSE          Stage\n",
      "Linear Regression Linear Regression 1.009699 Feat. Selected\n",
      "Random Forest         Random Forest 1.030625 Feat. Selected\n",
      "SVR                             SVR 1.043454 Feat. Selected\n",
      "Regression Tree     Regression Tree 1.275154 Feat. Selected\n",
      "\n",
      "\n",
      "--- Final Model Selection ---\n",
      "The best overall model is:\n",
      "                       Model      RMSE   Stage\n",
      "Random Forest1 Random Forest 0.9882938 Cleaned\n"
     ]
    }
   ],
   "source": [
    "# --- Improvement Stage 1: Advanced Data Processing ---\n",
    "cat(\"\\n\\n--- Improvement Stage 1: Advanced Data Processing ---\\n\")\n",
    "\n",
    "# Start with the full raw data from Step 1\n",
    "train_processed <- dialogue_features_train_raw\n",
    "validation_processed <- dialogue_features_validation_raw\n",
    "\n",
    "# Optional: Outlier Filtering using IQR method\n",
    "# To use this, uncomment the following block.\n",
    "# -------------------------------------------------------------------\n",
    "# cat(\"Applying optional outlier filtering...\\n\")\n",
    "# for (col in features_to_use) {\n",
    "#   if (is.numeric(train_processed[[col]])) {\n",
    "#     Q1 <- quantile(train_processed[[col]], 0.25, na.rm = TRUE)\n",
    "#     Q3 <- quantile(train_processed[[col]], 0.75, na.rm = TRUE)\n",
    "#     IQR <- Q3 - Q1\n",
    "#     lower_bound <- Q1 - 1.5 * IQR\n",
    "#     upper_bound <- Q3 + 1.5 * IQR\n",
    "#     train_processed[[col]][train_processed[[col]] < lower_bound | train_processed[[col]] > upper_bound] <- NA\n",
    "#   }\n",
    "# }\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Optional: Log Scaling for 'word count' and 'time' features\n",
    "# To use this, uncomment the following blocks.\n",
    "# -------------------------------------------------------------------\n",
    "# word_count_cols <- names(train_processed)[grepl(\"words|length|num_\", names(train_processed))]\n",
    "# cat(\"Applying optional log scaling to word count columns:\\n\", paste(word_count_cols, collapse=\", \"), \"\\n\")\n",
    "# train_processed <- train_processed %>% mutate(across(all_of(word_count_cols), ~ log1p(.x)))\n",
    "# validation_processed <- validation_processed %>% mutate(across(all_of(word_count_cols), ~ log1p(.x)))\n",
    "#\n",
    "# time_cols <- names(train_processed)[grepl(\"duration|time\", names(train_processed))]\n",
    "# cat(\"Applying optional log scaling to time columns:\\n\", paste(time_cols, collapse=\", \"), \"\\n\")\n",
    "# train_processed <- train_processed %>% mutate(across(all_of(time_cols), ~ log1p(.x)))\n",
    "# validation_processed <- validation_processed %>% mutate(across(all_of(time_cols), ~ log1p(.x)))\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Mandatory: Impute NA and Inf values (this is our main cleaning method)\n",
    "cat(\"Applying mandatory NA/Inf imputation...\\n\")\n",
    "for (col in features_to_use) {\n",
    "    # Handle Inf using training set statistics\n",
    "    is_inf_train <- is.infinite(train_processed[[col]])\n",
    "    if(any(is_inf_train)) {\n",
    "        max_finite_train <- max(train_processed[[col]][!is_inf_train], na.rm = TRUE)\n",
    "        train_processed[[col]][is_inf_train] <- max_finite_train + 1\n",
    "        validation_processed[[col]][is.infinite(validation_processed[[col]])] <- max_finite_train + 1\n",
    "    }\n",
    "    # Handle NA using training set statistics\n",
    "    if(any(is.na(train_processed[[col]]))) {\n",
    "        mean_val_train <- mean(train_processed[[col]], na.rm = TRUE)\n",
    "        train_processed[[col]][is.na(train_processed[[col]])] <- mean_val_train\n",
    "        validation_processed[[col]][is.na(validation_processed[[col]])] <- mean_val_train\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- Re-evaluate all models on the fully processed data ---\n",
    "cat(\"\\n--- Evaluating all models on Cleaned & Transformed Data ---\\n\")\n",
    "lm_model_cleaned <- lm(formula_all, data = train_processed)\n",
    "rt_model_cleaned <- rpart(formula_all, data = train_processed, method = \"anova\")\n",
    "set.seed(123)\n",
    "rf_model_cleaned <- randomForest(formula_all, data = train_processed, ntree = 500, importance = TRUE)\n",
    "svr_model_cleaned <- svm(formula_all, data = train_processed)\n",
    "\n",
    "models_cleaned <- list(\"Linear Regression\" = lm_model_cleaned, \"Regression Tree\" = rt_model_cleaned, \"Random Forest\" = rf_model_cleaned, \"SVR\" = svr_model_cleaned)\n",
    "rmse_cleaned <- sapply(models_cleaned, function(model) {\n",
    "    preds <- predict(model, newdata = validation_processed)\n",
    "    RMSE(validation_processed$Usefulness_score, preds)\n",
    "})\n",
    "performance_df_cleaned <- data.frame(Model = names(rmse_cleaned), RMSE = rmse_cleaned, Stage = \"Cleaned\")\n",
    "print(performance_df_cleaned %>% arrange(RMSE))\n",
    "\n",
    "# --- Improvement Stage 2: Feature Selection ---\n",
    "cat(\"\\n\\n--- Improvement Stage 2: Feature Selection ---\\n\")\n",
    "# Use the clean RF model to get importance.\n",
    "importance_scores <- importance(rf_model_cleaned, type = 1)\n",
    "importance_df <- data.frame(Feature = rownames(importance_scores), Importance = importance_scores[,1]) %>% arrange(desc(Importance))\n",
    "\n",
    "# New selection logic: Select features with importance > mean importance\n",
    "mean_importance <- mean(importance_df$Importance)\n",
    "selected_features <- as.character(importance_df$Feature[importance_df$Importance > mean_importance])\n",
    "cat(paste(\"Selected\", length(selected_features), \"features with importance > mean importance:\\n\"))\n",
    "print(selected_features)\n",
    "\n",
    "# --- Re-evaluate all models on the selected feature set ---\n",
    "formula_selected <- as.formula(paste(\"Usefulness_score ~\", paste(selected_features, collapse = \" + \")))\n",
    "cat(\"\\n--- Evaluating all models on Selected Features ---\\n\")\n",
    "lm_model_selected <- lm(formula_selected, data = train_processed)\n",
    "rt_model_selected <- rpart(formula_selected, data = train_processed, method = \"anova\")\n",
    "set.seed(123)\n",
    "rf_model_selected <- randomForest(formula_selected, data = train_processed, ntree = 500)\n",
    "svr_model_selected <- svm(formula_selected, data = train_processed)\n",
    "\n",
    "models_selected <- list(\"Linear Regression\" = lm_model_selected, \"Regression Tree\" = rt_model_selected, \"Random Forest\" = rf_model_selected, \"SVR\" = svr_model_selected)\n",
    "rmse_selected <- sapply(models_selected, function(model) {\n",
    "    preds <- predict(model, newdata = validation_processed)\n",
    "    RMSE(validation_processed$Usefulness_score, preds)\n",
    "})\n",
    "performance_df_selected <- data.frame(Model = names(rmse_selected), RMSE = rmse_selected, Stage = \"Feat. Selected\")\n",
    "print(performance_df_selected %>% arrange(RMSE))\n",
    "\n",
    "\n",
    "# --- Final Model Selection ---\n",
    "cat(\"\\n\\n--- Final Model Selection ---\\n\")\n",
    "full_performance <- rbind(performance_df_baseline, performance_df_cleaned, performance_df_selected)\n",
    "best_run <- full_performance[which.min(full_performance$RMSE), ]\n",
    "cat(\"The best overall model is:\\n\")\n",
    "print(best_run)\n",
    "\n",
    "# Store the final best model object and its feature list for Steps 4 & 5\n",
    "best_overall_model <- NULL\n",
    "final_feature_list <- NULL\n",
    "if(best_run$Stage == \"Baseline\") {\n",
    "    best_overall_model <- models_baseline[[best_run$Model]]\n",
    "    final_feature_list <- features_to_use\n",
    "} else if (best_run$Stage == \"Cleaned\") {\n",
    "    best_overall_model <- models_cleaned[[best_run$Model]]\n",
    "    final_feature_list <- features_to_use\n",
    "} else { # Feat. Selected\n",
    "    best_overall_model <- models_selected[[best_run$Model]]\n",
    "    final_feature_list <- selected_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c60ae324",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Improvement Step: Data Cleaning (NA/Inf Imputation) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Performance after Data Cleaning ---\n",
      "                              Model      RMSE   Stage\n",
      "SVR                             SVR 0.9883981 Cleaned\n",
      "Random Forest         Random Forest 0.9917741 Cleaned\n",
      "Linear Regression Linear Regression 1.1116204 Cleaned\n",
      "Regression Tree     Regression Tree 1.1935477 Cleaned\n",
      "\n",
      "\n",
      "--- Further Experiment: Cleaning + Outlier Filtering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Model     RMSE           Stage\n",
      "Random Forest         Random Forest 1.003032 Cleaned+Outlier\n",
      "SVR                             SVR 1.017961 Cleaned+Outlier\n",
      "Regression Tree     Regression Tree 1.118453 Cleaned+Outlier\n",
      "Linear Regression Linear Regression 1.761196 Cleaned+Outlier\n",
      "\n",
      "\n",
      "--- Further Experiment: Cleaning + Feature Selection ---\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m in `rename()`:\u001b[22m\n\u001b[33m!\u001b[39m Can't rename columns that don't exist.\n\u001b[31m✖\u001b[39m Column `MeanDecreaseAccuracy` doesn't exist.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m in `rename()`:\u001b[22m\n\u001b[33m!\u001b[39m Can't rename columns that don't exist.\n\u001b[31m✖\u001b[39m Column `MeanDecreaseAccuracy` doesn't exist.\nTraceback:\n",
      "1. arrange(., desc(Importance))",
      "2. rename(., Importance = MeanDecreaseAccuracy)",
      "3. rename.data.frame(., Importance = MeanDecreaseAccuracy)",
      "4. tidyselect::eval_rename(expr(c(...)), .data)",
      "5. rename_impl(data, names(data), as_quosure(expr, env), strict = strict, \n .     name_spec = name_spec, allow_predicates = allow_predicates, \n .     error_call = error_call)",
      "6. eval_select_impl(x, names, {\n .     {\n .         sel\n .     }\n . }, strict = strict, name_spec = name_spec, type = \"rename\", allow_predicates = allow_predicates, \n .     error_call = error_call)",
      "7. with_subscript_errors(out <- vars_select_eval(vars, expr, strict = strict, \n .     data = x, name_spec = name_spec, uniquely_named = uniquely_named, \n .     allow_rename = allow_rename, allow_empty = allow_empty, allow_predicates = allow_predicates, \n .     type = type, error_call = error_call), type = type)",
      "8. withCallingHandlers(expr, vctrs_error_subscript = function(cnd) {\n .     cnd$subscript_action <- subscript_action(type)\n .     cnd$subscript_elt <- \"column\"\n .     cnd_signal(cnd)\n . })",
      "9. vars_select_eval(vars, expr, strict = strict, data = x, name_spec = name_spec, \n .     uniquely_named = uniquely_named, allow_rename = allow_rename, \n .     allow_empty = allow_empty, allow_predicates = allow_predicates, \n .     type = type, error_call = error_call)",
      "10. walk_data_tree(expr, data_mask, context_mask)",
      "11. eval_c(expr, data_mask, context_mask)",
      "12. reduce_sels(node, data_mask, context_mask, init = init)",
      "13. walk_data_tree(new, data_mask, context_mask)",
      "14. as_indices_sel_impl(out, vars = vars, strict = strict, data = data, \n  .     allow_predicates = allow_predicates, call = error_call, arg = as_label(expr))",
      "15. as_indices_impl(x, vars, call = call, arg = arg, strict = strict)",
      "16. chr_as_locations(x, vars, call = call, arg = arg)",
      "17. vctrs::vec_as_location(x, n = length(vars), names = vars, call = call, \n  .     arg = arg)",
      "18. (function () \n  . stop_subscript_oob(i = i, subscript_type = subscript_type, names = names, \n  .     subscript_action = subscript_action, subscript_arg = subscript_arg, \n  .     call = call))()",
      "19. stop_subscript_oob(i = i, subscript_type = subscript_type, names = names, \n  .     subscript_action = subscript_action, subscript_arg = subscript_arg, \n  .     call = call)",
      "20. stop_subscript(class = \"vctrs_error_subscript_oob\", i = i, subscript_type = subscript_type, \n  .     ..., call = call)",
      "21. abort(class = c(class, \"vctrs_error_subscript\"), i = i, ..., \n  .     call = call)",
      "22. signal_abort(cnd, .file)",
      "23. signalCondition(cnd)",
      "24. (function (cnd) \n  . {\n  .     cnd$subscript_action <- subscript_action(type)\n  .     cnd$subscript_elt <- \"column\"\n  .     cnd_signal(cnd)\n  . })(structure(list(message = \"\", trace = structure(list(call = list(\n  .     IRkernel::main(), kernel$run(), handle_shell(), executor$execute(msg), \n  .     tryCatch(evaluate(request$content$code, envir = .GlobalEnv, \n  .         output_handler = oh, stop_on_error = 1L), interrupt = function(cond) {\n  .         log_debug(\"Interrupt during execution\")\n  .         interrupted <<- TRUE\n  .     }, error = .self$handle_error), tryCatchList(expr, classes, \n  .         parentenv, handlers), tryCatchOne(tryCatchList(expr, \n  .         names[-nh], parentenv, handlers[-nh]), names[nh], parentenv, \n  .         handlers[[nh]]), doTryCatch(return(expr), name, parentenv, \n  .         handler), tryCatchList(expr, names[-nh], parentenv, handlers[-nh]), \n  .     tryCatchOne(expr, names, parentenv, handlers[[1L]]), doTryCatch(return(expr), \n  .         name, parentenv, handler), evaluate(request$content$code, \n  .         envir = .GlobalEnv, output_handler = oh, stop_on_error = 1L), \n  .     withRestarts(with_handlers({\n  .         for (expr in tle$exprs) {\n  .             ev <- withVisible(eval(expr, envir))\n  .             watcher$capture_plot_and_output()\n  .             watcher$print_value(ev$value, ev$visible, envir)\n  .         }\n  .         TRUE\n  .     }, handlers), eval_continue = function() TRUE, eval_stop = function() FALSE), \n  .     withRestartList(expr, restarts), withOneRestart(withRestartList(expr, \n  .         restarts[-nr]), restarts[[nr]]), doWithOneRestart(return(expr), \n  .         restart), withRestartList(expr, restarts[-nr]), withOneRestart(expr, \n  .         restarts[[1L]]), doWithOneRestart(return(expr), restart), \n  .     with_handlers({\n  .         for (expr in tle$exprs) {\n  .             ev <- withVisible(eval(expr, envir))\n  .             watcher$capture_plot_and_output()\n  .             watcher$print_value(ev$value, ev$visible, envir)\n  .         }\n  .         TRUE\n  .     }, handlers), eval(call), eval(call), withCallingHandlers(code, \n  .         message = `<fn>`, warning = `<fn>`, error = `<fn>`), \n  .     withVisible(eval(expr, envir)), eval(expr, envir), eval(expr, \n  .         envir), importance(rf_for_importance, type = 1) %>% as.data.frame() %>% \n  .         rename(Importance = MeanDecreaseAccuracy) %>% arrange(desc(Importance)), \n  .     arrange(., desc(Importance)), rename(., Importance = MeanDecreaseAccuracy), \n  .     rename.data.frame(., Importance = MeanDecreaseAccuracy), \n  .     tidyselect::eval_rename(expr(c(...)), .data), rename_impl(data, \n  .         names(data), as_quosure(expr, env), strict = strict, \n  .         name_spec = name_spec, allow_predicates = allow_predicates, \n  .         error_call = error_call), eval_select_impl(x, names, \n  .         {\n  .             {\n  .                 sel\n  .             }\n  .         }, strict = strict, name_spec = name_spec, type = \"rename\", \n  .         allow_predicates = allow_predicates, error_call = error_call), \n  .     with_subscript_errors(out <- vars_select_eval(vars, expr, \n  .         strict = strict, data = x, name_spec = name_spec, uniquely_named = uniquely_named, \n  .         allow_rename = allow_rename, allow_empty = allow_empty, \n  .         allow_predicates = allow_predicates, type = type, error_call = error_call), \n  .         type = type), withCallingHandlers(expr, vctrs_error_subscript = function(cnd) {\n  .         cnd$subscript_action <- subscript_action(type)\n  .         cnd$subscript_elt <- \"column\"\n  .         cnd_signal(cnd)\n  .     }), vars_select_eval(vars, expr, strict = strict, data = x, \n  .         name_spec = name_spec, uniquely_named = uniquely_named, \n  .         allow_rename = allow_rename, allow_empty = allow_empty, \n  .         allow_predicates = allow_predicates, type = type, error_call = error_call), \n  .     walk_data_tree(expr, data_mask, context_mask), eval_c(expr, \n  .         data_mask, context_mask), reduce_sels(node, data_mask, \n  .         context_mask, init = init), walk_data_tree(new, data_mask, \n  .         context_mask), as_indices_sel_impl(out, vars = vars, \n  .         strict = strict, data = data, allow_predicates = allow_predicates, \n  .         call = error_call, arg = as_label(expr)), as_indices_impl(x, \n  .         vars, call = call, arg = arg, strict = strict), chr_as_locations(x, \n  .         vars, call = call, arg = arg), vctrs::vec_as_location(x, \n  .         n = length(vars), names = vars, call = call, arg = arg), \n  .     `<fn>`(), stop_subscript_oob(i = i, subscript_type = subscript_type, \n  .         names = names, subscript_action = subscript_action, subscript_arg = subscript_arg, \n  .         call = call), stop_subscript(class = \"vctrs_error_subscript_oob\", \n  .         i = i, subscript_type = subscript_type, ..., call = call), \n  .     abort(class = c(class, \"vctrs_error_subscript\"), i = i, ..., \n  .         call = call)), parent = c(0L, 1L, 2L, 3L, 4L, 5L, 6L, \n  . 7L, 6L, 9L, 10L, 4L, 12L, 13L, 14L, 15L, 14L, 17L, 18L, 12L, \n  . 20L, 21L, 20L, 12L, 12L, 25L, 0L, 0L, 0L, 0L, 30L, 31L, 32L, \n  . 33L, 34L, 33L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 43L, 0L, 45L, \n  . 46L, 47L), visible = c(TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  . TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  . TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, \n  . TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE), namespace = c(\"IRkernel\", NA, \"IRkernel\", NA, \"base\", \n  . \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"evaluate\", \"base\", \n  . \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"evaluate\", \"base\", \n  . \"base\", \"base\", \"base\", \"base\", \"base\", NA, \"dplyr\", \"dplyr\", \n  . \"dplyr\", \"tidyselect\", \"tidyselect\", \"tidyselect\", \"tidyselect\", \n  . \"base\", \"tidyselect\", \"tidyselect\", \"tidyselect\", \"tidyselect\", \n  . \"tidyselect\", \"tidyselect\", \"tidyselect\", \"tidyselect\", \"vctrs\", \n  . \"vctrs\", \"vctrs\", \"vctrs\", \"rlang\"), scope = c(\"::\", NA, \"local\", \n  . NA, \"::\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \n  . \"::\", \"::\", \"local\", \"local\", \"local\", \"local\", \"local\", \"local\", \n  . \":::\", \"::\", \"::\", \"::\", \"::\", \"::\", \"::\", NA, \"::\", \"::\", \":::\", \n  . \"::\", \":::\", \":::\", \":::\", \"::\", \":::\", \":::\", \":::\", \":::\", \n  . \":::\", \":::\", \":::\", \":::\", \"::\", \"local\", \":::\", \":::\", \"::\"\n  . ), error_frame = c(FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, \n  . FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)), row.names = c(NA, \n  . -48L), version = 2L, class = c(\"rlang_trace\", \"rlib_trace\", \"tbl\", \n  . \"data.frame\")), parent = NULL, i = \"MeanDecreaseAccuracy\", subscript_type = \"character\", \n  .     names = character(0), subscript_action = NULL, subscript_arg = \"MeanDecreaseAccuracy\", \n  .     rlang = list(inherit = TRUE), call = rename(., Importance = MeanDecreaseAccuracy)), class = c(\"vctrs_error_subscript_oob\", \n  . \"vctrs_error_subscript\", \"rlang_error\", \"error\", \"condition\")))",
      "25. cnd_signal(cnd)",
      "26. signal_abort(cnd)",
      "27. signalCondition(cnd)"
     ]
    }
   ],
   "source": [
    "# --- Improvement: Data Cleaning ---\n",
    "# This is the step you identified as successfully improving performance over the baseline.\n",
    "cat(\"\\n\\n--- Improvement Step: Data Cleaning (NA/Inf Imputation) ---\\n\")\n",
    "\n",
    "# Start with the full raw data from Step 1\n",
    "train_cleaned <- dialogue_features_train_raw\n",
    "validation_cleaned <- dialogue_features_validation_raw\n",
    "\n",
    "# Apply mandatory NA and Inf imputation\n",
    "imputed_data <- apply_imputation(train_cleaned, validation_cleaned, features_to_use)\n",
    "train_cleaned <- imputed_data$train\n",
    "validation_cleaned <- imputed_data$valid\n",
    "\n",
    "# Evaluate all models on the cleaned data\n",
    "results_cleaned <- evaluate_performance(train_cleaned, validation_cleaned, features_to_use, \"Cleaned\")\n",
    "cat(\"--- Performance after Data Cleaning ---\\n\")\n",
    "print(results_cleaned$performance %>% arrange(RMSE))\n",
    "\n",
    "# --- Further Experiments (Analysis of Non-Improving Techniques) ---\n",
    "# Here, we show other techniques that were tried but did not yield better results than basic cleaning.\n",
    "\n",
    "# Experiment A: Outlier Filtering\n",
    "cat(\"\\n\\n--- Further Experiment: Cleaning + Outlier Filtering ---\\n\")\n",
    "train_temp_outlier <- train_cleaned # Start with cleaned data\n",
    "for (col in features_to_use) {\n",
    "  if (is.numeric(train_temp_outlier[[col]])) {\n",
    "    Q1 <- quantile(train_temp_outlier[[col]],0.25); Q3 <- quantile(train_temp_outlier[[col]],0.75)\n",
    "    IQR <- Q3 - Q1\n",
    "    train_temp_outlier[[col]][train_temp_outlier[[col]] < (Q1-1.5*IQR) | train_temp_outlier[[col]] > (Q3+1.5*IQR)] <- NA\n",
    "  }\n",
    "}\n",
    "imputed_data_outlier <- apply_imputation(train_temp_outlier, validation_cleaned, features_to_use)\n",
    "results_outlier <- evaluate_performance(imputed_data_outlier$train, imputed_data_outlier$valid, features_to_use, \"Cleaned+Outlier\")\n",
    "print(results_outlier$performance %>% arrange(RMSE))\n",
    "\n",
    "# Experiment B: Feature Selection\n",
    "cat(\"\\n\\n--- Further Experiment: Cleaning + Feature Selection ---\\n\")\n",
    "# We use the Random Forest model from the successful \"Cleaned\" stage to find important features.\n",
    "rf_for_importance <- results_cleaned$models[[\"Random Forest\"]]\n",
    "\n",
    "# importance_scores <- importance(rf_for_importance, type = 1)\n",
    "# importance_df <- data.frame(Feature = rownames(importance_scores), Importance = importance_scores[,1]) %>% arrange(desc(Importance))\n",
    "\n",
    "importance_df <- importance(rf_for_importance, type=1) %>% as.data.frame() %>%\n",
    "  rename(Importance = MeanDecreaseAccuracy) %>% arrange(desc(Importance))\n",
    "selected_features <- rownames(importance_df)[importance_df$Importance > mean(importance_df$Importance)]\n",
    "results_feat_selection <- evaluate_performance(train_cleaned, validation_cleaned, selected_features, \"Cleaned+FeatSelect\")\n",
    "print(results_feat_selection$performance %>% arrange(RMSE))\n",
    "\n",
    "\n",
    "# --- Final Model Selection ---\n",
    "cat(\"\\n\\n--- Final Performance Summary Across All Stages ---\\n\")\n",
    "full_performance <- rbind(performance_df_baseline, results_cleaned$performance, results_outlier$performance, results_feat_selection$performance)\n",
    "print(full_performance %>% arrange(RMSE))\n",
    "\n",
    "best_run <- full_performance[which.min(full_performance$RMSE), ]\n",
    "cat(\"\\n--- The Best Overall Model Found ---\\n\")\n",
    "print(best_run)\n",
    "\n",
    "# Store the final best model object, its feature list, and the validation data it needs\n",
    "best_overall_model <- NULL\n",
    "final_feature_list <- NULL\n",
    "validation_data_final <- NULL\n",
    "\n",
    "if(best_run$Stage == \"Baseline\") {\n",
    "    best_overall_model <- models_baseline[[best_run$Model]]\n",
    "    final_feature_list <- features_to_use\n",
    "    validation_data_final <- dialogue_features_validation_baseline\n",
    "} else if (best_run$Stage == \"Cleaned\") {\n",
    "    best_overall_model <- results_cleaned$models[[best_run$Model]]\n",
    "    final_feature_list <- features_to_use\n",
    "    validation_data_final <- validation_cleaned\n",
    "} else if (best_run$Stage == \"Cleaned+Outlier\") {\n",
    "    best_overall_model <- results_outlier$models[[best_run$Model]]\n",
    "    final_feature_list <- features_to_use\n",
    "    validation_data_final <- imputed_data_outlier$valid\n",
    "} else { # Cleaned+FeatSelect\n",
    "    best_overall_model <- results_feat_selection$models[[best_run$Model]]\n",
    "    final_feature_list <- selected_features\n",
    "    validation_data_final <- validation_cleaned\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59c0de58",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 4: Analyze a Specific Dialogue ---\n",
      "Selected Dialogue_ID: 5980 \n",
      "\n",
      "--- Prediction vs. Ground Truth ---\n",
      "Ground Truth Score: 4 \n",
      "Predicted Score: 3.66 \n",
      "\n",
      "Is the prediction close to the ground truth (within 0.5)? TRUE \n",
      "\n",
      "The model made a successful prediction. The features that likely played an important role are:\n",
      "\n",
      "To determine feature importance quantitatively, we can use model-specific methods:\n",
      "- For tree-based models (Random Forest, XGBoost), importance is calculated as the total reduction in node impurity (e.g., Gini impurity or variance) from splits made on that feature, averaged over all trees.\n",
      "- For linear models, the absolute magnitude of the scaled coefficients can be used.\n",
      "- For any model, permutation importance is a reliable method. It measures the decrease in model performance when a single feature's values are randomly shuffled.\n",
      "\n",
      "The importance scores from our model are:\n",
      "function (x, ...) \n",
      "UseMethod(\"importance\")\n",
      "<bytecode: 0x000001ca768657e8>\n",
      "<environment: namespace:randomForest>\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n\\n--- Step 4: Analyze a Specific Dialogue ---\\n\")\n",
    "\n",
    "# Determine which validation data to use based on the best model's stage\n",
    "validation_data_for_pred <- if(best_run$Stage == \"Baseline\") dialogue_features_validation_baseline else validation_processed\n",
    "\n",
    "# Select a random dialogue\n",
    "set.seed(42)\n",
    "random_dialogue_id <- sample(df_usefulness_validation$Dialogue_ID, 1)\n",
    "cat(paste(\"Selected Dialogue_ID:\", random_dialogue_id, \"\\n\"))\n",
    "\n",
    "# Get the feature vector for this dialogue\n",
    "dialogue_feature_vector <- validation_data_for_pred %>% filter(Dialogue_ID == random_dialogue_id)\n",
    "# dialogue_feature_vector <- validation_data_for_pred %>% filter(row.names(.) == random_dialogue_id)\n",
    "\n",
    "# *** FIX: Use the final_feature_list to subset the data before predicting ***\n",
    "predicted_score <- predict(best_overall_model, newdata = dialogue_feature_vector[, final_feature_list])\n",
    "ground_truth_score <- dialogue_feature_vector$Usefulness_score\n",
    "\n",
    "\n",
    "\n",
    "cat(\"\\n--- Prediction vs. Ground Truth ---\\n\")\n",
    "cat(paste(\"Ground Truth Score:\", ground_truth_score, \"\\n\"))\n",
    "cat(paste(\"Predicted Score:\", round(predicted_score, 2), \"\\n\"))\n",
    "\n",
    "# --- 3. Analysis ---\n",
    "# (The analysis and feature importance explanation text remains the same as the previous version)\n",
    "is_close <- abs(ground_truth_score - predicted_score) <= 0.5\n",
    "cat(paste(\"\\nIs the prediction close to the ground truth (within 0.5)?\", is_close, \"\\n\"))\n",
    "\n",
    "if (is_close) {\n",
    "    cat(\"\\nThe model made a successful prediction. The features that likely played an important role are:\\n\")\n",
    "} else {\n",
    "    cat(\"\\nThe prediction was not close to the ground truth. Possible reasons include:\\n\")\n",
    "    cat(\"- The model may not capture the specific nuances of this conversation.\\n\")\n",
    "    cat(\"- The dialogue might be an outlier or have unique characteristics not well-represented in the training data.\\n\")\n",
    "    cat(\"- The features, while generally useful, failed to describe what made this particular dialogue useful or unuseful.\\n\\n\")\n",
    "    cat(\"The most important features for the model are generally:\\n\")\n",
    "}\n",
    "\n",
    "# --- 4. Quantifying Feature Importance ---\n",
    "cat(\"\\nTo determine feature importance quantitatively, we can use model-specific methods:\\n\")\n",
    "cat(\"- For tree-based models (Random Forest, XGBoost), importance is calculated as the total reduction in node impurity (e.g., Gini impurity or variance) from splits made on that feature, averaged over all trees.\\n\")\n",
    "cat(\"- For linear models, the absolute magnitude of the scaled coefficients can be used.\\n\")\n",
    "cat(\"- For any model, permutation importance is a reliable method. It measures the decrease in model performance when a single feature's values are randomly shuffled.\\n\\n\")\n",
    "\n",
    "cat(\"The importance scores from our model are:\\n\")\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84dcca5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 4: Analyze a Specific Dialogue ---\n",
      "Selected Dialogue_ID: 5980 \n",
      "\n",
      "--- Prediction vs. Ground Truth ---\n",
      "Ground Truth Score: 4 \n",
      "Predicted Score: 3.66 \n",
      "\n",
      "--- Determining Feature Importance Quantitatively ---\n",
      "To determine feature importance, we can inspect the model. For tree-based models like Random Forest, importance is measured by how much a feature contributes to reducing impurity or prediction error when it is used to make a split in a tree. This is averaged across all trees.\n",
      "\n",
      "The best overall model is a Random Forest. Its feature importances are:\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "\u001b[1m\u001b[33mError\u001b[39m in `arrange()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `..1 = MeanDecreaseAccuracy`.\n\u001b[1mCaused by error:\u001b[22m\n\u001b[33m!\u001b[39m object 'MeanDecreaseAccuracy' not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[33mError\u001b[39m in `arrange()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[36mℹ\u001b[39m In argument: `..1 = MeanDecreaseAccuracy`.\n\u001b[1mCaused by error:\u001b[22m\n\u001b[33m!\u001b[39m object 'MeanDecreaseAccuracy' not found\nTraceback:\n",
      "1. arrange(., desc(MeanDecreaseAccuracy))",
      "2. arrange.data.frame(., desc(MeanDecreaseAccuracy))",
      "3. arrange_rows(.data, dots = dots, locale = .locale)",
      "4. mutate(data, `:=`(\"{name}\", !!dot), .keep = \"none\")",
      "5. mutate.data.frame(data, `:=`(\"{name}\", !!dot), .keep = \"none\")",
      "6. mutate_cols(.data, dplyr_quosures(...), by)",
      "7. withCallingHandlers(for (i in seq_along(dots)) {\n .     poke_error_context(dots, i, mask = mask)\n .     context_poke(\"column\", old_current_column)\n .     new_columns <- mutate_col(dots[[i]], data, mask, new_columns)\n . }, error = dplyr_error_handler(dots = dots, mask = mask, bullets = mutate_bullets, \n .     error_call = error_call, error_class = \"dplyr:::mutate_error\"), \n .     warning = dplyr_warning_handler(state = warnings_state, mask = mask, \n .         error_call = error_call))",
      "8. mutate_col(dots[[i]], data, mask, new_columns)",
      "9. mask$eval_all_mutate(quo)",
      "10. eval()",
      "11. .handleSimpleError(function (cnd) \n  . {\n  .     local_error_context(dots, i = frame[[i_sym]], mask = mask)\n  .     if (inherits(cnd, \"dplyr:::internal_error\")) {\n  .         parent <- error_cnd(message = bullets(cnd))\n  .     }\n  .     else {\n  .         parent <- cnd\n  .     }\n  .     message <- c(cnd_bullet_header(action), i = if (has_active_group_context(mask)) cnd_bullet_cur_group_label())\n  .     abort(message, class = error_class, parent = parent, call = error_call)\n  . }, \"object 'MeanDecreaseAccuracy' not found\", base::quote(NULL))",
      "12. h(simpleError(msg, call))",
      "13. abort(message, class = error_class, parent = parent, call = error_call)",
      "14. signal_abort(cnd, .file)",
      "15. signalCondition(cnd)"
     ]
    }
   ],
   "source": [
    "cat(\"\\n\\n--- Step 4: Analyze a Specific Dialogue ---\\n\")\n",
    "\n",
    "# Select a random dialogue\n",
    "set.seed(42)\n",
    "random_dialogue_id <- sample(df_usefulness_validation$Dialogue_ID, 1)\n",
    "cat(paste(\"Selected Dialogue_ID:\", random_dialogue_id, \"\\n\"))\n",
    "\n",
    "# Use the correctly processed validation set from the best experiment\n",
    "dialogue_feature_vector <- validation_data_final %>% filter(Dialogue_ID == random_dialogue_id)\n",
    "\n",
    "if (nrow(dialogue_feature_vector) > 0) {\n",
    "    # Predict using the final best model and its specific feature list\n",
    "    predicted_score <- predict(best_overall_model, newdata = dialogue_feature_vector[, final_feature_list])\n",
    "    ground_truth_score <- dialogue_feature_vector$Usefulness_score\n",
    "\n",
    "    cat(\"\\n--- Prediction vs. Ground Truth ---\\n\")\n",
    "    cat(paste(\"Ground Truth Score:\", ground_truth_score, \"\\n\"))\n",
    "    cat(paste(\"Predicted Score:\", round(predicted_score, 2), \"\\n\"))\n",
    "    \n",
    "    # --- Quantitative Feature Importance Analysis ---\n",
    "    cat(\"\\n--- Determining Feature Importance Quantitatively ---\\n\")\n",
    "    cat(\"To determine feature importance, we can inspect the model. For tree-based models like Random Forest, importance is measured by how much a feature contributes to reducing impurity or prediction error when it is used to make a split in a tree. This is averaged across all trees.\\n\\n\")\n",
    "\n",
    "    # FIX: Check if the best model is Random Forest. If so, get its importance.\n",
    "    # If not (e.g., SVR), explain why and use the RF from the same stage as a proxy.\n",
    "    if (inherits(best_overall_model, \"randomForest\")) {\n",
    "        cat(\"The best overall model is a Random Forest. Its feature importances are:\\n\")\n",
    "        importance_scores_final <- importance(best_overall_model, type=1) %>% \n",
    "            as.data.frame() %>%\n",
    "            rename(MeanDecreaseAccuracy = \"IncNodePurity\") %>%\n",
    "            arrange(desc(MeanDecreaseAccuracy))\n",
    "        print(importance_scores_final)\n",
    "    } else {\n",
    "        cat(paste(\"The best overall model is\", class(best_overall_model)[1], \"which does not have a direct feature importance method like Random Forest.\\n\"))\n",
    "        cat(\"However, we can use the feature importances from the Random Forest model trained in the same successful stage ('\", best_run$Stage, \"') as a strong indicator of feature relevance:\\n\", sep=\"\")\n",
    "        \n",
    "        # Find the corresponding RF model to get importance from\n",
    "        rf_proxy_model <- NULL\n",
    "        if(best_run$Stage == \"Cleaned\") {\n",
    "           rf_proxy_model <- results_cleaned$models[[\"Random Forest\"]]\n",
    "        } else if(best_run$Stage == \"Cleaned+FeatSelect\") {\n",
    "           rf_proxy_model <- results_feat_selection$models[[\"Random Forest\"]]\n",
    "        } else { # Baseline or other stages\n",
    "           rf_proxy_model <- models_baseline[[\"Random Forest\"]]\n",
    "        }\n",
    "        \n",
    "        if (!is.null(rf_proxy_model)) {\n",
    "            importance_scores_proxy <- importance(rf_proxy_model, type=1) %>% \n",
    "                as.data.frame() %>%\n",
    "                rename(MeanDecreaseAccuracy = \"IncNodePurity\") %>%\n",
    "                arrange(desc(MeanDecreaseAccuracy))\n",
    "            print(importance_scores_proxy)\n",
    "        }\n",
    "    }\n",
    "\n",
    "} else {\n",
    "    cat(\"\\nCould not find the selected dialogue in the final processed validation set.\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b93ce9e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named numeric(0)\n",
      "numeric(0)\n"
     ]
    }
   ],
   "source": [
    "print(predicted_score)\n",
    "print(ground_truth_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7642aa",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# # This code now uses `best_overall_model` and `validation_data_final` from Step 3.\n",
    "\n",
    "# # Select a random dialogue from the validation set\n",
    "# set.seed(42) # for reproducibility\n",
    "# random_dialogue_id <- sample(df_usefulness_validation$Dialogue_ID, 1)\n",
    "\n",
    "# cat(paste(\"Selected Dialogue_ID:\", random_dialogue_id, \"\\n\\n\"))\n",
    "\n",
    "# # --- 1. Dialogue Text ---\n",
    "# dialogue_text <- df_merged_validation %>%\n",
    "#     filter(Dialogue_ID == random_dialogue_id) %>%\n",
    "#     arrange(Timestamp)\n",
    "\n",
    "# cat(\"--- Full Dialogue Text ---\\n\")\n",
    "# for (i in 1:nrow(dialogue_text)) {\n",
    "#     cat(paste0(dialogue_text$Interlocutor[i], \": \", dialogue_text$Utterance_text[i], \"\\n\"))\n",
    "# }\n",
    "\n",
    "# # --- 2. Predict Usefulness with the BEST Model ---\n",
    "# # Get the feature vector for this dialogue.\n",
    "# # We filter from `validation_data_final` which has the correct feature set.\n",
    "# dialogue_feature_vector <- validation_data_final %>%\n",
    "#     # The dialogue_ID is stored in the row names of the feature dataframes.\n",
    "#     filter(row.names(.) == random_dialogue_id) \n",
    "\n",
    "# ground_truth_score <- dialogue_feature_vector$Usefulness_score\n",
    "\n",
    "# # Use the single best model for prediction.\n",
    "# predicted_score <- predict(best_overall_model, newdata = dialogue_feature_vector)\n",
    "\n",
    "# cat(\"\\n--- Prediction vs. Ground Truth ---\\n\")\n",
    "# cat(paste(\"Ground Truth Usefulness Score:\", ground_truth_score, \"\\n\"))\n",
    "# cat(paste(\"Predicted Usefulness Score:\", round(predicted_score, 2), \"\\n\"))\n",
    "\n",
    "# # --- 3. Analysis ---\n",
    "# # (The analysis and feature importance explanation text remains the same as the previous version)\n",
    "# is_close <- abs(ground_truth_score - predicted_score) <= 0.5\n",
    "# cat(paste(\"\\nIs the prediction close to the ground truth (within 0.5)?\", is_close, \"\\n\"))\n",
    "\n",
    "# # ... (rest of the text explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "072589c9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Step 5: Predict on Test Set ---\n",
      "Applying imputation to test data...\n",
      "\n",
      "Submission file saved as: LastName_StudentNumber_dialogue_usefulness_test.csv \n",
      "  Dialogue_ID Usefulness_score\n",
      "1        5427         4.217600\n",
      "2        5452         3.819000\n",
      "3        5454         3.592400\n",
      "4        5479         3.848867\n",
      "5        5522         3.423333\n",
      "6        5536         3.953133\n"
     ]
    }
   ],
   "source": [
    "cat(\"\\n\\n--- Step 5: Predict on Test Set ---\\n\")\n",
    "\n",
    "# --- 1. Load and Process Test Data ---\n",
    "df_utterance_test <- read.csv(\"git_ignore/dialogue_utterance_test.csv\")\n",
    "df_usefulness_test_orig <- read.csv(\"git_ignore/dialogue_usefulness_test.csv\")\n",
    "df_merged_test <- left_join(df_utterance_test, df_usefulness_test_orig, by = \"Dialogue_ID\") %>%\n",
    "    mutate(Timestamp = ymd_hms(Timestamp)) %>%\n",
    "    arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "# Engineer features\n",
    "df_merged_test_readable <- calculate_readability(df_merged_test)\n",
    "test_processed <- engineer_features(df_merged_test_readable)\n",
    "\n",
    "# --- 2. Apply the SAME processing pipeline as the best model ---\n",
    "# NOTE: This assumes the best pipeline was the \"Cleaned\" or \"Feat. Selected\" one.\n",
    "# If your best model is from the \"Baseline\" (unlikely), this part would need adjustment.\n",
    "# The optional log/outlier steps would also need to be applied here if you used them in Step 3.\n",
    "cat(\"Applying imputation to test data...\\n\")\n",
    "for (col in features_to_use) {\n",
    "    # Use the imputation values calculated from the TRAINING set in Step 3\n",
    "    mean_val_train <- mean(train_processed[[col]], na.rm = TRUE) # Assumes train_processed is available\n",
    "    test_processed[[col]][is.na(test_processed[[col]])] <- mean_val_train\n",
    "    if(any(is.infinite(test_processed[[col]]))){\n",
    "       max_finite_train <- max(train_processed[[col]][!is.infinite(train_processed[[col]])], na.rm=TRUE)\n",
    "       test_processed[[col]][is.infinite(test_processed[[col]])] <- max_finite_train + 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# --- 3. Make Predictions ---\n",
    "# *** FIX: Use the final_feature_list to subset the test data ***\n",
    "test_predictions <- predict(best_overall_model, newdata = test_processed[, final_feature_list])\n",
    "\n",
    "# --- 4. Create and Save Submission File ---\n",
    "# This should now work as test_predictions will have the correct length\n",
    "submission_df <- data.frame(\n",
    "    Dialogue_ID = test_processed$Dialogue_ID,\n",
    "    Usefulness_score = test_predictions\n",
    ")\n",
    "submission_df <- submission_df[match(df_usefulness_test_orig$Dialogue_ID, submission_df$Dialogue_ID), ]\n",
    "output_filename <- \"LastName_StudentNumber_dialogue_usefulness_test.csv\"\n",
    "write.csv(submission_df, output_filename, row.names = FALSE, quote = FALSE)\n",
    "\n",
    "cat(paste(\"\\nSubmission file saved as:\", output_filename, \"\\n\"))\n",
    "print(head(submission_df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
