{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c053c4",
   "metadata": {},
   "source": [
    "# Task A - Shell commands\n",
    "help me to explore and wrangle my data file, consumer_complaints.csv. use only shell script commands. the format of the columns in consumer_complaints.csv is:\n",
    "Column Name - Description \n",
    "Complaint ID - The unique identification number for a complaint \n",
    "Date_received - The date of the complaint received \n",
    "Product - The type of product the consumer identified in the complaint \n",
    "Sub-product - The type of sub-product the consumer identified in the complaint \n",
    "Issue - The issue the consumer identified in the complaint \n",
    "Sub-issue - The sub-issue the consumer identified in the complaint \n",
    "Consumer complaint narrative - Consumer complaint narrative is the consumer-submitted \n",
    "description of \"what happened\" from the complaint \n",
    "Company public response - Companies can choose to select a response from a pre-set list of \n",
    "options that will be posted on the public database \n",
    "Company - The complaint is about this company \n",
    "State - The state of the mailing address provided by the consumer \n",
    "ZIP code - The mailing ZIP code provided by the consumer \n",
    "Tags - Data that supports easier searching and sorting of complaints submitted by or on behalf of consumers. \n",
    "Consumer consent provided? - Identifies whether the consumer opted in to publish their complaint \n",
    "narrative. \n",
    "Submitted via - How the complaint was submitted \n",
    "\n",
    "with that, help me with the shell script commands to identify the following:\n",
    "1.  What is the Date_received range of the collected complaints?   \n",
    " \n",
    "2.  I want to preprocess the Complaint_ID and Date_received columns.   \n",
    "a.  Count lines with a complaint id that is not a number of 7 digits long, i.e., id values that contain anything other than numbers OR are of a length more/less than 7.   \n",
    "b.  Remove the lines mentioned in 2-a and remove time values in the Date_received \n",
    "column. For example, the Date_received column will contain “29/04/2020”, instead \n",
    "of having “29/04/2020 23:13”.   \n",
    "c.  Display  the  first  3  lines  (including  a  header)  of  the  dataset  that  was  filtered  in Question 2-b. Store the filtered dataset in a file named “filtered_complaints.csv” and use this file for the remaining questions in Task A.   \n",
    "3.  When  was  the  first  and  last  mention  of  the  term  “Student  loan”  in  the  column Consumer_complaint_narrative? Please note that the first and last mention of a term refers  to  the  chronologically  earliest  and latest paragraph containing the term in the dataset and the term to be searched is case sensitive.   \n",
    "4.  Let’s investigate the product column.   \n",
    "a.  How many unique values are there in the product column?   \n",
    "b.  Write commands to list the top 5 most frequent product values in the dataset \n",
    "(i.e., the top 5 products with the largest number of paragraphs)?   \n",
    "5.  Let’s investigate the Consumer complaint narrative column.   \n",
    "a.  How many complaints mention fraud in relation to a credit card? (Note: Please ignore \n",
    "cases and consider variations.)   \n",
    "b.  How many complaints are there about long wait times? (Note: Please ignore cases, \n",
    "consider variations, and include the time period waited.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4230c",
   "metadata": {},
   "source": [
    "cd /cygdrive/c/PersonalStuff/Monash/Sem2/FIT5145FoundationDataSci/Assignments/Assignment4/git_ignore\n",
    "\n",
    "# 1. What is the Date_received range of the collected complaints?\n",
    "# Get the Date_received column (assuming it's the 2nd column based on your description)\n",
    "# Skip the header row (NR > 1)\n",
    "# Extract the date part only (awk '{print $1}' to split by space and get first part of the date)\n",
    "# Sort in ascending order\n",
    "# Get the first and last unique dates\n",
    "# Earliest Date_received, excluding \"NA\" or \"N/A\"\n",
    "awk -F',' 'NR > 1 {print $2}' consumer_complaints.csv | \\\n",
    "awk '{if ($1 != \"NA\" && $1 != \"N/A\") print $1}' | \\\n",
    "sort -u | head -n 1\n",
    "\n",
    "# Latest Date_received, excluding \"NA\" or \"N/A\"\n",
    "awk -F',' 'NR > 1 {print $2}' consumer_complaints.csv | \\\n",
    "awk '{if ($1 != \"NA\" && $1 != \"N/A\") print $1}' | \\\n",
    "sort -u | tail -n 1\n",
    "\n",
    "# 2. Preprocess Complaint_ID and Date_received columns.\n",
    "# 2a. Count lines with a complaint id that is not a number of 7 digits long, i.e., id values that contain anything other than numbers OR are of a length more/less than 7.\n",
    "# Assuming Complaint ID is the 1st column\n",
    "awk -F',' 'NR > 1 {print $1}' consumer_complaints.csv | grep -vE '^[0-9]{7}$' | wc -l\n",
    "\n",
    "# 2b. Remove the lines mentioned in 2-a and remove time values in the Date_received column.\n",
    "\n",
    "# 2c. Display the first 3 lines (including a header) of the dataset that was filtered in Question 2-b. Store the filtered dataset in a file named “filtered_complaints.csv” and use this file for the remaining questions in Task A.\n",
    "# Get the header\n",
    "head -n 1 consumer_complaints.csv > filtered_complaints.csv\n",
    "\n",
    "# Filter data:\n",
    "# 1. Skip header (NR > 1)\n",
    "# 2. Check if Complaint ID (column 1) is exactly 7 digits\n",
    "# 3. Reformat Date_received (column 2) by removing time part\n",
    "# 4. Print the entire line, ensuring comma delimiters are maintained\n",
    "awk -F',' 'BEGIN {OFS = \",\"} NR > 1 {\n",
    "    if ($1 ~ /^[0-9]{7}$/) {\n",
    "        gsub(/ .*/, \"\", $2); # Remove space and everything after it from column 2\n",
    "        print $0;           # Print the entire modified line with original delimiters\n",
    "    }\n",
    "}' consumer_complaints.csv >> filtered_complaints.csv\n",
    "\n",
    "awk -F',' 'BEGIN {OFS = \",\"} NR > 1 {\n",
    "    if ($1 ~ /^[0-9]{7}$/) { # Q2a 7 digits long\n",
    "        gsub(/ .*/, \"\", $2); # Q2b also remove space and everything after it from column 2\n",
    "        print $0;           # Print the entire modified line with original delimiters\n",
    "    }\n",
    "}' consumer_complaints.csv >> filtered_complaints.csv\n",
    "\n",
    "# Display the first 3 lines of the filtered file\n",
    "head -n 3 filtered_complaints.csv\n",
    "\n",
    "# 3. When was the first and last mention of the term “Student loan” in the column Consumer_complaint_narrative?\n",
    "# Assuming Date_received is column 2 and Consumer complaint narrative is column 7\n",
    "# (adjust column numbers if necessary based on your file structure after filtering)\n",
    "\n",
    "# Find lines containing \"Student loan\" (case-sensitive)\n",
    "# Extract Date_received (column 2) and Consumer complaint narrative (column 7)\n",
    "# Remove time from Date_received for consistent sorting\n",
    "# Sort chronologically by date\n",
    "# Get the first and last occurrences\n",
    "\n",
    "# Earliest mention of \"Student loan\" (and variations)\n",
    "awk -F',' '{print $2, $7}' filtered_complaints.csv | \\\n",
    "grep -iE 'student[[:space:]_.-]*loans?' | \\\n",
    "awk '{gsub(/ .*/, \"\", $1); print $1, $0}' | sort -k1,1 | head -n 1\n",
    "\n",
    "# Latest mention of \"Student loan\" (and variations)\n",
    "awk -F',' '{print $2, $7}' filtered_complaints.csv | \\\n",
    "grep -iE 'student[[:space:]_.-]*loans?' | \\\n",
    "awk '{gsub(/ .*/, \"\", $1); print $1, $0}' | sort -k1,1 | tail -n 1\n",
    "\n",
    "# 4. Let’s investigate the Product column.\n",
    "# 4a. How many unique values are there in the product column?\n",
    "# Assuming Product is column 3\n",
    "awk -F',' 'NR > 1 {print $3}' filtered_complaints.csv | sort -u | wc -l\n",
    "\n",
    "# 4b. Write commands to list the top 5 most frequent product values in the dataset.\n",
    "# Assuming Product is column 3\n",
    "awk -F',' 'NR > 1 {print $3}' filtered_complaints.csv | sort | uniq -c | sort -rn | head -n 5\n",
    "\n",
    "# 5. Let’s investigate the Consumer complaint narrative column.\n",
    "# 5a. How many complaints mention fraud in relation to a credit card? (Note: Please ignore cases and consider variations.)\n",
    "# Assuming Consumer complaint narrative is column 7\n",
    "awk -F',' 'NR > 1 {print tolower($7)}' filtered_complaints.csv | \\\n",
    "grep -E 'fraud' | \\\n",
    "grep -E 'credit[[:space:]_.-]?card' | wc -l\n",
    "# 5b. How many complaints are there about long wait times? (Note: Please ignore cases, consider variations, and include the time period waited.)\n",
    "# Assuming Consumer complaint narrative is column 7\n",
    "awk -F',' 'NR > 1 {print tolower($7)}' filtered_complaints.csv | grep -E 'long wait time|wait time long|waited [0-9]+ (minutes?|hours?|days?|weeks?|months?|years?)' | wc -l\n",
    "\n",
    "\n",
    "\n",
    "echo \"--- Unique Product Categories ---\"\n",
    "# Assuming Product is column 3\n",
    "awk -F',' 'NR > 1 {print $3}' filtered_complaints.csv | sort -u\n",
    "\n",
    "echo \"\" # Add a blank line for readability\n",
    "\n",
    "echo \"--- Unique Sub-Product Categories ---\"\n",
    "# Assuming Sub-product is column 4\n",
    "awk -F',' 'NR > 1 {print $4}' filtered_complaints.csv | sort -u\n",
    "\n",
    "# Search for all Sub-products under a specific Product\n",
    "# Example: Find all sub-products for \"Credit reporting, credit repair services, or other personal consumer reports\"\n",
    "PRODUCT_NAME=\"debt collection\"\n",
    "awk -F',' -v product_name=\"$PRODUCT_NAME\" '\n",
    "NR > 1 && $3 == product_name {\n",
    "    print $4\n",
    "}' filtered_complaints.csv | sort -u\n",
    "\n",
    "\n",
    "# Search for all Products associated with a specific Sub-product\n",
    "# Example: Find all products associated with the sub-product \"Other (i.e. phone, health club, etc.)\"\n",
    "SUB_PRODUCT_NAME=\"Other (i.e. phone, health club, etc.)\"\n",
    "awk -F',' -v sub_product_name=\"$SUB_PRODUCT_NAME\" '\n",
    "NR > 1 && $4 == sub_product_name {\n",
    "    print $3\n",
    "}' filtered_complaints.csv | sort -u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ae1a7",
   "metadata": {},
   "source": [
    "\"Credit reporting and credit repair\"\n",
    "Credit repair services\n",
    "Credit reporting\n",
    "Other personal consumer report\n",
    "\"Credit reporting\"\n",
    "\"Credit reporting Credit reporting Credit reporting\"\n",
    "\n",
    "\"Credit card or prepaid card\"\n",
    "General-purpose credit card or charge card\n",
    "General-purpose prepaid card\n",
    "Gift card\n",
    "Government benefit card\n",
    "Payroll card\n",
    "Store credit card\n",
    "Student prepaid card\n",
    "\"Credit card\"\n",
    "\n",
    "\"Prepaid card\"\n",
    "Electronic Benefit Transfer / EBT card\n",
    "General purpose card\n",
    "Gift or merchant card\n",
    "Government benefit payment card\n",
    "ID prepaid card\n",
    "Mobile wallet\n",
    "Other special purpose card\n",
    "Payroll card\n",
    "Transit card\n",
    "\n",
    "\"Money transfer  virtual currency  or money service\"\n",
    "Check cashing service\n",
    "Debt settlement\n",
    "Domestic (US) money transfer\n",
    "Foreign currency exchange\n",
    "International money transfer\n",
    "Mobile or digital wallet\n",
    "Money order\n",
    "Refund anticipation check\n",
    "Traveler's check or cashier's check\n",
    "Virtual currency\n",
    "\"Money transfers\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer\n",
    "\"Virtual currency\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdd01d",
   "metadata": {},
   "source": [
    "my manual analysis has detected the following anomalies in the product vs sub-products. they are formatted as \"Product\" in double quotes, followed by a list of sub-products, and then a line break. can you make up the aliasing categories for these?\n",
    "the anomalies are:\n",
    "\"Credit reporting  credit repair services  or other personal consumer reports\"\n",
    "Conventional home mortgage\n",
    "Credit repair services\n",
    "Credit reporting\n",
    "Other personal consumer report\n",
    "\n",
    "\"Credit card or prepaid card\"\n",
    "General-purpose credit card or charge card\n",
    "General-purpose prepaid card\n",
    "Gift card\n",
    "Government benefit card\n",
    "Payroll card\n",
    "Store credit card\n",
    "Student prepaid card\n",
    "\n",
    "\"Credit card\"\n",
    "NA\n",
    "\n",
    "\"Prepaid card\"\n",
    "Electronic Benefit Transfer / EBT card\n",
    "General purpose card\n",
    "Gift or merchant card\n",
    "Government benefit payment card\n",
    "ID prepaid card\n",
    "Mobile wallet\n",
    "Other special purpose card\n",
    "Payroll card\n",
    "Transit card\n",
    "\n",
    "\"Credit reporting\"\n",
    "NA\n",
    "\n",
    "\"Credit reporting Credit reporting Credit reporting\"\n",
    "NA\n",
    "\n",
    "\"Money transfer  virtual currency  or money service\"\n",
    "Check cashing service\n",
    "Debt settlement\n",
    "Domestic (US) money transfer\n",
    "Foreign currency exchange\n",
    "International money transfer\n",
    "Mobile or digital wallet\n",
    "Money order\n",
    "Refund anticipation check\n",
    "Traveler's check or cashier's check\n",
    "Virtual currency\n",
    "\n",
    "\"Money transfers\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer\n",
    "\n",
    "\"Virtual currency\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer\n",
    "\n",
    "\"Other financial service\"\n",
    "Check cashing\n",
    "Credit repair\n",
    "Debt settlement\n",
    "Foreign currency exchange\n",
    "Money order\n",
    "Refund anticipation check\n",
    "Traveler’s/Cashier’s checks\n",
    "\n",
    "\n",
    "\"Payday loan  title loan  or personal loan\"\n",
    "Installment loan\n",
    "Pawn loan\n",
    "Payday loan\n",
    "Personal line of credit\n",
    "Title loan\n",
    "\n",
    "\"Payday loan\"\n",
    "NA\n",
    "\n",
    "\"Student loan\"\n",
    "Federal student loan servicing\n",
    "Non-federal student loan\n",
    "Private student loan\n",
    "\n",
    "\"Consumer Loan\"\n",
    "Installment loan\n",
    "Pawn loan\n",
    "Personal line of credit\n",
    "Title loan\n",
    "Vehicle lease\n",
    "Vehicle loan\n",
    "\n",
    "\"Vehicle loan or lease\"\n",
    "Lease\n",
    "Loan\n",
    "Title loan\n",
    "\n",
    "\"Bank account or service\"\n",
    "(CD) Certificate of deposit\n",
    "Cashing a check without an account\n",
    "Checking account\n",
    "Other bank product/service\n",
    "Savings account\n",
    "\n",
    "\n",
    "\"Checking or savings account\"\n",
    "CD (Certificate of Deposit)\n",
    "Checking account\n",
    "Other banking product or service\n",
    "Personal line of credit\n",
    "Savings account\n",
    "\n",
    "\"NA\"\n",
    "Checking account\n",
    "\n",
    "there's also a \"Debt collection\" and \"debt collection\" product categories, which obviously should be merged.\n",
    "there's a Mortgage product category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be97f2c",
   "metadata": {},
   "source": [
    "i've inspected the products vs sub-products. they are formatted as \"Product\" in double quotes, followed by a list of sub-products, and then a line break. can you make up the aliasing categories for these?\n",
    "the products vs sub-products are:\n",
    "\"Credit reporting  credit repair services  or other personal consumer reports\"\n",
    "Conventional home mortgage\n",
    "Credit repair services\n",
    "Credit reporting\n",
    "Other personal consumer report\n",
    "\n",
    "\"Credit card or prepaid card\"\n",
    "General-purpose credit card or charge card\n",
    "General-purpose prepaid card\n",
    "Gift card\n",
    "Government benefit card\n",
    "Payroll card\n",
    "Store credit card\n",
    "Student prepaid card\n",
    "\n",
    "\"Credit card\"\n",
    "NA\n",
    "\n",
    "\"Prepaid card\"\n",
    "Electronic Benefit Transfer / EBT card\n",
    "General purpose card\n",
    "Gift or merchant card\n",
    "Government benefit payment card\n",
    "ID prepaid card\n",
    "Mobile wallet\n",
    "Other special purpose card\n",
    "Payroll card\n",
    "Transit card\n",
    "\n",
    "\"Credit reporting\"\n",
    "NA\n",
    "\n",
    "\"Credit reporting Credit reporting Credit reporting\"\n",
    "NA\n",
    "\n",
    "\"Money transfer  virtual currency  or money service\"\n",
    "Check cashing service\n",
    "Debt settlement\n",
    "Domestic (US) money transfer\n",
    "Foreign currency exchange\n",
    "International money transfer\n",
    "Mobile or digital wallet\n",
    "Money order\n",
    "Refund anticipation check\n",
    "Traveler's check or cashier's check\n",
    "Virtual currency\n",
    "\n",
    "\"Money transfers\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer\n",
    "\n",
    "\"Virtual currency\"\n",
    "Domestic (US) money transfer\n",
    "International money transfer\n",
    "\n",
    "\"Other financial service\"\n",
    "Check cashing\n",
    "Credit repair\n",
    "Debt settlement\n",
    "Foreign currency exchange\n",
    "Money order\n",
    "Refund anticipation check\n",
    "Traveler’s/Cashier’s checks\n",
    "\n",
    "\"Payday loan  title loan  or personal loan\"\n",
    "Installment loan\n",
    "Pawn loan\n",
    "Payday loan\n",
    "Personal line of credit\n",
    "Title loan\n",
    "\n",
    "\"Payday loan\"\n",
    "NA\n",
    "\n",
    "\"Student loan\"\n",
    "Federal student loan servicing\n",
    "Non-federal student loan\n",
    "Private student loan\n",
    "\n",
    "\"Consumer Loan\"\n",
    "Installment loan\n",
    "Pawn loan\n",
    "Personal line of credit\n",
    "Title loan\n",
    "Vehicle lease\n",
    "Vehicle loan\n",
    "\n",
    "\"Vehicle loan or lease\"\n",
    "Lease\n",
    "Loan\n",
    "Title loan\n",
    "\n",
    "\"Bank account or service\"\n",
    "(CD) Certificate of deposit\n",
    "Cashing a check without an account\n",
    "Checking account\n",
    "Other bank product/service\n",
    "Savings account\n",
    "\n",
    "\n",
    "\"Checking or savings account\"\n",
    "CD (Certificate of Deposit)\n",
    "Checking account\n",
    "Other banking product or service\n",
    "Personal line of credit\n",
    "Savings account\n",
    "\n",
    "\"NA\"\n",
    "Checking account\n",
    "\n",
    "\"Mortgage\"\n",
    "Conventional adjustable mortgage (ARM)\n",
    "Conventional fixed mortgage\n",
    "Conventional home mortgage\n",
    "FHA mortgage\n",
    "Home equity loan or line of credit\n",
    "Home equity loan or line of credit (HELOC)\n",
    "Other mortgage\n",
    "Other type of mortgage\n",
    "Reverse mortgage\n",
    "Second mortgage\n",
    "VA mortgage\n",
    "\n",
    "\"Debt collection\"\n",
    "Auto\n",
    "Auto debt\n",
    "Credit card\n",
    "Credit card debt\n",
    "Federal student loan\n",
    "Federal student loan debt\n",
    "I do not know\n",
    "Medical\n",
    "Medical debt\n",
    "Mortgage\n",
    "Mortgage debt\n",
    "Non-federal student loan\n",
    "Other (i.e. phone  health club  etc.)\n",
    "Other debt\n",
    "Payday loan\n",
    "Payday loan debt\n",
    "Private student loan debt\n",
    "\n",
    "\"debt collection\"\n",
    "Credit card\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3df7f3",
   "metadata": {},
   "source": [
    "# Task C:  Exploratory Data Analysis Using R \n",
    "help me to explore and wrangle my data file, property_transaction_victoria.csv. use only R code. assume property_transaction_victoria.csv is located in the same directory as the R script being run. the format of the columns in property_transaction_victoria.csv is:\n",
    "Column Name - Description \n",
    "id - The unique ID of a transaction record, which usually consists of 9 \n",
    "digits \n",
    "badge - Whether a property is for rent or buy or it’s already sold \n",
    "url - URL of a property \n",
    "suburb - The suburb where a property locates in \n",
    "state - The state where a property locates in \n",
    "postcode - The postcode of a property \n",
    "short_address - Short address of the property \n",
    "full_address - Full address of the property \n",
    "property_type - Whether a property is a House, Townhouse, etc. \n",
    "price - The price for which the property was sold \n",
    "bedrooms - The number of bedrooms that a property has \n",
    "bathrooms - The number of bathrooms that a property has \n",
    "parking_spaces - The number of parking spaces that a property has  \n",
    "building_size - The building size of a property \n",
    "building_size_unit - The unit of building size of a property (measured in square metres) \n",
    "land_size - The area size of a property \n",
    "land_size_unit - The unit of the area size of a property (measured in square metres) \n",
    "listing_company_id - Real estate agent who managed the transaction \n",
    "listing_company_name - Name of a real estate agent \n",
    "listing_company_phone - Phone number of a real estate agent \n",
    "aution_date - Date of a property auction \n",
    "available_date - Available date that a buyer can move into a property  \n",
    "sold_date - The date on which the transaction was made \n",
    "description - A textual description that real estate agents used to describe the \n",
    "property and attract potential buyers before the transaction was made.  \n",
    "images - Url of a property images \n",
    "images_floorplans - Url of a property floor plan images \n",
    "listers - List of the real estate agent information \n",
    "inspections - Inspection date on a property\n",
    "\n",
    "with that, help me with the R code to answer the following:\n",
    "1.  Identify the top 3 suburbs with the highest number of property transactions over the \n",
    "years, and plot their monthly transaction counts for the year 2022. Include Toorak in \n",
    "the plot as well, if it is not among the top 3 suburbs.   \n",
    "2.  What  are  the  3  most  important  keywords  in  the  description  column  that  impact property prices? (Note: Since the description column contains a large volume of text, please extract a 10% sample from the original dataset to answer this.)   \n",
    "3.  Compute the correlation between price and land size for each suburb among the top 3 \n",
    "suburbs identified in Q1, and for each property type: house, unit, townhouse, and \n",
    "apartment.  Present  the  correlations  along  with  their  corresponding  suburb  and \n",
    "property  type.  (Note:  If price and land size values are not available for a certain \n",
    "property type and suburb, there is no need to present their correlation.)   \n",
    "4.  Owning property has long been considered a reliable way to build personal wealth. \n",
    "Which properties have experienced the highest price increases since their first sale? \n",
    "Please exclude properties where the time between the first and last sale exceeds five \n",
    "years.  List  the  top  five  properties  along  with  their  address, capital gain, and the duration between the first and last sale.   \n",
    "5.  Property price trends can vary not only across suburbs but also across property types. \n",
    "Identify  which  suburb–property  type  combination  exhibited  the  most volatility in \n",
    "median  property prices over the months of 2022. Display the top 5 most volatile \n",
    "combinations and provide an appropriate plot. (Note: Consider only the following \n",
    "property types — house, unit, townhouse, and apartment.)   \n",
    "6.  Chris is looking for a renovated house to purchase. He wants the property to be close \n",
    "to a shopping centre, and since he has a 7-year-old son, proximity to a primary school \n",
    "is  also  important.  He  is  looking  for  a  home  with  4  bedrooms and 2 bathrooms. \n",
    "Currently, he is considering six suburbs—Mulgrave, Vermont South, Doncaster East, \n",
    "Rowville, Glen Waverley, and Wheelers Hill—and plans to choose one from this list. \n",
    "Please provide the predicted price for September 2025 of a house that meets the above \n",
    "criteria  in  each  of  the  six  suburbs,  and  display  the predicted price alongside the corresponding suburb name. (Note: When you build the prediction model, please  use \n",
    "only the provided dataset and the period it covers.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2cd00",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'dplyr' successfully unpacked and MD5 sums checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"cannot remove prior installation of package 'dplyr'\"\n",
      "Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n",
      "\"problem copying C:\\Users\\adria\\R\\library\\00LOCK\\dplyr\\libs\\x64\\dplyr.dll to C:\\Users\\adria\\R\\library\\dplyr\\libs\\x64\\dplyr.dll: Permission denied\"\n",
      "Warning message:\n",
      "\"restored 'dplyr'\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'ggplot2' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'lubridate' successfully unpacked and MD5 sums checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"cannot remove prior installation of package 'lubridate'\"\n",
      "Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n",
      "\"problem copying C:\\Users\\adria\\R\\library\\00LOCK\\lubridate\\libs\\x64\\lubridate.dll to C:\\Users\\adria\\R\\library\\lubridate\\libs\\x64\\lubridate.dll: Permission denied\"\n",
      "Warning message:\n",
      "\"restored 'lubridate'\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'stringr' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n",
      "also installing the dependencies 'SnowballC', 'janeaustenr', 'tokenizers'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'SnowballC' successfully unpacked and MD5 sums checked\n",
      "package 'janeaustenr' successfully unpacked and MD5 sums checked\n",
      "package 'tokenizers' successfully unpacked and MD5 sums checked\n",
      "package 'tidytext' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n",
      "also installing the dependencies 'NLP', 'slam', 'BH'\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'NLP' successfully unpacked and MD5 sums checked\n",
      "package 'slam' successfully unpacked and MD5 sums checked\n",
      "package 'BH' successfully unpacked and MD5 sums checked\n",
      "package 'tm' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing package into 'C:/Users/adria/R/library'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'tidyr' successfully unpacked and MD5 sums checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"cannot remove prior installation of package 'tidyr'\"\n",
      "Warning message in file.copy(savedcopy, lib, recursive = TRUE):\n",
      "\"problem copying C:\\Users\\adria\\R\\library\\00LOCK\\tidyr\\libs\\x64\\tidyr.dll to C:\\Users\\adria\\R\\library\\tidyr\\libs\\x64\\tidyr.dll: Permission denied\"\n",
      "Warning message:\n",
      "\"restored 'tidyr'\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\adria\\AppData\\Local\\Temp\\RtmpOgFuRO\\downloaded_packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'stats' is in use and will not be installed\"\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Install and load necessary libraries\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"lubridate\")\n",
    "install.packages(\"stringr\")\n",
    "install.packages(\"tidytext\")\n",
    "install.packages(\"tm\")\n",
    "install.packages(\"tidyr\")\n",
    "install.packages(\"stats\") # For lm() and cor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6707ba",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "library(tidytext)\n",
    "library(tm)\n",
    "library(tidyr)\n",
    "\n",
    "# --- Load the dataset ---\n",
    "# Assuming 'property_transaction_victoria.csv' is in the same directory as your R script\n",
    "df <- read.csv(\"property_transaction_victoria.csv\", stringsAsFactors = FALSE)\n",
    "\n",
    "# Convert relevant columns to appropriate types\n",
    "df$sold_date <- as.Date(df$sold_date)\n",
    "df$auction_date <- as.Date(df$auction_date)\n",
    "df$available_date <- as.Date(df$available_date)\n",
    "\n",
    "# Convert price, building_size, land_size to numeric.\n",
    "# Coerce non-numeric values to NA and warn.\n",
    "df$price <- as.numeric(gsub(\"[^0-9.]\", \"\", df$price))\n",
    "df$building_size <- as.numeric(gsub(\"[^0-9.]\", \"\", df$building_size))\n",
    "df$land_size <- as.numeric(gsub(\"[^0-9.]\", \"\", df$land_size))\n",
    "\n",
    "# --- Question 1: Top 3 suburbs with highest transactions & monthly plot for 2022 ---\n",
    "cat(\"\\n--- Question 1: Top 3 Suburbs with Highest Transactions & Monthly Plot for 2022 ---\\n\")\n",
    "\n",
    "# Identify the top 3 suburbs with the highest number of property transactions\n",
    "top_suburbs <- df %>%\n",
    "  filter(!is.na(sold_date)) %>%\n",
    "  count(suburb, sort = TRUE) %>%\n",
    "  top_n(3, n) %>%\n",
    "  pull(suburb)\n",
    "\n",
    "# Add 'Toorak' to the list if not already in top 3\n",
    "if (!\"Toorak\" %in% top_suburbs) {\n",
    "  top_suburbs <- c(top_suburbs, \"Toorak\")\n",
    "}\n",
    "\n",
    "cat(\"Top 3 Suburbs (and Toorak if not in top 3):\", paste(top_suburbs, collapse = \", \"), \"\\n\")\n",
    "\n",
    "# Filter data for 2022 and selected suburbs\n",
    "monthly_transactions_2022 <- df %>%\n",
    "  filter(year(sold_date) == 2022, suburb %in% top_suburbs) %>%\n",
    "  mutate(month = floor_date(sold_date, \"month\")) %>%\n",
    "  group_by(suburb, month) %>%\n",
    "  summarise(transaction_count = n(), .groups = 'drop') %>%\n",
    "  complete(suburb, month = seq.Date(min(.$month), max(.$month), by = \"month\"), fill = list(transaction_count = 0))\n",
    "\n",
    "\n",
    "# Plot monthly transaction counts for 2022\n",
    "q1_plot <- ggplot(monthly_transactions_2022, aes(x = month, y = transaction_count, color = suburb)) +\n",
    "  geom_line() +\n",
    "  geom_point() +\n",
    "  labs(\n",
    "    title = \"Monthly Property Transaction Counts in 2022\",\n",
    "    x = \"Month\",\n",
    "    y = \"Number of Transactions\",\n",
    "    color = \"Suburb\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\")\n",
    "\n",
    "print(q1_plot)\n",
    "\n",
    "# --- Question 2: 3 Most Important Keywords in Description that Impact Property Prices ---\n",
    "cat(\"\\n--- Question 2: 3 Most Important Keywords in Description that Impact Property Prices ---\\n\")\n",
    "# Note: This is a simplified approach. A full NLP pipeline for feature extraction\n",
    "# and more robust regression models would be needed for a comprehensive analysis.\n",
    "# This code aims to demonstrate a basic method.\n",
    "\n",
    "# Take a 10% sample from the original dataset\n",
    "set.seed(123) # for reproducibility\n",
    "sample_df <- df %>%\n",
    "  filter(!is.na(description), !is.na(price)) %>%\n",
    "  sample_frac(0.10)\n",
    "\n",
    "# Text cleaning and tokenization\n",
    "# Create a custom stop word list (optional, but good for real estate context)\n",
    "custom_stop_words <- tibble(word = c(\"property\", \"house\", \"home\", \"apartment\", \"unit\", \"townhouse\", \"bedroom\", \"bathroom\", \"car\", \"space\", \"featuring\", \"boasting\", \"located\", \"close\", \"walk\", \"minutes\", \"access\"))\n",
    "\n",
    "cleaned_description_words <- sample_df %>%\n",
    "  unnest_tokens(word, description) %>%\n",
    "  anti_join(stop_words) %>% # Remove common English stop words\n",
    "  anti_join(custom_stop_words) %>% # Remove custom stop words\n",
    "  filter(str_detect(word, \"[a-z]\")) # Remove tokens that are not alphabetic\n",
    "\n",
    "\n",
    "# Count word frequencies for each description\n",
    "word_counts_per_doc <- cleaned_description_words %>%\n",
    "  count(id, word, sort = TRUE) %>%\n",
    "  cast_dtm(id, word, n)\n",
    "\n",
    "# Convert DTM to a data frame for regression\n",
    "dtm_df <- as.data.frame(as.matrix(word_counts_per_doc))\n",
    "\n",
    "# Ensure column names are valid for R formulas (e.g., no numbers at start, no special chars)\n",
    "colnames(dtm_df) <- make.names(colnames(dtm_df))\n",
    "\n",
    "# Merge with price data\n",
    "# Ensure 'id' in dtm_df matches 'id' in sample_df for merging\n",
    "dtm_df$id <- as.numeric(rownames(dtm_df))\n",
    "regression_data <- sample_df %>%\n",
    "  select(id, price) %>%\n",
    "  inner_join(dtm_df, by = \"id\") %>%\n",
    "  select(-id) # Remove id as it's not a predictor\n",
    "\n",
    "# Build a linear regression model\n",
    "# Due to the high dimensionality, we might only select a subset of common words\n",
    "# For demonstration, select top N most frequent words from the sample\n",
    "top_words_overall <- cleaned_description_words %>%\n",
    "  count(word, sort = TRUE) %>%\n",
    "  top_n(50, n) %>% # Adjust N based on computational capacity/relevance\n",
    "  pull(word)\n",
    "\n",
    "# Filter dtm_df to include only these top words, plus 'price'\n",
    "regression_data_filtered <- regression_data %>%\n",
    "  select(price, intersect(top_words_overall, colnames(regression_data)))\n",
    "\n",
    "# Remove columns with zero variance (if any, after filtering)\n",
    "regression_data_filtered <- regression_data_filtered[, sapply(regression_data_filtered, function(x) length(unique(x)) > 1)]\n",
    "\n",
    "# Create the formula dynamically\n",
    "formula_str <- paste(\"price ~\", paste(colnames(regression_data_filtered)[-1], collapse = \" + \"))\n",
    "model <- lm(as.formula(formula_str), data = regression_data_filtered)\n",
    "\n",
    "# Extract coefficients and identify important keywords\n",
    "# Filter for significant and positive coefficients (indicating positive impact on price)\n",
    "coefficients <- as.data.frame(summary(model)$coefficients)\n",
    "coefficients$word <- rownames(coefficients)\n",
    "colnames(coefficients)[c(1, 4)] <- c(\"Estimate\", \"p_value\")\n",
    "\n",
    "# Rank by absolute estimate, filter for significant p-values (e.g., < 0.05)\n",
    "important_keywords <- coefficients %>%\n",
    "  filter(word != \"(Intercept)\", p_value < 0.05) %>%\n",
    "  arrange(desc(abs(Estimate))) %>%\n",
    "  head(3) # Get top 3\n",
    "\n",
    "if (nrow(important_keywords) > 0) {\n",
    "  cat(\"Top 3 Keywords impacting property prices (based on simplified regression):\\n\")\n",
    "  print(important_keywords %>% select(word, Estimate, p_value))\n",
    "} else {\n",
    "  cat(\"No significant keywords found with the current model and data subset.\\n\")\n",
    "}\n",
    "\n",
    "# --- Question 3: Correlation between Price and Land Size per Suburb & Property Type ---\n",
    "cat(\"\\n--- Question 3: Correlation between Price and Land Size per Suburb & Property Type ---\\n\")\n",
    "\n",
    "# Define target property types\n",
    "target_property_types <- c(\"House\", \"Unit\", \"Townhouse\", \"Apartment\")\n",
    "\n",
    "# Compute correlations\n",
    "correlations <- df %>%\n",
    "  filter(suburb %in% top_suburbs, property_type %in% target_property_types) %>%\n",
    "  group_by(suburb, property_type) %>%\n",
    "  summarise(\n",
    "    correlation = cor(price, land_size, use = \"pairwise.complete.obs\"),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  filter(!is.na(correlation)) # Remove combinations where correlation could not be computed\n",
    "\n",
    "if (nrow(correlations) > 0) {\n",
    "  cat(\"Correlation between Price and Land Size:\\n\")\n",
    "  print(correlations)\n",
    "} else {\n",
    "  cat(\"No correlations found for the specified suburbs and property types due to missing data.\\n\")\n",
    "}\n",
    "\n",
    "# --- Question 4: Properties with Highest Price Increases ---\n",
    "cat(\"\\n--- Question 4: Properties with Highest Price Increases ---\\n\")\n",
    "\n",
    "# Group by a unique property identifier (assuming full_address is unique per property)\n",
    "property_gains <- df %>%\n",
    "  filter(!is.na(full_address), !is.na(sold_date), !is.na(price)) %>%\n",
    "  group_by(full_address) %>%\n",
    "  summarise(\n",
    "    first_sale_date = min(sold_date),\n",
    "    last_sale_date = max(sold_date),\n",
    "    first_sale_price = price[match(min(sold_date), sold_date)],\n",
    "    last_sale_price = price[match(max(sold_date), sold_date)],\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  # Filter for properties with more than one sale record and distinct prices\n",
    "  filter(first_sale_date != last_sale_date, first_sale_price != last_sale_price) %>%\n",
    "  mutate(\n",
    "    capital_gain = last_sale_price - first_sale_price,\n",
    "    duration_days = as.numeric(last_sale_date - first_sale_date)\n",
    "  ) %>%\n",
    "  filter(duration_days <= (5 * 365)) %>% # Filter out properties where duration exceeds five years\n",
    "  arrange(desc(capital_gain)) %>%\n",
    "  head(5)\n",
    "\n",
    "if (nrow(property_gains) > 0) {\n",
    "  cat(\"Top 5 Properties with Highest Capital Gains (within 5 years):\\n\")\n",
    "  print(property_gains %>% select(full_address, capital_gain, duration_days))\n",
    "} else {\n",
    "  cat(\"No properties found with multiple sales records meeting the criteria.\\n\")\n",
    "}\n",
    "\n",
    "\n",
    "# --- Question 5: Most Volatile Suburb-Property Type Combination in 2022 ---\n",
    "cat(\"\\n--- Question 5: Most Volatile Suburb-Property Type Combination in 2022 ---\\n\")\n",
    "\n",
    "# Define target property types\n",
    "target_property_types_q5 <- c(\"House\", \"Unit\", \"Townhouse\", \"Apartment\")\n",
    "\n",
    "# Calculate monthly median prices for 2022\n",
    "monthly_median_prices_2022 <- df %>%\n",
    "  filter(year(sold_date) == 2022, property_type %in% target_property_types_q5) %>%\n",
    "  mutate(month = floor_date(sold_date, \"month\")) %>%\n",
    "  group_by(suburb, property_type, month) %>%\n",
    "  summarise(\n",
    "    median_price = median(price, na.rm = TRUE),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  filter(!is.na(median_price))\n",
    "\n",
    "# Calculate volatility (standard deviation of monthly median prices)\n",
    "volatility_combinations <- monthly_median_prices_2022 %>%\n",
    "  group_by(suburb, property_type) %>%\n",
    "  summarise(\n",
    "    volatility = sd(median_price, na.rm = TRUE),\n",
    "    .groups = 'drop'\n",
    "  ) %>%\n",
    "  filter(!is.na(volatility)) %>%\n",
    "  arrange(desc(volatility)) %>%\n",
    "  head(5)\n",
    "\n",
    "if (nrow(volatility_combinations) > 0) {\n",
    "  cat(\"Top 5 Most Volatile Suburb-Property Type Combinations (2022):\\n\")\n",
    "  print(volatility_combinations)\n",
    "\n",
    "  # Prepare data for plotting the top volatile combinations\n",
    "  plot_data_volatility <- monthly_median_prices_2022 %>%\n",
    "    inner_join(volatility_combinations, by = c(\"suburb\", \"property_type\")) %>%\n",
    "    mutate(combination = paste(suburb, property_type, sep = \" - \"))\n",
    "\n",
    "  # Plot median prices over months for the most volatile combinations\n",
    "  q5_plot <- ggplot(plot_data_volatility, aes(x = month, y = median_price, color = combination)) +\n",
    "    geom_line() +\n",
    "    geom_point() +\n",
    "    labs(\n",
    "      title = \"Monthly Median Property Prices for Most Volatile Combinations (2022)\",\n",
    "      x = \"Month\",\n",
    "      y = \"Median Price\",\n",
    "      color = \"Suburb - Property Type\"\n",
    "    ) +\n",
    "    theme_minimal() +\n",
    "    scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\")\n",
    "\n",
    "  print(q5_plot)\n",
    "} else {\n",
    "  cat(\"No volatile combinations found for the specified criteria in 2022.\\n\")\n",
    "}\n",
    "\n",
    "\n",
    "# --- Question 6: Predicted Price for September 2025 ---\n",
    "cat(\"\\n--- Question 6: Predicted Price for September 2025 ---\\n\")\n",
    "# Note: Predictions are based on the available data and a simple linear model.\n",
    "# External factors, economic changes, or detailed property features (like 'renovated')\n",
    "# are not explicitly modeled here.\n",
    "\n",
    "# Define the target suburbs\n",
    "chris_suburbs <- c(\"Mulgrave\", \"Vermont South\", \"Doncaster East\", \"Rowville\", \"Glen Waverley\", \"Wheelers Hill\")\n",
    "\n",
    "# Filter data for model training\n",
    "model_data <- df %>%\n",
    "  filter(\n",
    "    property_type == \"House\",\n",
    "    bedrooms == 4,\n",
    "    bathrooms == 2,\n",
    "    suburb %in% chris_suburbs,\n",
    "    !is.na(price),\n",
    "    !is.na(sold_date),\n",
    "    !is.na(parking_spaces),\n",
    "    !is.na(building_size),\n",
    "    !is.na(land_size)\n",
    "  ) %>%\n",
    "  mutate(\n",
    "    # Create a numerical time variable (e.g., days since the earliest date in the dataset)\n",
    "    time_index = as.numeric(sold_date - min(.$sold_date, na.rm = TRUE))\n",
    "  )\n",
    "\n",
    "# Check if there is enough data for modeling\n",
    "if (nrow(model_data) == 0) {\n",
    "  cat(\"Not enough data to build a prediction model for the specified criteria.\\n\")\n",
    "} else {\n",
    "  # Build a linear regression model\n",
    "  # Factors like 'renovated' or 'proximity to shopping/school' are not explicit features\n",
    "  # in the dataset. The model will rely on the available numerical and categorical features.\n",
    "  # We use the median values for numerical features for prediction as no specific values are given.\n",
    "  median_parking_spaces <- median(model_data$parking_spaces, na.rm = TRUE)\n",
    "  median_building_size <- median(model_data$building_size, na.rm = TRUE)\n",
    "  median_land_size <- median(model_data$land_size, na.rm = TRUE)\n",
    "\n",
    "  # Use the available features for the model\n",
    "  # Consider 'suburb' as a categorical variable\n",
    "  model <- lm(price ~ time_index + suburb + parking_spaces + building_size + land_size, data = model_data)\n",
    "\n",
    "  # Prepare new data for prediction for September 2025\n",
    "  # Calculate the time_index for September 1, 2025\n",
    "  prediction_date <- as.Date(\"2025-09-01\")\n",
    "  # Assuming min(sold_date) from training data for time_index calculation\n",
    "  min_sold_date_training <- min(model_data$sold_date, na.rm = TRUE)\n",
    "  prediction_time_index <- as.numeric(prediction_date - min_sold_date_training)\n",
    "\n",
    "  # Create a data frame for prediction\n",
    "  predict_df <- data.frame(\n",
    "    time_index = rep(prediction_time_index, length(chris_suburbs)),\n",
    "    suburb = factor(chris_suburbs, levels = levels(model_data$suburb)), # Ensure factor levels match\n",
    "    parking_spaces = rep(median_parking_spaces, length(chris_suburbs)),\n",
    "    building_size = rep(median_building_size, length(chris_suburbs)),\n",
    "    land_size = rep(median_land_size, length(chris_suburbs))\n",
    "  )\n",
    "\n",
    "  # Predict prices\n",
    "  predicted_prices <- predict(model, newdata = predict_df)\n",
    "  predict_df$predicted_price <- round(predicted_prices, 2)\n",
    "\n",
    "  cat(\"Predicted Prices for September 2025 (House, 4 Beds, 2 Baths, Median Parking/Building/Land Size):\\n\")\n",
    "  print(predict_df %>% select(suburb, predicted_price))\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
