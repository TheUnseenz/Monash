{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbb70b8e",
   "metadata": {},
   "source": [
    "# Task D:  Predictive Data Analysis using R\n",
    "I need to train a machine learning model on dialogue utterance vs dialogue usefulness. \n",
    "\n",
    "The data files I have, with the descriptions of their columns are:\n",
    "dialogue_utterance_train/validation/test.csv \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Usefulness_score - This score is given by a student to indicate their perceived \n",
    "usefulness of the FLoRA chatbot when answering the post-task \n",
    "questionnaire Question 3 (i.e., “To what extent do you think the \n",
    "GPT-powered chatbot on FLoRA is useful for you to accomplish \n",
    "the assignment?”). The value range of this feature is [1,5], with 1 \n",
    "representing “very unuseful”, 2 representing “unuseful”, 3 \n",
    "representing “neutral”, 4 representing “useful”, and 5 \n",
    "representing “very useful”.\n",
    "\n",
    "dialogue_usefulness_train/validation/test.csv\n",
    "Column Name - Description \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Timestamp - When an utterance contained in the dialogue was made \n",
    "Interlocutor - Whether the utterance was made by the student or the chatbot (\"Student\"/\"Chatbot\")\n",
    "Utterance_text - The text of the utterance\n",
    "\n",
    "dialogue_utterance_train has 117k lines, split over 303 unique dialogue IDs.\n",
    "1.  What  features  can  you  engineer  to  empower  the  training  of  a  machine  learning model? You may propose as many as you believe are useful. Please note that the number of the features should not exceed the number of the dialogues contained in the training set. Otherwise, the constructed machine learning models are prone to have overfitting issues. Select two features that you propose and try to use boxplots to visualise  the  feature  value  between  the following two groups of dialogues in the training  set:  (i)  those  with  Usefulness_score  of  1  or  2;  and  (ii)  those  with Usefulness_score  of  4  or  5.  Show if there  any  difference  between  the  two  groups  of dialogues? How can you tell whether the difference is statistically significant? Ideally, identify features that display statistically significant differences. \n",
    " \n",
    "2.  Build a machine learning model (e.g., polynomial regressions, regression tree) based on the training set by taking all the features that you have proposed and evaluate the performance of the model on the validation set using the relevant evaluation metrics you learned in class. Aim to include at least 5 features in this model. The best-performing model here is denoted as Model 1. \n",
    "3.  Now we want to improve the performance of Model 1 (i.e., to get a more accurate model).  For  example,  you  may  try  some  of the following methods to improve a model: \n",
    "●  Select  a  subset  of  the  features  (especially  the  important  ones  in  your opinions) as input to empower a machine learning model or a subset of the \n",
    "data in a dialogue (given that some questions asked by students might not be \n",
    "directly relevant to solving the assignment). \n",
    "●  Deal with errors (e.g.: filtering out data outliers). \n",
    "●  Rescale  data  (i.e.,  bringing  different  variables  with  different  scales  to  a common scale). \n",
    "●  Transform data (i.e., transforming the distribution of variables). \n",
    "●  Try other machine learning algorithms that you know. \n",
    " \n",
    "Please build the predictive models by trying some of the above methods or some other methods you can think of and evaluate the performance of the models and report whether Model 1 can be improved. \n",
    "Explain how you have improved your model by including code, output, \n",
    "and  explanations  (explaining  the  code  or the process) and justify why you have chosen some of the above methods or some other methods to improve a model \n",
    "(e.g., why this subset of the variables are chosen to build a model). \n",
    "4.  What is the Dialogue_ID of the dialogue you generated? Please copy and paste the whole  dialogue  text  that  you  generated  with  the  chatbot  here.  With  the best-performing model constructed from Question 2&3, what is the prediction value for the dialogue you generated? Is the prediction value close to the groundtruth value? \n",
    "If yes, what features do you think play important roles here to enable the model to successfully make the prediction? How can you determine the importance of features quantitatively? If not, what might be the reasons? For students whose dialogues are included in the test set, you may randomly select a dialogue from the validation set to analyse and answer this question. \n",
    "5.  The  groundtruth  Usefulness_score  values  in  the  file “dialogue_usefulness_test.csv” are unavailable now. Here, your task is to use the best-performing model constructed from Question 2&3 to predict the usefulness of the dialogues contained in the  test  set.  You  need  to  populate  your  prediction  results  (i.e.,  the  predicted Usefulness_score values) into the file “dialogue_usefulness_test.csv” and upload it to Moodle to measure the overall performance of your model. Please ensure the number of  columns  and  rows  remains  the  same  as  in  the  original  file (dialogue_usefulness_test.csv),  and  only  fill  in  the  prediction  results  in  the 'Usefulness_score'  column.  Please  name  the  submission  file  using  the  following format: LastName_StudentNumber_dialogue_usefulness_test.csv. \n",
    "\n",
    "The performance level of the model will be measured by RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e4c2bb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of engineered features: 9\n",
      "Number of unique dialogues in training set: 303\n",
      "All engineered features: num_utterances, total_dialogue_length, dialogue_duration, avg_len_student_utterance, avg_len_chatbot_utterance, num_student_questions, num_chatbot_questions, avg_time_between_utterances, ratio_student_chatbot_len\n",
      "\n",
      "\n",
      "--- Statistical Significance Tests ---\n",
      "Feature: total_dialogue_length\n",
      "  Welch's t-test p-value: 6e-04\n",
      "  Difference is statistically significant (p < 0.05)\n",
      "  Wilcoxon rank-sum test p-value: 0.0068\n",
      "  Wilcoxon rank-sum test: Difference is statistically significant (p < 0.05)\n",
      "\n",
      "Feature: ratio_student_chatbot_len\n",
      "  Welch's t-test p-value: 0.4538\n",
      "  Difference is not statistically significant (p >= 0.05)\n",
      "  Wilcoxon rank-sum test p-value: 0.2996\n",
      "  Wilcoxon rank-sum test: Difference is not statistically significant (p >= 0.05)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install and load necessary packages\n",
    "if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
    "if (!requireNamespace(\"ggplot2\", quietly = TRUE)) install.packages(\"ggplot2\")\n",
    "if (!requireNamespace(\"lubridate\", quietly = TRUE)) install.packages(\"lubridate\")\n",
    "if (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\n",
    "\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "\n",
    "# Load the training data\n",
    "df_utterance <- read.csv(\"git_ignore/dialogue_utterance_train.csv\")\n",
    "df_usefulness <- read.csv(\"git_ignore/dialogue_usefulness_train.csv\")\n",
    "\n",
    "# Merge the dataframes on Dialogue_ID\n",
    "# Use `left_join` to ensure all dialogues from df_utterance are kept,\n",
    "# as usefulness score is defined per dialogue.\n",
    "df_merged <- left_join(df_utterance, df_usefulness, by = \"Dialogue_ID\")\n",
    "\n",
    "# Convert Timestamp to datetime objects\n",
    "df_merged$Timestamp <- ymd_hms(df_merged$Timestamp)\n",
    "\n",
    "# Sort by Dialogue_ID and Timestamp for accurate sequential calculations\n",
    "df_merged <- df_merged %>%\n",
    "  arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "dialogue_features <- df_merged %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    num_utterances = n(),\n",
    "    total_dialogue_length = sum(nchar(Utterance_text), na.rm = TRUE),\n",
    "    dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "    avg_len_student_utterance = mean(nchar(Utterance_text[Interlocutor == \"Student\"]), na.rm = TRUE),\n",
    "    avg_len_chatbot_utterance = mean(nchar(Utterance_text[Interlocutor == \"Chatbot\"]), na.rm = TRUE),\n",
    "    num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "    num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "# Handle cases where a specific interlocutor might not have uttered anything (NaNs from mean)\n",
    "dialogue_features$avg_len_student_utterance[is.nan(dialogue_features$avg_len_student_utterance)] <- 0\n",
    "dialogue_features$avg_len_chatbot_utterance[is.nan(dialogue_features$avg_len_chatbot_utterance)] <- 0\n",
    "\n",
    "# Calculate average time between utterances\n",
    "# This needs to be done iteratively per dialogue or using a custom function\n",
    "avg_time_between_utterances <- df_merged %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    time_diffs = list(as.numeric(diff(Timestamp), units = \"secs\")),\n",
    "    avg_time_between_utterances = ifelse(length(time_diffs[[1]]) > 0, mean(time_diffs[[1]], na.rm = TRUE), 0)\n",
    "  ) %>%\n",
    "  select(Dialogue_ID, avg_time_between_utterances)\n",
    "\n",
    "dialogue_features <- left_join(dialogue_features, avg_time_between_utterances, by = \"Dialogue_ID\")\n",
    "\n",
    "# New Feature: Ratio of average student utterance length to average chatbot utterance length\n",
    "dialogue_features <- dialogue_features %>%\n",
    "  mutate(\n",
    "    ratio_student_chatbot_len = ifelse(avg_len_chatbot_utterance > 0, avg_len_student_utterance / avg_len_chatbot_utterance,\n",
    "                                       ifelse(avg_len_student_utterance > 0, Inf, 0)) # Handle division by zero\n",
    "  )\n",
    "\n",
    "# Add Usefulness_score to the dialogue features dataframe\n",
    "df_utterance_scores_unique <- df_merged %>%\n",
    "  select(Dialogue_ID, Usefulness_score) %>%\n",
    "  distinct()\n",
    "\n",
    "dialogue_features <- left_join(dialogue_features, df_utterance_scores_unique, by = \"Dialogue_ID\")\n",
    "\n",
    "# Verify the number of features does not exceed 303 (number of unique dialogues)\n",
    "# Exclude Dialogue_ID and Usefulness_score from the count\n",
    "num_features_engineered <- ncol(dialogue_features) - 2\n",
    "num_unique_dialogues <- n_distinct(dialogue_features$Dialogue_ID)\n",
    "\n",
    "cat(paste0(\"Number of engineered features: \", num_features_engineered, \"\\n\"))\n",
    "cat(paste0(\"Number of unique dialogues in training set: \", num_unique_dialogues, \"\\n\"))\n",
    "cat(paste0(\"All engineered features: \", paste(setdiff(names(dialogue_features), c(\"Dialogue_ID\", \"Usefulness_score\")), collapse = \", \"), \"\\n\\n\"))\n",
    "\n",
    "# --- Feature Selection for Visualization ---\n",
    "# Let's select 'total_dialogue_length' and 'ratio_student_chatbot_len' for visualization.\n",
    "# These seem to capture different aspects of conversation dynamics related to engagement/usefulness.\n",
    "\n",
    "selected_features <- c('total_dialogue_length', 'ratio_student_chatbot_len')\n",
    "\n",
    "# --- Data Preparation for Visualization ---\n",
    "# Create groups based on Usefulness_score\n",
    "group1 <- dialogue_features %>% filter(Usefulness_score %in% c(1, 2))\n",
    "group2 <- dialogue_features %>% filter(Usefulness_score %in% c(4, 5))\n",
    "\n",
    "# Combine data for plotting\n",
    "plot_data <- data.frame(\n",
    "  Feature = character(),\n",
    "  Value = numeric(),\n",
    "  Score_Group = character(),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "for (feature in selected_features) {\n",
    "  # Add 'Unuseful' group data\n",
    "  plot_data <- rbind(plot_data, data.frame(\n",
    "    Feature = feature,\n",
    "    Value = group1[[feature]],\n",
    "    Score_Group = \"Unuseful (1-2)\"\n",
    "  ))\n",
    "  # Add 'Useful' group data\n",
    "  plot_data <- rbind(plot_data, data.frame(\n",
    "    Feature = feature,\n",
    "    Value = group2[[feature]],\n",
    "    Score_Group = \"Useful (4-5)\"\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Convert 'Feature' to a factor for faceting in ggplot\n",
    "plot_data$Feature <- factor(plot_data$Feature, levels = selected_features)\n",
    "\n",
    "# --- Visualization ---\n",
    "# Ensure the Value column is numeric for plotting\n",
    "plot_data$Value <- as.numeric(plot_data$Value)\n",
    "\n",
    "ggplot(plot_data, aes(x = Score_Group, y = Value, fill = Score_Group)) +\n",
    "  geom_boxplot() +\n",
    "  facet_wrap(~ Feature, scales = \"free_y\", ncol = 2) +\n",
    "  labs(\n",
    "    title = \"Distribution of Engineered Features by Dialogue Usefulness Score\",\n",
    "    x = \"Usefulness Score Group\",\n",
    "    y = \"Feature Value\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")\n",
    "\n",
    "# --- Statistical Significance Tests ---\n",
    "cat(\"\\n--- Statistical Significance Tests ---\\n\")\n",
    "\n",
    "for (feature in selected_features) {\n",
    "  cat(paste0(\"Feature: \", feature, \"\\n\"))\n",
    "  \n",
    "  # Extract data for the two groups, removing NA values\n",
    "  data_group1 <- na.omit(group1[[feature]])\n",
    "  data_group2 <- na.omit(group2[[feature]])\n",
    "  \n",
    "  # Check if there's enough data for both groups\n",
    "  if (length(data_group1) > 1 && length(data_group2) > 1) {\n",
    "    # Perform Welch's t-test (does not assume equal variances)\n",
    "    ttest_result <- t.test(data_group1, data_group2, var.equal = FALSE)\n",
    "    cat(paste0(\"  Welch's t-test p-value: \", round(ttest_result$p.value, 4), \"\\n\"))\n",
    "    \n",
    "    if (ttest_result$p.value < 0.05) {\n",
    "      cat(\"  Difference is statistically significant (p < 0.05)\\n\")\n",
    "    } else {\n",
    "      cat(\"  Difference is not statistically significant (p >= 0.05)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Also perform Wilcoxon rank-sum test (non-parametric, does not assume normality)\n",
    "    wilcox_result <- wilcox.test(data_group1, data_group2)\n",
    "    cat(paste0(\"  Wilcoxon rank-sum test p-value: \", round(wilcox_result$p.value, 4), \"\\n\"))\n",
    "    \n",
    "    if (wilcox_result$p.value < 0.05) {\n",
    "      cat(\"  Wilcoxon rank-sum test: Difference is statistically significant (p < 0.05)\\n\")\n",
    "    } else {\n",
    "      cat(\"  Wilcoxon rank-sum test: Difference is not statistically significant (p >= 0.05)\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"  Not enough data in one or both groups to perform statistical tests.\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb63cdcd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Linear Regression Model ---\n",
      "Linear Regression - RMSE: 1.1385, MAE: 0.8958, R-squared: 0.0012\n",
      "\n",
      "--- Training Regression Tree Model ---\n",
      "Regression Tree - RMSE: 1.3458, MAE: 1.0051, R-squared: 3e-04\n",
      "\n",
      "--- Training Random Forest Regression Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - RMSE: 0.9926, MAE: 0.7995, R-squared: 0.073\n",
      "\n",
      "--- Training Support Vector Regression (SVR) Model ---\n",
      "SVR - RMSE: 0.9898, MAE: 0.7799, R-squared: 0.0939\n",
      "\n",
      "--- Model Performance Summary on Validation Set ---\n",
      "              Model      RMSE       MAE    R_squared\n",
      "1 Linear Regression 1.1384619 0.8957676 0.0011560435\n",
      "2   Regression Tree 1.3458483 1.0050839 0.0003409744\n",
      "3     Random Forest 0.9926369 0.7995344 0.0729638323\n",
      "4               SVR 0.9897626 0.7799025 0.0938594253\n",
      "\n",
      "Best performing model (Model 1) based on RMSE: SVR\n",
      "\n",
      "Model 1 has been identified and stored.\n"
     ]
    }
   ],
   "source": [
    "# Install and load necessary packages\n",
    "if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
    "if (!requireNamespace(\"ggplot2\", quietly = TRUE)) install.packages(\"ggplot2\") # Already installed, but good for completeness\n",
    "if (!requireNamespace(\"lubridate\", quietly = TRUE)) install.packages(\"lubridate\") # Already installed, but good for completeness\n",
    "if (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\") # Already installed, but good for completeness\n",
    "if (!requireNamespace(\"caret\", quietly = TRUE)) install.packages(\"caret\") # For model training and evaluation\n",
    "if (!requireNamespace(\"randomForest\", quietly = TRUE)) install.packages(\"randomForest\") # For Random Forest\n",
    "if (!requireNamespace(\"e1071\", quietly = TRUE)) install.packages(\"e1071\") # For SVR\n",
    "if (!requireNamespace(\"rpart\", quietly = TRUE)) install.packages(\"rpart\") # For Regression Tree\n",
    "if (!requireNamespace(\"rpart.plot\", quietly = TRUE)) install.packages(\"rpart.plot\") # For plotting regression tree\n",
    "\n",
    "library(dplyr)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(e1071)\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "\n",
    "# --- Load and Prepare Training Data (Re-run from previous task to ensure consistency) ---\n",
    "df_utterance_train <- read.csv(\"git_ignore/dialogue_utterance_train.csv\")\n",
    "df_usefulness_train <- read.csv(\"git_ignore/dialogue_usefulness_train.csv\")\n",
    "df_merged_train <- left_join(df_utterance_train, df_usefulness_train, by = \"Dialogue_ID\")\n",
    "df_merged_train$Timestamp <- ymd_hms(df_merged_train$Timestamp)\n",
    "df_merged_train <- df_merged_train %>% arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "dialogue_features_train <- df_merged_train %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    num_utterances = n(),\n",
    "    total_dialogue_length = sum(nchar(Utterance_text), na.rm = TRUE),\n",
    "    dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "    avg_len_student_utterance = mean(nchar(Utterance_text[Interlocutor == \"Student\"]), na.rm = TRUE),\n",
    "    avg_len_chatbot_utterance = mean(nchar(Utterance_text[Interlocutor == \"Chatbot\"]), na.rm = TRUE),\n",
    "    num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "    num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "dialogue_features_train$avg_len_student_utterance[is.nan(dialogue_features_train$avg_len_student_utterance)] <- 0\n",
    "dialogue_features_train$avg_len_chatbot_utterance[is.nan(dialogue_features_train$avg_len_chatbot_utterance)] <- 0\n",
    "\n",
    "avg_time_between_utterances_train <- df_merged_train %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    time_diffs = list(as.numeric(diff(Timestamp), units = \"secs\")),\n",
    "    avg_time_between_utterances = ifelse(length(time_diffs[[1]]) > 0, mean(time_diffs[[1]], na.rm = TRUE), 0)\n",
    "  ) %>%\n",
    "  select(Dialogue_ID, avg_time_between_utterances)\n",
    "\n",
    "dialogue_features_train <- left_join(dialogue_features_train, avg_time_between_utterances_train, by = \"Dialogue_ID\")\n",
    "\n",
    "dialogue_features_train <- dialogue_features_train %>%\n",
    "  mutate(\n",
    "    ratio_student_chatbot_len = ifelse(avg_len_chatbot_utterance > 0, avg_len_student_utterance / avg_len_chatbot_utterance,\n",
    "                                       ifelse(avg_len_student_utterance > 0, Inf, 0))\n",
    "  )\n",
    "\n",
    "df_usefulness_scores_train_unique <- df_merged_train %>%\n",
    "  select(Dialogue_ID, Usefulness_score) %>%\n",
    "  distinct()\n",
    "\n",
    "dialogue_features_train <- left_join(dialogue_features_train, df_usefulness_scores_train_unique, by = \"Dialogue_ID\")\n",
    "\n",
    "\n",
    "# --- Load and Prepare Validation Data (applying the same feature engineering) ---\n",
    "df_utterance_validation <- read.csv(\"git_ignore/dialogue_utterance_validation.csv\")\n",
    "df_usefulness_validation <- read.csv(\"git_ignore/dialogue_usefulness_validation.csv\")\n",
    "\n",
    "df_merged_validation <- left_join(df_utterance_validation, df_usefulness_validation, by = \"Dialogue_ID\")\n",
    "df_merged_validation$Timestamp <- ymd_hms(df_merged_validation$Timestamp)\n",
    "df_merged_validation <- df_merged_validation %>% arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "dialogue_features_validation <- df_merged_validation %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    num_utterances = n(),\n",
    "    total_dialogue_length = sum(nchar(Utterance_text), na.rm = TRUE),\n",
    "    dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "    avg_len_student_utterance = mean(nchar(Utterance_text[Interlocutor == \"Student\"]), na.rm = TRUE),\n",
    "    avg_len_chatbot_utterance = mean(nchar(Utterance_text[Interlocutor == \"Chatbot\"]), na.rm = TRUE),\n",
    "    num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "    num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "dialogue_features_validation$avg_len_student_utterance[is.nan(dialogue_features_validation$avg_len_student_utterance)] <- 0\n",
    "dialogue_features_validation$avg_len_chatbot_utterance[is.nan(dialogue_features_validation$avg_len_chatbot_utterance)] <- 0\n",
    "\n",
    "avg_time_between_utterances_validation <- df_merged_validation %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    time_diffs = list(as.numeric(diff(Timestamp), units = \"secs\")),\n",
    "    avg_time_between_utterances = ifelse(length(time_diffs[[1]]) > 0, mean(time_diffs[[1]], na.rm = TRUE), 0)\n",
    "  ) %>%\n",
    "  select(Dialogue_ID, avg_time_between_utterances)\n",
    "\n",
    "dialogue_features_validation <- left_join(dialogue_features_validation, avg_time_between_utterances_validation, by = \"Dialogue_ID\")\n",
    "\n",
    "dialogue_features_validation <- dialogue_features_validation %>%\n",
    "  mutate(\n",
    "    ratio_student_chatbot_len = ifelse(avg_len_chatbot_utterance > 0, avg_len_student_utterance / avg_len_chatbot_utterance,\n",
    "                                       ifelse(avg_len_student_utterance > 0, Inf, 0))\n",
    "  )\n",
    "\n",
    "df_usefulness_scores_validation_unique <- df_merged_validation %>%\n",
    "  select(Dialogue_ID, Usefulness_score) %>%\n",
    "  distinct()\n",
    "\n",
    "dialogue_features_validation <- left_join(dialogue_features_validation, df_usefulness_scores_validation_unique, by = \"Dialogue_ID\")\n",
    "\n",
    "# Identify features for training (all engineered features except Dialogue_ID and Usefulness_score)\n",
    "features_to_use <- setdiff(names(dialogue_features_train), c(\"Dialogue_ID\", \"Usefulness_score\"))\n",
    "\n",
    "# Ensure the features are numeric and handle any Inf values (from ratio_student_chatbot_len)\n",
    "# Replace Inf with a large but finite number, or max out at a reasonable value or 0 if it represents no chatbot utterance\n",
    "# For simplicity, replacing Inf with NA and then using mean imputation or removing rows.\n",
    "# Given the small number of dialogues, removing rows might be too aggressive.\n",
    "# Let's replace Inf with a large number (e.g., max non-Inf value + buffer) if it makes sense in the context of the feature.\n",
    "# Or, if it's truly infinite (e.g., chatbot never spoke), maybe 0 or a very small number is better for ratio_student_chatbot_len?\n",
    "# For now, let's cap Inf to a reasonable maximum or the max value in the dataset, which is a common practice.\n",
    "\n",
    "for (col in features_to_use) {\n",
    "  # For ratio_student_chatbot_len, Inf means student spoke but chatbot didn't\n",
    "  # Let's replace Inf with the max finite value of that column in the training set\n",
    "  # and if there are no finite values, then 0. This needs to be done carefully.\n",
    "  if (any(is.infinite(dialogue_features_train[[col]]))) {\n",
    "    max_val <- max(dialogue_features_train[[col]][is.finite(dialogue_features_train[[col]])])\n",
    "    if (is.infinite(max_val) || is.na(max_val)) max_val <- 0 # If no finite values, set to 0 or a default\n",
    "    dialogue_features_train[[col]][is.infinite(dialogue_features_train[[col]])] <- max_val * 2 # A heuristic, might need tuning\n",
    "  }\n",
    "  if (any(is.infinite(dialogue_features_validation[[col]]))) {\n",
    "    max_val <- max(dialogue_features_validation[[col]][is.finite(dialogue_features_validation[[col]])])\n",
    "    if (is.infinite(max_val) || is.na(max_val)) max_val <- 0\n",
    "    dialogue_features_validation[[col]][is.infinite(dialogue_features_validation[[col]])] <- max_val * 2\n",
    "  }\n",
    "\n",
    "  # Ensure all features are numeric\n",
    "  dialogue_features_train[[col]] <- as.numeric(dialogue_features_train[[col]])\n",
    "  dialogue_features_validation[[col]] <- as.numeric(dialogue_features_validation[[col]])\n",
    "}\n",
    "\n",
    "# Handle any remaining NA values (e.g., from `mean` on empty sets, or `nchar` on NA text)\n",
    "# Impute with mean or median, or remove. For simplicity, let's use mean imputation for now.\n",
    "# Or, more robustly, use preProcess from caret.\n",
    "# Let's use mean imputation for NA values in features.\n",
    "for (col in features_to_use) {\n",
    "  # Training data NA imputation\n",
    "  if (any(is.na(dialogue_features_train[[col]]))) {\n",
    "    mean_val_train <- mean(dialogue_features_train[[col]], na.rm = TRUE)\n",
    "    dialogue_features_train[[col]][is.na(dialogue_features_train[[col]])] <- mean_val_train\n",
    "  }\n",
    "  # Validation data NA imputation (using training data's mean to prevent data leakage)\n",
    "  if (any(is.na(dialogue_features_validation[[col]]))) {\n",
    "    mean_val_validation <- mean(dialogue_features_train[[col]], na.rm = TRUE) # Use training mean\n",
    "    dialogue_features_validation[[col]][is.na(dialogue_features_validation[[col]])] <- mean_val_validation\n",
    "  }\n",
    "}\n",
    "\n",
    "# Ensure Usefulness_score is numeric\n",
    "dialogue_features_train$Usefulness_score <- as.numeric(dialogue_features_train$Usefulness_score)\n",
    "dialogue_features_validation$Usefulness_score <- as.numeric(dialogue_features_validation$Usefulness_score)\n",
    "\n",
    "# Check for any remaining NAs or Infs for the target variable\n",
    "dialogue_features_train <- dialogue_features_train[!is.na(dialogue_features_train$Usefulness_score) & !is.infinite(dialogue_features_train$Usefulness_score), ]\n",
    "dialogue_features_validation <- dialogue_features_validation[!is.na(dialogue_features_validation$Usefulness_score) & !is.infinite(dialogue_features_validation$Usefulness_score), ]\n",
    "\n",
    "# --- Model Training and Evaluation ---\n",
    "\n",
    "results <- list()\n",
    "\n",
    "# Define RMSE and MAE function\n",
    "RMSE <- function(y_true, y_pred) {\n",
    "  sqrt(mean((y_true - y_pred)^2))\n",
    "}\n",
    "\n",
    "MAE <- function(y_true, y_pred) {\n",
    "  mean(abs(y_true - y_pred))\n",
    "}\n",
    "\n",
    "# 1. Linear Regression\n",
    "cat(\"\\n--- Training Linear Regression Model ---\\n\")\n",
    "lm_model <- lm(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")])\n",
    "lm_predictions <- predict(lm_model, newdata = dialogue_features_validation)\n",
    "lm_rmse <- RMSE(dialogue_features_validation$Usefulness_score, lm_predictions)\n",
    "lm_mae <- MAE(dialogue_features_validation$Usefulness_score, lm_predictions)\n",
    "lm_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ lm_predictions))$r.squared # Calculate R-squared for predictions vs actual\n",
    "results[[\"Linear Regression\"]] <- list(RMSE = lm_rmse, MAE = lm_mae, R_squared = lm_r_squared)\n",
    "cat(paste0(\"Linear Regression - RMSE: \", round(lm_rmse, 4), \", MAE: \", round(lm_mae, 4), \", R-squared: \", round(lm_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# 2. Regression Tree (CART)\n",
    "cat(\"\\n--- Training Regression Tree Model ---\\n\")\n",
    "# Using rpart for regression tree\n",
    "# tuneGrid parameter can be used with train from caret for tuning\n",
    "# For simplicity, let's start with a default tree and then potentially tune.\n",
    "# We explicitly allow more features than the default maxdepth for deeper trees if needed.\n",
    "# For minsplit, a common value is 20 for larger datasets, but here with ~300 dialogues, keeping it low might be okay.\n",
    "rt_model <- rpart(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")], method = \"anova\", control = rpart.control(minsplit = 5, cp = 0.01))\n",
    "rt_predictions <- predict(rt_model, newdata = dialogue_features_validation)\n",
    "rt_rmse <- RMSE(dialogue_features_validation$Usefulness_score, rt_predictions)\n",
    "rt_mae <- MAE(dialogue_features_validation$Usefulness_score, rt_predictions)\n",
    "rt_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ rt_predictions))$r.squared\n",
    "results[[\"Regression Tree\"]] <- list(RMSE = rt_rmse, MAE = rt_mae, R_squared = rt_r_squared)\n",
    "cat(paste0(\"Regression Tree - RMSE: \", round(rt_rmse, 4), \", MAE: \", round(rt_mae, 4), \", R-squared: \", round(rt_r_squared, 4), \"\\n\"))\n",
    "# Plot the regression tree (optional, for visualization of a single tree)\n",
    "# rpart.plot(rt_model)\n",
    "\n",
    "\n",
    "# 3. Random Forest Regression\n",
    "cat(\"\\n--- Training Random Forest Regression Model ---\\n\")\n",
    "# Set a seed for reproducibility\n",
    "set.seed(123)\n",
    "# Random Forest can take longer to train.\n",
    "# `ntree` (number of trees) and `mtry` (number of variables randomly sampled as candidates at each split) are key parameters.\n",
    "# `mtry` is typically sqrt(number of features) for classification and number of features / 3 for regression.\n",
    "rf_model <- randomForest(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")],\n",
    "                         ntree = 500, mtry = max(floor(length(features_to_use) / 3), 1), importance = TRUE) # mtry for regression\n",
    "rf_predictions <- predict(rf_model, newdata = dialogue_features_validation)\n",
    "rf_rmse <- RMSE(dialogue_features_validation$Usefulness_score, rf_predictions)\n",
    "rf_mae <- MAE(dialogue_features_validation$Usefulness_score, rf_predictions)\n",
    "rf_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ rf_predictions))$r.squared\n",
    "results[[\"Random Forest\"]] <- list(RMSE = rf_rmse, MAE = rf_mae, R_squared = rf_r_squared)\n",
    "cat(paste0(\"Random Forest - RMSE: \", round(rf_rmse, 4), \", MAE: \", round(rf_mae, 4), \", R-squared: \", round(rf_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# 4. Support Vector Regression (SVR)\n",
    "cat(\"\\n--- Training Support Vector Regression (SVR) Model ---\\n\")\n",
    "# SVR can be sensitive to feature scaling. For this comparison, not explicitly scaling,\n",
    "# but for production, scaling features is highly recommended for SVR.\n",
    "# A common kernel is \"radial\" (RBF). Parameters include 'cost' (C) and 'gamma'.\n",
    "# These parameters can be tuned with `caret::train` for better performance.\n",
    "svr_model <- svm(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")],\n",
    "                 type = \"eps-regression\", kernel = \"radial\")\n",
    "svr_predictions <- predict(svr_model, newdata = dialogue_features_validation)\n",
    "svr_rmse <- RMSE(dialogue_features_validation$Usefulness_score, svr_predictions)\n",
    "svr_mae <- MAE(dialogue_features_validation$Usefulness_score, svr_predictions)\n",
    "svr_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ svr_predictions))$r.squared\n",
    "results[[\"SVR\"]] <- list(RMSE = svr_rmse, MAE = svr_mae, R_squared = svr_r_squared)\n",
    "cat(paste0(\"SVR - RMSE: \", round(svr_rmse, 4), \", MAE: \", round(svr_mae, 4), \", R-squared: \", round(svr_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# --- Report Findings and Identify Model 1 ---\n",
    "cat(\"\\n--- Model Performance Summary on Validation Set ---\\n\")\n",
    "performance_df <- do.call(rbind, lapply(names(results), function(model_name) {\n",
    "  data.frame(Model = model_name,\n",
    "             RMSE = results[[model_name]]$RMSE,\n",
    "             MAE = results[[model_name]]$MAE,\n",
    "             R_squared = results[[model_name]]$R_squared)\n",
    "}))\n",
    "print(performance_df)\n",
    "\n",
    "# Identify the best performing model (Model 1) based on RMSE (lower is better) and MAE (lower is better)\n",
    "# and R-squared (higher is better). A common practice is to prioritize RMSE/MAE.\n",
    "best_model_name <- performance_df$Model[which.min(performance_df$RMSE)]\n",
    "Model1 <- NULL # Placeholder for the best model object\n",
    "\n",
    "cat(paste0(\"\\nBest performing model (Model 1) based on RMSE: \", best_model_name, \"\\n\"))\n",
    "\n",
    "# Assign the best model object to Model1\n",
    "if (best_model_name == \"Linear Regression\") Model1 <- lm_model\n",
    "if (best_model_name == \"Regression Tree\") Model1 <- rt_model\n",
    "if (best_model_name == \"Random Forest\") Model1 <- rf_model\n",
    "if (best_model_name == \"SVR\") Model1 <- svr_model\n",
    "\n",
    "cat(\"\\nModel 1 has been identified and stored.\\n\")\n",
    "\n",
    "# Display the structure of Model1 (example for the best model)\n",
    "# print(Model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e9a321",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRows: \u001b[22m\u001b[34m1446\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m4\u001b[39m\n",
      "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
      "\u001b[1mDelimiter:\u001b[22m \",\"\n",
      "\u001b[31mchr\u001b[39m  (2): Utterance_text, Interlocutor\n",
      "\u001b[32mdbl\u001b[39m  (1): Dialogue_ID\n",
      "\u001b[34mdttm\u001b[39m (1): Timestamp\n",
      "\n",
      "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
      "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "65"
      ],
      "text/latex": [
       "65"
      ],
      "text/markdown": [
       "65"
      ],
      "text/plain": [
       "[1] 65"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "dialogue <- read_csv(\"git_ignore/TaskD_Dataset/dialogue_utterance_validation.csv\")\n",
    "length(unique(dialogue$Dialogue_ID))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d6e68",
   "metadata": {},
   "source": [
    "I need to train a machine learning model on dialogue utterance vs dialogue usefulness. you may use any language you want to do evaluation to report findings to me, but do provide R code for me to test and verify the findings.\n",
    "\n",
    "The data files I have, with the descriptions of their columns are:\n",
    "dialogue_utterance_train/validation/test.csv \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Usefulness_score - This score is given by a student to indicate their perceived \n",
    "usefulness of the FLoRA chatbot when answering the post-task \n",
    "questionnaire Question 3 (i.e., “To what extent do you think the \n",
    "GPT-powered chatbot on FLoRA is useful for you to accomplish \n",
    "the assignment?”). The value range of this feature is [1,5], with 1 \n",
    "representing “very unuseful”, 2 representing “unuseful”, 3 \n",
    "representing “neutral”, 4 representing “useful”, and 5 \n",
    "representing “very useful”.\n",
    "\n",
    "dialogue_usefulness_train/validation/test.csv\n",
    "Column Name - Description \n",
    "Dialogue_ID - The unique ID of a dialogue \n",
    "Timestamp - When an utterance contained in the dialogue was made \n",
    "Interlocutor - Whether the utterance was made by the student or the chatbot (\"Student\"/\"Chatbot\")\n",
    "Utterance_text - The text of the utterance\n",
    "\n",
    "dialogue_utterance_train has 117k lines, split over 303 unique dialogue IDs.\n",
    "the first task I had to do was:\n",
    "1.  What  features  can  you  engineer  to  empower  the  training  of  a  machine  learning model? You may propose as many as you believe are useful. Please note that the number of the features should not exceed the number of the dialogues contained in the training set. Otherwise, the constructed machine learning models are prone to have overfitting issues. Select two features that you propose and try to use boxplots to visualise  the  feature  value  between  the following two groups of dialogues in the training  set:  (i)  those  with  Usefulness_score  of  1  or  2;  and  (ii)  those  with Usefulness_score  of  4  or  5.  Show if there  any  difference  between  the  two  groups  of dialogues? How can you tell whether the difference is statistically significant? Ideally, identify features that display statistically significant differences. \n",
    "\n",
    "in the data, the student and chatbot will always take turns to talk, so the ratio of the student to chatbot conversation inputs will always be an identical 1:1 ratio.\n",
    "\n",
    "the code i had for my first task was:\n",
    "# Install and load necessary packages\n",
    "if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
    "if (!requireNamespace(\"ggplot2\", quietly = TRUE)) install.packages(\"ggplot2\")\n",
    "if (!requireNamespace(\"lubridate\", quietly = TRUE)) install.packages(\"lubridate\")\n",
    "if (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\n",
    "\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "\n",
    "# Load the training data\n",
    "df_utterance <- read.csv(\"git_ignore/dialogue_utterance_train.csv\")\n",
    "df_usefulness <- read.csv(\"git_ignore/dialogue_usefulness_train.csv\")\n",
    "\n",
    "# Merge the dataframes on Dialogue_ID\n",
    "# Use `left_join` to ensure all dialogues from df_utterance are kept,\n",
    "# as usefulness score is defined per dialogue.\n",
    "df_merged <- left_join(df_utterance, df_usefulness, by = \"Dialogue_ID\")\n",
    "\n",
    "# Convert Timestamp to datetime objects\n",
    "df_merged$Timestamp <- ymd_hms(df_merged$Timestamp)\n",
    "\n",
    "# Sort by Dialogue_ID and Timestamp for accurate sequential calculations\n",
    "df_merged <- df_merged %>%\n",
    "  arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "dialogue_features <- df_merged %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    num_utterances = n(),\n",
    "    total_dialogue_length = sum(nchar(Utterance_text), na.rm = TRUE),\n",
    "    dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "    avg_len_student_utterance = mean(nchar(Utterance_text[Interlocutor == \"Student\"]), na.rm = TRUE),\n",
    "    avg_len_chatbot_utterance = mean(nchar(Utterance_text[Interlocutor == \"Chatbot\"]), na.rm = TRUE),\n",
    "    num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "    num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE)\n",
    "  )\n",
    "\n",
    "# Handle cases where a specific interlocutor might not have uttered anything (NaNs from mean)\n",
    "dialogue_features$avg_len_student_utterance[is.nan(dialogue_features$avg_len_student_utterance)] <- 0\n",
    "dialogue_features$avg_len_chatbot_utterance[is.nan(dialogue_features$avg_len_chatbot_utterance)] <- 0\n",
    "\n",
    "# Calculate average time between utterances\n",
    "# This needs to be done iteratively per dialogue or using a custom function\n",
    "avg_time_between_utterances <- df_merged %>%\n",
    "  group_by(Dialogue_ID) %>%\n",
    "  summarise(\n",
    "    time_diffs = list(as.numeric(diff(Timestamp), units = \"secs\")),\n",
    "    avg_time_between_utterances = ifelse(length(time_diffs[[1]]) > 0, mean(time_diffs[[1]], na.rm = TRUE), 0)\n",
    "  ) %>%\n",
    "  select(Dialogue_ID, avg_time_between_utterances)\n",
    "\n",
    "dialogue_features <- left_join(dialogue_features, avg_time_between_utterances, by = \"Dialogue_ID\")\n",
    "\n",
    "# New Feature: Ratio of average student utterance length to average chatbot utterance length\n",
    "dialogue_features <- dialogue_features %>%\n",
    "  mutate(\n",
    "    ratio_student_chatbot_len = ifelse(avg_len_chatbot_utterance > 0, avg_len_student_utterance / avg_len_chatbot_utterance,\n",
    "                                       ifelse(avg_len_student_utterance > 0, Inf, 0)) # Handle division by zero\n",
    "  )\n",
    "\n",
    "# Add Usefulness_score to the dialogue features dataframe\n",
    "df_utterance_scores_unique <- df_merged %>%\n",
    "  select(Dialogue_ID, Usefulness_score) %>%\n",
    "  distinct()\n",
    "\n",
    "dialogue_features <- left_join(dialogue_features, df_utterance_scores_unique, by = \"Dialogue_ID\")\n",
    "\n",
    "# Verify the number of features does not exceed 303 (number of unique dialogues)\n",
    "# Exclude Dialogue_ID and Usefulness_score from the count\n",
    "num_features_engineered <- ncol(dialogue_features) - 2\n",
    "num_unique_dialogues <- n_distinct(dialogue_features$Dialogue_ID)\n",
    "\n",
    "cat(paste0(\"Number of engineered features: \", num_features_engineered, \"\\n\"))\n",
    "cat(paste0(\"Number of unique dialogues in training set: \", num_unique_dialogues, \"\\n\"))\n",
    "cat(paste0(\"All engineered features: \", paste(setdiff(names(dialogue_features), c(\"Dialogue_ID\", \"Usefulness_score\")), collapse = \", \"), \"\\n\\n\"))\n",
    "\n",
    "# --- Feature Selection for Visualization ---\n",
    "# Let's select 'total_dialogue_length' and 'ratio_student_chatbot_len' for visualization.\n",
    "# These seem to capture different aspects of conversation dynamics related to engagement/usefulness.\n",
    "\n",
    "selected_features <- c('total_dialogue_length', 'ratio_student_chatbot_len')\n",
    "\n",
    "# --- Data Preparation for Visualization ---\n",
    "# Create groups based on Usefulness_score\n",
    "group1 <- dialogue_features %>% filter(Usefulness_score %in% c(1, 2))\n",
    "group2 <- dialogue_features %>% filter(Usefulness_score %in% c(4, 5))\n",
    "\n",
    "# Combine data for plotting\n",
    "plot_data <- data.frame(\n",
    "  Feature = character(),\n",
    "  Value = numeric(),\n",
    "  Score_Group = character(),\n",
    "  stringsAsFactors = FALSE\n",
    ")\n",
    "\n",
    "for (feature in selected_features) {\n",
    "  # Add 'Unuseful' group data\n",
    "  plot_data <- rbind(plot_data, data.frame(\n",
    "    Feature = feature,\n",
    "    Value = group1[[feature]],\n",
    "    Score_Group = \"Unuseful (1-2)\"\n",
    "  ))\n",
    "  # Add 'Useful' group data\n",
    "  plot_data <- rbind(plot_data, data.frame(\n",
    "    Feature = feature,\n",
    "    Value = group2[[feature]],\n",
    "    Score_Group = \"Useful (4-5)\"\n",
    "  ))\n",
    "}\n",
    "\n",
    "# Convert 'Feature' to a factor for faceting in ggplot\n",
    "plot_data$Feature <- factor(plot_data$Feature, levels = selected_features)\n",
    "\n",
    "# --- Visualization ---\n",
    "# Ensure the Value column is numeric for plotting\n",
    "plot_data$Value <- as.numeric(plot_data$Value)\n",
    "\n",
    "ggplot(plot_data, aes(x = Score_Group, y = Value, fill = Score_Group)) +\n",
    "  geom_boxplot() +\n",
    "  facet_wrap(~ Feature, scales = \"free_y\", ncol = 2) +\n",
    "  labs(\n",
    "    title = \"Distribution of Engineered Features by Dialogue Usefulness Score\",\n",
    "    x = \"Usefulness Score Group\",\n",
    "    y = \"Feature Value\"\n",
    "  ) +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")\n",
    "\n",
    "# --- Statistical Significance Tests ---\n",
    "cat(\"\\n--- Statistical Significance Tests ---\\n\")\n",
    "\n",
    "for (feature in selected_features) {\n",
    "  cat(paste0(\"Feature: \", feature, \"\\n\"))\n",
    "  \n",
    "  # Extract data for the two groups, removing NA values\n",
    "  data_group1 <- na.omit(group1[[feature]])\n",
    "  data_group2 <- na.omit(group2[[feature]])\n",
    "  \n",
    "  # Check if there's enough data for both groups\n",
    "  if (length(data_group1) > 1 && length(data_group2) > 1) {\n",
    "    # Perform Welch's t-test (does not assume equal variances)\n",
    "    ttest_result <- t.test(data_group1, data_group2, var.equal = FALSE)\n",
    "    cat(paste0(\"  Welch's t-test p-value: \", round(ttest_result$p.value, 4), \"\\n\"))\n",
    "    \n",
    "    if (ttest_result$p.value < 0.05) {\n",
    "      cat(\"  Difference is statistically significant (p < 0.05)\\n\")\n",
    "    } else {\n",
    "      cat(\"  Difference is not statistically significant (p >= 0.05)\\n\")\n",
    "    }\n",
    "    \n",
    "    # Also perform Wilcoxon rank-sum test (non-parametric, does not assume normality)\n",
    "    wilcox_result <- wilcox.test(data_group1, data_group2)\n",
    "    cat(paste0(\"  Wilcoxon rank-sum test p-value: \", round(wilcox_result$p.value, 4), \"\\n\"))\n",
    "    \n",
    "    if (wilcox_result$p.value < 0.05) {\n",
    "      cat(\"  Wilcoxon rank-sum test: Difference is statistically significant (p < 0.05)\\n\")\n",
    "    } else {\n",
    "      cat(\"  Wilcoxon rank-sum test: Difference is not statistically significant (p >= 0.05)\\n\")\n",
    "    }\n",
    "  } else {\n",
    "    cat(\"  Not enough data in one or both groups to perform statistical tests.\\n\")\n",
    "  }\n",
    "  cat(\"\\n\")\n",
    "}\n",
    "\n",
    "the next task i need to do now is:\n",
    "2.  Build a machine learning model (e.g., polynomial regressions, regression tree) based on the training set by taking all the features that you have proposed and evaluate the performance of the model on the validation set using the relevant evaluation metrics you learned in class. Aim to include at least 5 features in this model. The best-performing model here is denoted as Model 1. \n",
    "for task 2, show me the reasoning for the machine learning models you are considering, and then show me the results of the performance of each model you try, before shortlisting the best-performing model as Model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a66f15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb749da0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of engineered features (updated): 17\n",
      "Number of unique dialogues in training set: 303\n",
      "All engineered features (updated): num_utterances, total_dialogue_length_words, dialogue_duration, avg_len_student_utterance_words, avg_len_chatbot_utterance_words, num_student_questions, num_chatbot_questions, avg_readability_score_student, avg_readability_score_chatbot, num_unique_words_student, num_unique_words_chatbot, total_words_student, total_words_chatbot, ttr_student, ttr_chatbot, variance_time_between_utterances, ratio_student_chatbot_len_words\n",
      "\n",
      "\n",
      "--- Training Linear Regression Model ---\n",
      "Linear Regression - RMSE: 1.1116, MAE: 0.9044, R-squared: 0.0015\n",
      "\n",
      "--- Training Regression Tree Model ---\n",
      "Regression Tree - RMSE: 1.2592, MAE: 0.9909, R-squared: 0.0195\n",
      "\n",
      "--- Training Random Forest Regression Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in randomForest.default(m, y, ...):\n",
      "\"The response has five or fewer unique values.  Are you sure you want to do regression?\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - RMSE: 0.9883, MAE: 0.8109, R-squared: 0.08\n",
      "\n",
      "--- Training Support Vector Regression (SVR) Model ---\n",
      "SVR - RMSE: 0.9884, MAE: 0.8007, R-squared: 0.0892\n",
      "\n",
      "--- Model Performance Summary on Validation Set (with new features) ---\n",
      "              Model      RMSE       MAE   R_squared\n",
      "1 Linear Regression 1.1116204 0.9044433 0.001518223\n",
      "2   Regression Tree 1.2592470 0.9909295 0.019494171\n",
      "3     Random Forest 0.9882938 0.8108662 0.079954901\n",
      "4               SVR 0.9883981 0.8006706 0.089168566\n",
      "\n",
      "Best performing model (Model 1) based on RMSE: Random Forest\n",
      "\n",
      "Model 1 has been identified and stored.\n"
     ]
    }
   ],
   "source": [
    "# Install and load necessary packages\n",
    "if (!requireNamespace(\"dplyr\", quietly = TRUE)) install.packages(\"dplyr\")\n",
    "if (!requireNamespace(\"ggplot2\", quietly = TRUE)) install.packages(\"ggplot2\")\n",
    "if (!requireNamespace(\"lubridate\", quietly = TRUE)) install.packages(\"lubridate\")\n",
    "if (!requireNamespace(\"stringr\", quietly = TRUE)) install.packages(\"stringr\")\n",
    "if (!requireNamespace(\"caret\", quietly = TRUE)) install.packages(\"caret\") # For model training and evaluation\n",
    "if (!requireNamespace(\"randomForest\", quietly = TRUE)) install.packages(\"randomForest\") # For Random Forest\n",
    "if (!requireNamespace(\"e1071\", quietly = TRUE)) install.packages(\"e1071\") # For SVR\n",
    "if (!requireNamespace(\"rpart\", quietly = TRUE)) install.packages(\"rpart\") # For Regression Tree\n",
    "if (!requireNamespace(\"rpart.plot\", quietly = TRUE)) install.packages(\"rpart.plot\") # For plotting regression tree\n",
    "# New packages for advanced text features\n",
    "if (!requireNamespace(\"quanteda\", quietly = TRUE)) install.packages(\"quanteda\")\n",
    "if (!requireNamespace(\"quanteda.textstats\", quietly = TRUE)) install.packages(\"quanteda.textstats\")\n",
    "\n",
    "\n",
    "library(dplyr)\n",
    "library(lubridate)\n",
    "library(stringr)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(e1071)\n",
    "library(rpart)\n",
    "library(rpart.plot)\n",
    "library(quanteda)\n",
    "library(quanteda.textstats)\n",
    "\n",
    "\n",
    "# --- Load Training and Validation Data ---\n",
    "df_utterance_train <- read.csv(\"git_ignore/dialogue_utterance_train.csv\")\n",
    "df_usefulness_train <- read.csv(\"git_ignore/dialogue_usefulness_train.csv\")\n",
    "df_utterance_validation <- read.csv(\"git_ignore/dialogue_utterance_validation.csv\")\n",
    "df_usefulness_validation <- read.csv(\"git_ignore/dialogue_usefulness_validation.csv\")\n",
    "\n",
    "# Merge the dataframes on Dialogue_ID\n",
    "df_merged_train <- left_join(df_utterance_train, df_usefulness_train, by = \"Dialogue_ID\")\n",
    "df_merged_validation <- left_join(df_utterance_validation, df_usefulness_validation, by = \"Dialogue_ID\")\n",
    "\n",
    "# Convert Timestamp to datetime objects\n",
    "df_merged_train$Timestamp <- ymd_hms(df_merged_train$Timestamp)\n",
    "df_merged_validation$Timestamp <- ymd_hms(df_merged_validation$Timestamp)\n",
    "\n",
    "# Sort by Dialogue_ID and Timestamp for accurate sequential calculations\n",
    "df_merged_train <- df_merged_train %>% arrange(Dialogue_ID, Timestamp)\n",
    "df_merged_validation <- df_merged_validation %>% arrange(Dialogue_ID, Timestamp)\n",
    "\n",
    "\n",
    "# --- New Feature Engineering Steps ---\n",
    "\n",
    "# Step 1: Calculate Readability Scores per Utterance\n",
    "# Create a unique ID for each utterance to be used as docid in quanteda corpus\n",
    "df_merged_train$utterance_id <- paste0(\"train_utt_\", 1:nrow(df_merged_train))\n",
    "df_merged_validation$utterance_id <- paste0(\"val_utt_\", 1:nrow(df_merged_validation))\n",
    "\n",
    "calculate_readability <- function(df_merged_data) {\n",
    "  # Create a corpus from the merged data, using 'utterance_id' as document IDs\n",
    "  utterance_corpus <- corpus(df_merged_data, text_field = \"Utterance_text\", docid_field = \"utterance_id\")\n",
    "\n",
    "  readability_scores <- data.frame(utterance_id = character(), readability_score = numeric(), stringsAsFactors = FALSE)\n",
    "\n",
    "  # Check if the corpus is not empty\n",
    "  if (ndoc(utterance_corpus) > 0) {\n",
    "    # Calculate Flesch-Kincaid readability for each document (utterance) in the corpus\n",
    "    # textstat_readability works directly on a corpus object\n",
    "    readability_results <- textstat_readability(utterance_corpus, measure = \"Flesch.Kincaid\") %>%\n",
    "      select(document, Flesch.Kincaid) %>%\n",
    "      rename(utterance_id = document, readability_score = Flesch.Kincaid)\n",
    "\n",
    "    readability_scores <- readability_results\n",
    "  }\n",
    "\n",
    "\n",
    "  # Join back to the original merged dataframe\n",
    "  # Ensure all original utterances are kept, with NA for those without readability score (e.g., empty text)\n",
    "  df_with_readability <- left_join(df_merged_data, readability_scores, by = \"utterance_id\")\n",
    "\n",
    "  # Fill NA readability scores with 0 or a reasonable default (e.g., if utterance was empty or calculation failed)\n",
    "  df_with_readability$readability_score[is.na(df_with_readability$readability_score)] <- 0\n",
    "  return(df_with_readability)\n",
    "}\n",
    "\n",
    "df_merged_train_with_readability <- calculate_readability(df_merged_train)\n",
    "df_merged_validation_with_readability <- calculate_readability(df_merged_validation)\n",
    "\n",
    "\n",
    "# Step 2: Function to engineer all features for a given dataframe\n",
    "engineer_features <- function(df_merged_data_with_readability) {\n",
    "  dialogue_features_df <- df_merged_data_with_readability %>%\n",
    "    group_by(Dialogue_ID) %>%\n",
    "    summarise(\n",
    "      num_utterances = n(),\n",
    "\n",
    "      # Changed to word count\n",
    "      total_dialogue_length_words = sum(sapply(str_split(Utterance_text, \"\\\\s+\"), length), na.rm = TRUE),\n",
    "      dialogue_duration = as.numeric(difftime(max(Timestamp), min(Timestamp), units = \"secs\")),\n",
    "\n",
    "      # Changed to word count\n",
    "      avg_len_student_utterance_words = mean(sapply(str_split(Utterance_text[Interlocutor == \"Student\"], \"\\\\s+\"), length), na.rm = TRUE),\n",
    "      avg_len_chatbot_utterance_words = mean(sapply(str_split(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\s+\"), length), na.rm = TRUE),\n",
    "\n",
    "      num_student_questions = sum(str_detect(Utterance_text[Interlocutor == \"Student\"], \"\\\\?\"), na.rm = TRUE),\n",
    "      num_chatbot_questions = sum(str_detect(Utterance_text[Interlocutor == \"Chatbot\"], \"\\\\?\"), na.rm = TRUE),\n",
    "\n",
    "      # New: Lexical Richness/Diversity (by dialogue, per interlocutor)\n",
    "      # Collect all words from student/chatbot utterances in this dialogue\n",
    "      all_student_words = list(unlist(str_split(paste(Utterance_text[Interlocutor == \"Student\"], collapse = \" \"), \"\\\\s+\"))),\n",
    "      all_chatbot_words = list(unlist(str_split(paste(Utterance_text[Interlocutor == \"Chatbot\"], collapse = \" \"), \"\\\\s+\"))),\n",
    "\n",
    "      # New: Average Readability Scores per interlocutor (averaged over their utterances in the dialogue)\n",
    "      avg_readability_score_student = mean(readability_score[Interlocutor == \"Student\"], na.rm = TRUE),\n",
    "      avg_readability_score_chatbot = mean(readability_score[Interlocutor == \"Chatbot\"], na.rm = TRUE),\n",
    "\n",
    "      # New: Turn-taking and Interruption Metrics\n",
    "      time_diffs_raw = list(as.numeric(diff(Timestamp), units = \"secs\"))\n",
    "    )\n",
    "\n",
    "  # Post-summarize calculations for features that need list columns or additional processing\n",
    "  dialogue_features_df <- dialogue_features_df %>%\n",
    "    mutate(\n",
    "      # Lexical Richness/Diversity continued\n",
    "      # Filter out empty strings from word lists before counting unique words\n",
    "      num_unique_words_student = sapply(all_student_words, function(x) length(unique(x[x != \"\" & !is.na(x)]))),\n",
    "      num_unique_words_chatbot = sapply(all_chatbot_words, function(x) length(unique(x[x != \"\" & !is.na(x)]))),\n",
    "\n",
    "      # Calculate total words for TTR accurately for the entire dialogue for each interlocutor\n",
    "      total_words_student = sapply(all_student_words, function(x) length(x[x != \"\" & !is.na(x)])),\n",
    "      total_words_chatbot = sapply(all_chatbot_words, function(x) length(x[x != \"\" & !is.na(x)])),\n",
    "\n",
    "      # Calculate TTR, handle division by zero\n",
    "      ttr_student = ifelse(total_words_student > 0, num_unique_words_student / total_words_student, 0),\n",
    "      ttr_chatbot = ifelse(total_words_chatbot > 0, num_unique_words_chatbot / total_words_chatbot, 0),\n",
    "\n",
    "      # Turn-taking and Interruption Metrics continued (Variance of time between utterances)\n",
    "      # Ensure there's enough data points for variance calculation (at least 2 time differences, so 3 utterances)\n",
    "      variance_time_between_utterances = sapply(time_diffs_raw, function(x) ifelse(length(x) > 1, var(x, na.rm = TRUE), 0))\n",
    "    ) %>%\n",
    "    select(-all_student_words, -all_chatbot_words, -time_diffs_raw) # Remove temporary list columns\n",
    "\n",
    "  # Re-calculate ratio_student_chatbot_len based on word counts\n",
    "  dialogue_features_df <- dialogue_features_df %>%\n",
    "    mutate(\n",
    "      ratio_student_chatbot_len_words = ifelse(avg_len_chatbot_utterance_words > 0, avg_len_student_utterance_words / avg_len_chatbot_utterance_words,\n",
    "                                               ifelse(avg_len_student_utterance_words > 0, Inf, 0)) # Handle division by zero\n",
    "    )\n",
    "\n",
    "  # Add Usefulness_score to the dialogue features dataframe\n",
    "  df_usefulness_scores_unique <- df_merged_data_with_readability %>%\n",
    "    select(Dialogue_ID, Usefulness_score) %>%\n",
    "    distinct()\n",
    "  dialogue_features_df <- left_join(dialogue_features_df, df_usefulness_scores_unique, by = \"Dialogue_ID\")\n",
    "\n",
    "  return(dialogue_features_df)\n",
    "}\n",
    "\n",
    "dialogue_features_train <- engineer_features(df_merged_train_with_readability)\n",
    "dialogue_features_validation <- engineer_features(df_merged_validation_with_readability)\n",
    "\n",
    "\n",
    "# --- Handle Inf and NA values in engineered features ---\n",
    "# Identify features for training (all engineered features except Dialogue_ID and Usefulness_score)\n",
    "features_to_use <- setdiff(names(dialogue_features_train), c(\"Dialogue_ID\", \"Usefulness_score\"))\n",
    "\n",
    "for (col in features_to_use) {\n",
    "  # Convert to numeric if not already (important for Inf/NA checks)\n",
    "  dialogue_features_train[[col]] <- as.numeric(dialogue_features_train[[col]])\n",
    "  dialogue_features_validation[[col]] <- as.numeric(dialogue_features_validation[[col]])\n",
    "\n",
    "  # Handling Inf values\n",
    "  if (any(is.infinite(dialogue_features_train[[col]]))) {\n",
    "    max_finite_val_train <- max(dialogue_features_train[[col]][is.finite(dialogue_features_train[[col]])], na.rm = TRUE)\n",
    "    if (is.infinite(max_finite_val_train) || is.na(max_finite_val_train)) {\n",
    "        dialogue_features_train[[col]][is.infinite(dialogue_features_train[[col]])] <- 1000 # Default if no finite values\n",
    "    } else {\n",
    "        dialogue_features_train[[col]][is.infinite(dialogue_features_train[[col]])] <- max_finite_val_train + 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (any(is.infinite(dialogue_features_validation[[col]]))) {\n",
    "    # Use the max finite value from the training set for validation set to prevent data leakage\n",
    "    max_finite_val_validation_ref <- max(dialogue_features_train[[col]][is.finite(dialogue_features_train[[col]])], na.rm = TRUE)\n",
    "    if (is.infinite(max_finite_val_validation_ref) || is.na(max_finite_val_validation_ref)) {\n",
    "        dialogue_features_validation[[col]][is.infinite(dialogue_features_validation[[col]])] <- 1000\n",
    "    } else {\n",
    "        dialogue_features_validation[[col]][is.infinite(dialogue_features_validation[[col]])] <- max_finite_val_validation_ref + 1\n",
    "    }\n",
    "  }\n",
    "\n",
    "  # Impute NA values using the mean from the training data for both train and validation\n",
    "  mean_val_train <- mean(dialogue_features_train[[col]], na.rm = TRUE)\n",
    "  dialogue_features_train[[col]][is.na(dialogue_features_train[[col]])] <- mean_val_train\n",
    "\n",
    "  # Use training mean for validation set to prevent data leakage\n",
    "  dialogue_features_validation[[col]][is.na(dialogue_features_validation[[col]])] <- mean_val_train\n",
    "}\n",
    "\n",
    "# Ensure Usefulness_score is numeric and remove any NAs/Infs from the target variable\n",
    "dialogue_features_train$Usefulness_score <- as.numeric(dialogue_features_train$Usefulness_score)\n",
    "dialogue_features_validation$Usefulness_score <- as.numeric(dialogue_features_validation$Usefulness_score)\n",
    "\n",
    "dialogue_features_train <- dialogue_features_train[!is.na(dialogue_features_train$Usefulness_score) & !is.infinite(dialogue_features_train$Usefulness_score), ]\n",
    "dialogue_features_validation <- dialogue_features_validation[!is.na(dialogue_features_validation$Usefulness_score) & !is.infinite(dialogue_features_validation$Usefulness_score), ]\n",
    "\n",
    "# Verify the number of features does not exceed 303 (number of unique dialogues)\n",
    "# Exclude Dialogue_ID and Usefulness_score from the count\n",
    "num_engineered_features_updated <- ncol(dialogue_features_train) - 2\n",
    "num_unique_dialogues_train <- n_distinct(dialogue_features_train$Dialogue_ID)\n",
    "\n",
    "cat(paste0(\"\\nNumber of engineered features (updated): \", num_engineered_features_updated, \"\\n\"))\n",
    "cat(paste0(\"Number of unique dialogues in training set: \", num_unique_dialogues_train, \"\\n\"))\n",
    "cat(paste0(\"All engineered features (updated): \", paste(setdiff(names(dialogue_features_train), c(\"Dialogue_ID\", \"Usefulness_score\")), collapse = \", \"), \"\\n\\n\"))\n",
    "\n",
    "\n",
    "# --- Model Training and Evaluation (Same as before, but with new features) ---\n",
    "\n",
    "results <- list()\n",
    "\n",
    "# Define RMSE and MAE function\n",
    "RMSE <- function(y_true, y_pred) {\n",
    "  sqrt(mean((y_true - y_pred)^2))\n",
    "}\n",
    "\n",
    "MAE <- function(y_true, y_pred) {\n",
    "  mean(abs(y_true - y_pred))\n",
    "}\n",
    "\n",
    "# 1. Linear Regression\n",
    "cat(\"\\n--- Training Linear Regression Model ---\\n\")\n",
    "lm_model <- lm(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")])\n",
    "lm_predictions <- predict(lm_model, newdata = dialogue_features_validation)\n",
    "lm_rmse <- RMSE(dialogue_features_validation$Usefulness_score, lm_predictions)\n",
    "lm_mae <- MAE(dialogue_features_validation$Usefulness_score, lm_predictions)\n",
    "lm_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ lm_predictions))$r.squared\n",
    "results[[\"Linear Regression\"]] <- list(RMSE = lm_rmse, MAE = lm_mae, R_squared = lm_r_squared)\n",
    "cat(paste0(\"Linear Regression - RMSE: \", round(lm_rmse, 4), \", MAE: \", round(lm_mae, 4), \", R-squared: \", round(lm_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# 2. Regression Tree (CART)\n",
    "cat(\"\\n--- Training Regression Tree Model ---\\n\")\n",
    "rt_model <- rpart(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")], method = \"anova\", control = rpart.control(minsplit = 5, cp = 0.01))\n",
    "rt_predictions <- predict(rt_model, newdata = dialogue_features_validation)\n",
    "rt_rmse <- RMSE(dialogue_features_validation$Usefulness_score, rt_predictions)\n",
    "rt_mae <- MAE(dialogue_features_validation$Usefulness_score, rt_predictions)\n",
    "rt_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ rt_predictions))$r.squared\n",
    "results[[\"Regression Tree\"]] <- list(RMSE = rt_rmse, MAE = rt_mae, R_squared = rt_r_squared)\n",
    "cat(paste0(\"Regression Tree - RMSE: \", round(rt_rmse, 4), \", MAE: \", round(rt_mae, 4), \", R-squared: \", round(rt_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# 3. Random Forest Regression\n",
    "cat(\"\\n--- Training Random Forest Regression Model ---\\n\")\n",
    "set.seed(123)\n",
    "rf_model <- randomForest(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")],\n",
    "                         ntree = 500, mtry = max(floor(length(features_to_use) / 3), 1), importance = TRUE)\n",
    "rf_predictions <- predict(rf_model, newdata = dialogue_features_validation)\n",
    "rf_rmse <- RMSE(dialogue_features_validation$Usefulness_score, rf_predictions)\n",
    "rf_mae <- MAE(dialogue_features_validation$Usefulness_score, rf_predictions)\n",
    "rf_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ rf_predictions))$r.squared\n",
    "results[[\"Random Forest\"]] <- list(RMSE = rf_rmse, MAE = rf_mae, R_squared = rf_r_squared)\n",
    "cat(paste0(\"Random Forest - RMSE: \", round(rf_rmse, 4), \", MAE: \", round(rf_mae, 4), \", R-squared: \", round(rf_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# 4. Support Vector Regression (SVR)\n",
    "cat(\"\\n--- Training Support Vector Regression (SVR) Model ---\\n\")\n",
    "svr_model <- svm(Usefulness_score ~ ., data = dialogue_features_train[, c(features_to_use, \"Usefulness_score\")],\n",
    "                 type = \"eps-regression\", kernel = \"radial\")\n",
    "svr_predictions <- predict(svr_model, newdata = dialogue_features_validation)\n",
    "svr_rmse <- RMSE(dialogue_features_validation$Usefulness_score, svr_predictions)\n",
    "svr_mae <- MAE(dialogue_features_validation$Usefulness_score, svr_predictions)\n",
    "svr_r_squared <- summary(lm(dialogue_features_validation$Usefulness_score ~ svr_predictions))$r.squared\n",
    "results[[\"SVR\"]] <- list(RMSE = svr_rmse, MAE = svr_mae, R_squared = svr_r_squared)\n",
    "cat(paste0(\"SVR - RMSE: \", round(svr_rmse, 4), \", MAE: \", round(svr_mae, 4), \", R-squared: \", round(svr_r_squared, 4), \"\\n\"))\n",
    "\n",
    "\n",
    "# --- Report Findings and Identify Model 1 ---\n",
    "cat(\"\\n--- Model Performance Summary on Validation Set (with new features) ---\\n\")\n",
    "performance_df <- do.call(rbind, lapply(names(results), function(model_name) {\n",
    "  data.frame(Model = model_name,\n",
    "             RMSE = results[[model_name]]$RMSE,\n",
    "             MAE = results[[model_name]]$MAE,\n",
    "             R_squared = results[[model_name]]$R_squared)\n",
    "}))\n",
    "print(performance_df)\n",
    "\n",
    "# Identify the best performing model (Model 1) based on RMSE (lower is better)\n",
    "best_model_name <- performance_df$Model[which.min(performance_df$RMSE)]\n",
    "Model1 <- NULL # Placeholder for the best model object\n",
    "\n",
    "cat(paste0(\"\\nBest performing model (Model 1) based on RMSE: \", best_model_name, \"\\n\"))\n",
    "\n",
    "# Assign the best model object to Model1\n",
    "if (best_model_name == \"Linear Regression\") Model1 <- lm_model\n",
    "if (best_model_name == \"Regression Tree\") Model1 <- rt_model\n",
    "if (best_model_name == \"Random Forest\") Model1 <- rf_model\n",
    "if (best_model_name == \"SVR\") Model1 <- svr_model\n",
    "\n",
    "cat(\"\\nModel 1 has been identified and stored.\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
