{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d4a339",
   "metadata": {},
   "source": [
    "# Week 7 - Supervised Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1130c26",
   "metadata": {},
   "source": [
    "## Learning Outcomes (Week 7)\n",
    "By the end of this week, you should be able to:  \n",
    "- Understand basic statistical modelling  \n",
    "- Understand the difference between causation and correlation  \n",
    "- Utilise linear regression and polynominal regression  \n",
    "- Fit linear regression and polynomial regression models to a given dataset  \n",
    "- Utilise classification and regression trees  \n",
    "- Understand basic deep learning models  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e1acc",
   "metadata": {},
   "source": [
    "### Statistical modelling\n",
    "Models represent aspects of a scenario to help us understand it.  \n",
    "Statistical models represent the relationships between variables  \n",
    "- Independent variable(s)  \n",
    "- Dependent variable  \n",
    "\n",
    "A model can be used to predict about the dependent variable, given information about the independent variable(s)  \n",
    "Rather than trying to use all data about the scenario, the model just reduces the data set to a low dimensional summary.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c68c15",
   "metadata": {},
   "source": [
    "### Causation vs. correlation\n",
    "Variables in a scenario have relationships  \n",
    "Some variables influence the outcome of activities and thus other variables.  \n",
    "- Dependent vs independent  \n",
    "Influence diagrams model that dependency  \n",
    "It is not always easy to recognise what influences what  \n",
    "\n",
    "Causation implies correlation (normally), **but** correlation does *not* imply causation  \n",
    "Identifying causation requires controlled experiments that examine the data related to a situation with or without a possible correlated variable.  \n",
    "E.g. sunburn and ice cream are correlated.  \n",
    "\n",
    "With enough data, correlation is often enough to predict values  \n",
    "But, it is still important to understand the data and the context  \n",
    "E.g. number of people drowning by falling in swimming pools correlates with number of films Nicolas Cage appears in  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f4740b",
   "metadata": {},
   "source": [
    "### Regression models\n",
    "- Linear regression models  \n",
    "- Polynomial regression models  \n",
    "- Regression trees  \n",
    "- Classification trees  \n",
    "- Basic deep learning  \n",
    "\n",
    "Raising degree of polynomial regression induces risk of overfitting  \n",
    "- Poor generalization  \n",
    "Too low a degree of polynomial regression induces risk of underfitting  \n",
    "- Cannot capture model complexities  \n",
    "\n",
    "Bayesian information criterion (BIC) includes a penalty for\n",
    "using more variables. The preferred model is the model with\n",
    "the lowest BIC.  \n",
    "\n",
    "Other similar measures include the adjusted-R2, which\n",
    "imposes a penalty on additional variables that do not have a\n",
    "significant effect on explaining y. The preferred model is the\n",
    "model with the higher adjusted-R2.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a094e2",
   "metadata": {},
   "source": [
    "### Regression Trees\n",
    "A regression tree is a supervised machine learning\n",
    "algorithm that predicts a continuous-valued response\n",
    "variable by learning decision rules from the predictors (or\n",
    "independent variables).  \n",
    "- Decision tree  \n",
    "\n",
    "divide the data into subsets of similar values  \n",
    "estimate the response within each subset.  \n",
    "\n",
    "Binary tree structure  \n",
    "Terminal/leaf nodes are where model prediction is made  \n",
    "Paths from root node to terminal node represents decision rules  \n",
    "\n",
    "#### Choosing Segments\n",
    "1. Identify all possible (n-1) splits of the data.  \n",
    "2. Compute a metric that measures the quality of each possible split.  \n",
    "3. Choose the best split, break the data into two subsets.  \n",
    "4. Repeat steps 1 to 3 on each subset, then continue until a good stopping point is reached.  \n",
    "\n",
    "Anova:  \n",
    "- The partitioning is a top-down, greedy approach.  \n",
    "    - Start with all data  \n",
    "    - Once split, don’t change  \n",
    "- Searches every distinct value of every input predictor to find a pair of\n",
    "  predictor/value that best split the data into two subgroups (G1 and G2).  \n",
    "- As in for the population inside that node, this pair of predictor/value\n",
    "  improves the chosen criteria (e.g., ANOVA) the most.  \n",
    "\n",
    "#### Pros\n",
    "- The resulting tree is easy to understand  \n",
    "- Visualising the tree can reveal crucial information, such as how\n",
    "  decision rules are formed, the importance of different predictors\n",
    "  and the effect of the splitting points in the predictors.  \n",
    "- It can reveal information about the relationships between variables.  \n",
    "- Very useful for Exploratory Data Analysis (EDA)  \n",
    "- Implicitly performs feature selection as some of the predictors\n",
    "  may not be included in the tree.  \n",
    "- Not sensitive to the presence of missing values and outliers.  \n",
    "- No assumptions about the shape and the distribution of the data.  \n",
    "- It can be used to fit non-linear relationships.  \n",
    "\n",
    "#### Cons\n",
    "- The fit has a high variance meaning small changes in the\n",
    "  data set can lead to an entirely different tree.  \n",
    "    - Overfitting is a problem for decision tree models, but we can\n",
    "      adjust the stopping conditions and prune the tree.  \n",
    "- Can be inefficient when performing an exhaustive search\n",
    "  for the splitting points of continuous numerical predictors.  \n",
    "- Greedy algorithms cannot guarantee the return of the\n",
    "  globally optimal regression tree.  \n",
    "    - Global optimal may be on the other side of the tree  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f32e511",
   "metadata": {},
   "source": [
    "### Classification Trees\n",
    "• For classification task, if we want to use a decision tree,\n",
    "the result is a classification tree.  \n",
    "• Most popular split criteria are Gini and Entropy.  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ad334",
   "metadata": {},
   "source": [
    "### Basic deep learning models\n",
    "MLP  \n",
    "- basic foundation of deep learning  \n",
    "- calculate difference between label (truth) and output  \n",
    "- this is the loss  \n",
    "- adjust weights to minimise loss so output matches the label  \n",
    "\n",
    "CNN, RNN, LSTM\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
