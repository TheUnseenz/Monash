{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGTRJ9T-W2UD"
      },
      "outputs": [],
      "source": [
        "# If you are using Google Colab, you can mount your Google Drive to access your files.\n",
        "# If you are using local machine, you can skip this step.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MW5JNa5SscF"
      },
      "source": [
        "# **Week 5 Applied Session: Text Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nH8KEEbSc2g"
      },
      "source": [
        "This week, we focus on the basic techniques for the text data preprocessing. We will cover the following topics:\n",
        "\n",
        "* 1. Basic Text Pre-Processing Technologies\n",
        "* 2. Exploring Pre-Processed text and Generating Features\n",
        "* 3. Practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT3afWLISc2h"
      },
      "source": [
        "## **1. Basic Text Pre-Processing Technologies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgwyKk2DSc2h"
      },
      "source": [
        "The aim of this tutorial is to demonstrate the basic technologies used to pre-process text data in the text mining, Information Retrieval (IR) and Natural Language Processing (NLP) communities. Those technologies include\n",
        "* Tokenizing text\n",
        "* Removing stop words\n",
        "* Stemming & Lemmatization\n",
        "* Sentence segmentation\n",
        "\n",
        "The ultimate goal of pre-processing text is to convert unstructured and free language text into structured data so that text analysis algorithms can directly take the structured data as input. For example, the UCI machine learning database provides free download of the bag-of-words datasets that contain ENRON emails, NIPS articles, the New York Times news articles, <a href=\"https://www.ncbi.nlm.nih.gov/pubmed\">PubMed</a> articles. Those are the bench mark datasets used in text analysis. Lets have a look at one of the datasets, PubMed. The image below shows a screenshot of the first 15 lines in the data set\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1U6IrUW8_HG-OEWPQc3dra20O6nnOFPI9\">\n",
        "\n",
        "The the three lines are the total number of PubMed abstracts, the vocabulary size, and the total number of work tokens in the datasets. Each abstract is stored in a sparse format that is often used in text analysis, where each row contains document ID, word index and the corresponding word count in the document. For example, \"1 6811 1\" means word 6811 appears in document 1 just once. To find the word string for \"6811\", you then go to the vocabulary and find the 6811th word. Now, how can we pre-process text data and save the processed data in the spare format.\n",
        "\n",
        "Assume that we are going to analyze some medical reports that are about fungal disease. The goal of the analysis is to predict how likely a patient has fungal infection given some diagnostic report. The prediction can be formulated as a classification task where we are going to assign a binary label to a patient. 1 means the patient has fungal infection, and 0 means the patient does not. The text in the following cell contains a short diagnostic report for a patient. In this tutorial, you are going to learn the basic techniques often used in preprocessing text. In next tutorial, you will learn how to put these techniques together to count vocabulary and generate the final structure data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZlhxjF0Sc2i"
      },
      "source": [
        "Before we start, lets's prepare a text data for the example. The text data will be used to demonstrate the basic text pre-processing technologies in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Fv13Bqg-Sc2j",
        "outputId": "89994076-ace3-4406-80ee-7ba493baeb68"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'previous right upper lobe nodule? fungal question resolution change. findings: comparison is made to prior ct dated november 30, 2004. significant resolution in the previously noted fluid overload status. ectasia of the thoracic aorta measuring 4.2 cm. features of generalised centrilobular emphysema. resolution of right upper lobe nodule. there is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature. given the interval development of this fungal/inflammatory aetiology is likely. there is a 13 mm right axillary node which is a new finding since the prior study. no significant mediastinal or hilar adenopathy. conclusion: nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text = \"\"\"Previous right upper lobe nodule? Fungal question resolution change. Findings: Comparison is made to prior CT dated November 30, 2004. Significant resolution in the previously noted fluid overload status. Ectasia of the thoracic aorta measuring 4.2 cm. Features of generalised centrilobular emphysema. Resolution of right upper lobe nodule. There is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature. Given the interval development of this fungal/inflammatory aetiology is likely. There is a 13 mm right axillary node which is a new finding since the prior study. No significant mediastinal or hilar adenopathy. Conclusion: Nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\"\"\"\n",
        "raw_text = raw_text.lower()\n",
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO4rHoB4Sc2l"
      },
      "source": [
        "### 1.1 Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eseDVA6YSc2m"
      },
      "source": [
        "\n",
        "Now, we need to think about how to break such a long sequence of characters into word tokens. The task of breaking a character sequence into pieces is known as tokenization. In the lecture, we have covered different tokenizers built in NLTK. For example, whitespace tokenizer, regular expression tokenizer and etc. You can find more information on the NLTK website, e.g.,\n",
        "* <a href=\"http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize\">tokenize module</a> in nltk.\n",
        "* <a href=\"http://www.nltk.org/howto/tokenize.html\">tokenize</a>: shows you how to use Treebank tokenizer and Regexp tokenizer\n",
        "\n",
        "You can also refer to the Jupyter Notebook we provided. After tokenizing the <font color=\"brown\">raw_text</font>, you should derive the following list of tokens\n",
        "```\n",
        "['previous', 'right', 'upper', 'lobe', 'nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'features', 'of', 'generalised', 'centrilobular', 'emphysema', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory', 'aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE0pIGdISc2m"
      },
      "source": [
        "`nltk.tokenize` is a module that provides a number of tokenizers. In this tutorial, we are going to use the `RegexpTokenizer` to tokenize the text. The `RegexpTokenizer` allows you to specify a regular expression to match the tokens. For example, the regular expression `r'\\w+'` matches any word character. The following code shows how to use the `RegexpTokenizer` to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIIrQKZmSc2m",
        "outputId": "848cb6a1-37f1-425b-cc48-a155f5374feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previous', 'right', 'upper', 'lobe', 'nodule', 'fungal', 'question', 'resolution', 'change', 'findings']\n",
            "['comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004']\n",
            "['significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia']\n",
            "['of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'features', 'of', 'generalised']\n",
            "['centrilobular', 'emphysema', 'resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'there', 'is']\n",
            "['now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of']\n",
            "['the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is']\n",
            "['non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal']\n",
            "['inflammatory', 'aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right']\n",
            "['axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior']\n",
            "['study', 'no', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'conclusion', 'nodule', 'in']\n",
            "['the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "### write your code with regular expression below\n",
        "tokenizer = RegexpTokenizer(r\"\\w+(?:[-.]\\w+)?\")\n",
        "unigram_tokens = tokenizer.tokenize(raw_text)\n",
        "# print the tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(unigram_tokens), 10):\n",
        "    print(unigram_tokens[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC1sW_jgSc2m"
      },
      "source": [
        "The tokens are all unigrams. Except for \"non-specific\" that contains a hyphen and numbers, all the other tokens are single word tokens. As we know, phrases are more meaningful than single word, which makes us think that it would be good to tokenize a text so that phrases are kept as phrases. Then, the question is how can we merge multi-word expressions into single tokens. Assume that we are going to have the following multi-word expressions being treated as single tokens\n",
        "* \"<font color=\"red\">generalised centrilobular emphysema</font>\"\n",
        "* \"<font color=\"red\">inflammatory aetiology</font>\"\n",
        "* \"<font color=\"red\">lobe nodule</font>\"\n",
        "* \"<font color=\"red\">axillary node</font>\"\n",
        "* \"<font color=\"red\">thoracic aorta measuring</font>\"\n",
        "\n",
        "In other words, you cannot split those phrases into individual words. It is lucky that NLTK provides us a <a href=\"http://www.nltk.org/_modules/nltk/tokenize/mwe.html#MWETokenizer\">multi-word expression tokenizer</a>. The output should be\n",
        "```\n",
        "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of', 'the', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'of', 'generalised_centrilobular_emphysema', 'resolution', 'of', 'right', 'upper', 'lobe_nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory_aetiology', 'is', 'likely', 'there', 'is', 'a', '13', 'mm', 'right', 'axillary_node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar_adenopathy', 'conclusion', 'nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory_aetiology']\n",
        "```\n",
        "\n",
        "Firstly, you should think about how to expand the list of unique words give by the unigram tokenizer above. In order to get a unique list of tokens (about 76 tokens in total), you can use <font color=\"blue\">set</font> function, then convert the set to a list, and append the list with multi-word phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cE-MFqDSc2n",
        "outputId": "e7778532-8748-4836-e4ad-bccc8390bd89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 76 unique tokens in the unigram vocabulary.\n"
          ]
        }
      ],
      "source": [
        "### write your code below\n",
        "tokens = ['Previous', 'right', 'upper', 'lobe', 'nodule', 'Fungal', 'question', 'resolution', 'change', 'Findings', 'Comparison', 'is', 'made', 'to', 'prior', 'CT', 'dated', 'November', '30', '2004', 'Significant', 'resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'Ectasia', 'of', 'the', 'thoracic', 'aorta', 'measuring', '4.2', 'cm', 'Features', 'of', 'generalised', 'centrilobular', 'emphysema', 'Resolution', 'of', 'right', 'upper', 'lobe', 'nodule', 'There', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within', 'the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures', '5.4', 'mmand', 'is', 'non-specific', 'in', 'nature', 'Given', 'the', 'interval', 'development', 'of', 'this', 'fungal', 'inflammatory', 'aetiology', 'is', 'likely', 'There', 'is', 'a', '13', 'mm', 'right', 'axillary', 'node', 'which', 'is', 'a', 'new', 'finding', 'since', 'the', 'prior', 'study', 'No', 'significant', 'mediastinal', 'or', 'hilar', 'adenopathy', 'Conclusion', 'Nodule', 'in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory', 'aetiology']\n",
        "# using the set() function to remove duplicates\n",
        "uni_voc = list(set(unigram_tokens))\n",
        "print(f\"There are {len(uni_voc)} unique tokens in the unigram vocabulary.\")\n",
        "uni_voc.append(('generalised', 'centrilobular', 'emphysema'))\n",
        "uni_voc.append(('inflammatory', 'aetiology'))\n",
        "uni_voc.append(('hilar', 'adenopathy'))\n",
        "uni_voc.append(('lobe', 'nodule'))\n",
        "uni_voc.append(('axillary', 'node'))\n",
        "uni_voc.append(('thoracic', 'aorta', 'measuring'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHj9rsZiSc2n"
      },
      "source": [
        "Now we can have a look at the list of unique tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFUHBdIjSc2n",
        "outputId": "06de3ad9-5dd6-46e9-d7e3-cec475676740"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['or', 'november', 'overload', 'significant', 'a', 'no', 'presence', 'prior', 'of', 'now']\n",
            "['emphysema', '4.2', 'question', 'which', 'ectasia', 'inflammatory', 'is', 'noted', 'fluid', 'aorta']\n",
            "['within', 'mm', 'change', 'interval', 'comparison', 'likely', 'mediastinal', 'features', 'aetiology', 'new']\n",
            "['non-specific', 'fungal', 'previously', 'to', 'finding', 'resolution', 'measures', 'keeping', 'status', 'and']\n",
            "['lobe', 'this', '2004', '5.4', 'the', 'right', 'nodule', 'dated', 'nature', 'lower']\n",
            "['made', 'given', 'cm', 'development', 'hilar', 'ct', 'there', 'node', 'medial', 'study']\n",
            "['since', 'centrilobular', 'adenopathy', 'previous', 'measuring', 'upper', 'in', 'conclusion', 'with', 'generalised']\n",
            "['findings', 'thoracic', 'segment', '13', 'axillary', '30', ('generalised', 'centrilobular', 'emphysema'), ('inflammatory', 'aetiology'), ('hilar', 'adenopathy'), ('lobe', 'nodule')]\n",
            "[('axillary', 'node'), ('thoracic', 'aorta', 'measuring')]\n"
          ]
        }
      ],
      "source": [
        "# print the tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(uni_voc), 10):\n",
        "    print(uni_voc[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZecWGF1Sc2o"
      },
      "source": [
        "`MWETokenizer` is a multi-word expression tokenizer that allows you to specify a list of multi-word expressions.\n",
        "Now, we can use the `MWETokenizer` to tokenize the <font color=\"brown\">unigram_tokens</font> from raw_text. The following code shows how to use the `MWETokenizer` to tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1I6kZXySc2o",
        "outputId": "2e7d966d-00ef-40b9-ed14-db29124c49b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison']\n",
            "['is', 'made', 'to', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant']\n",
            "['resolution', 'in', 'the', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'of']\n",
            "['the', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'of', 'generalised_centrilobular_emphysema', 'resolution', 'of', 'right']\n",
            "['upper', 'lobe_nodule', 'there', 'is', 'now', 'presence', 'of', 'a', 'nodule', 'within']\n",
            "['the', 'medial', 'segment', 'of', 'the', 'right', 'lower', 'lobe', 'which', 'measures']\n",
            "['5.4', 'mm', 'and', 'is', 'non-specific', 'in', 'nature', 'given', 'the', 'interval']\n",
            "['development', 'of', 'this', 'fungal', 'inflammatory_aetiology', 'is', 'likely', 'there', 'is', 'a']\n",
            "['13', 'mm', 'right', 'axillary_node', 'which', 'is', 'a', 'new', 'finding', 'since']\n",
            "['the', 'prior', 'study', 'no', 'significant', 'mediastinal', 'or', 'hilar_adenopathy', 'conclusion', 'nodule']\n",
            "['in', 'the', 'right', 'lower', 'lobe', 'in', 'keeping', 'with', 'fungal', 'inflammatory_aetiology']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "### write your code below\n",
        "mwe_tokenizer = MWETokenizer(uni_voc)\n",
        "mwe_tokens = mwe_tokenizer.tokenize(unigram_tokens)\n",
        "# print the tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(mwe_tokens), 10):\n",
        "    print(mwe_tokens[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxIJMQaoSc2o"
      },
      "source": [
        "From above we can see that the <font color='brown'>raw_text</font> has been split into a list of tokens that contains both unigrams and multi-word expressions. However, the list contains a lot of functional words, such as \"to\", \"in\", \"the\", \"is\" and so on. These functional words usually do not contribute much to the semantics of the text, except for increase the dimensionality of the data in text analysis. Also, note that our goal is to build a classification model of predicting fungal disease. Thus, we are more interested in the meaning of the diagnostic report than the syntax. Therefore, we can choose to remove those words, which is your next task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oipU23jqSc2o"
      },
      "source": [
        "You can also have try the different tokenizer on line at http://text-processing.com/demo/tokenize/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ospoV7cPSc2o"
      },
      "source": [
        "### 1.2 Removing Stop Words\n",
        "\n",
        "As we have discussed in the lecture and in the Jupyter Notebook, stop words carry little lexical content. They are often functional words in English, for example, articles, pronouns, particles, and so on. In NLP and IR, we usually exclude stop words from the vocabulary. Otherwise, we will face the curse of dimensionality. There are some exceptions, such as syntactic analysis like parsing, we choose to keep those functional words. However, you are going to remove all the stop words in the above list by using the stop word list in `NLTK`, which is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tqtCdOESc2p",
        "outputId": "b007db85-54ae-411e-88af-f1d3316c122b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47nrfYm2Sc2p"
      },
      "source": [
        "`nltk.corpus` is a module that provides a number of corpora, including the stop words list. The following code shows how to use the `stopwords` corpus to remove stop words from the list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHICK1VQSc2p",
        "outputId": "e74b2d7d-ba74-4efb-c090-34b8c7f5f319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\"]\n",
            "['as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\"]\n",
            "['d', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few']\n",
            "['for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\"]\n",
            "[\"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\"]\n",
            "['in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm']\n",
            "['ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not']\n",
            "['now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own']\n",
            "['re', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some']\n",
            "['such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\"]\n",
            "[\"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn']\n",
            "[\"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who']\n",
            "['whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours']\n",
            "['yourself', 'yourselves', \"you've\"]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# get the list of stopwords\n",
        "stopwords_list = stopwords.words('english')\n",
        "# print the stopwords, every 15 stopwords in a new line\n",
        "for i in range(0, len(stopwords_list), 15):\n",
        "    print(stopwords_list[i:i+15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-_YV9e-Sc2p"
      },
      "source": [
        "Now, it is your turn to remove all the stop words from the output of your <b>MWEtokenizer</b>. While writing the list comprehension code, you should think about the difference between <font color=\"blue\">list</font> and <font color=\"blue\">set</font>. Sets are significantly faster when your task is to determine if an object is present in the set, but are slower than lists when you try to iterate over the elements. The list of tokens after stop words be removed should be\n",
        "\n",
        "`['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'ct', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'mm', 'non-specific', 'nature', 'given', 'interval', 'development', 'fungal', 'inflammatory_aetiology', 'likely', '13', 'mm', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'conclusion', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'fungal', 'inflammatory_aetiology']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeVs3-LOSc2p",
        "outputId": "b1da6677-1e16-41c1-c26c-99c89c531268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previous', 'right', 'upper', 'lobe_nodule', 'fungal', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'ct', 'dated', 'november']\n",
            "['30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'cm', 'features', 'generalised_centrilobular_emphysema']\n",
            "['resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'mm']\n",
            "['non-specific', 'nature', 'given', 'interval', 'development', 'fungal', 'inflammatory_aetiology', 'likely', '13', 'mm', 'right', 'axillary_node', 'new', 'finding', 'since']\n",
            "['prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'conclusion', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'fungal', 'inflammatory_aetiology']\n"
          ]
        }
      ],
      "source": [
        "# get the set of stopwords\n",
        "stopwords_set = set(stopwords_list)\n",
        "# remove the stopwords from the mwe_tokens\n",
        "stopped_tokens = [w for w in mwe_tokens if w not in stopwords_set]\n",
        "# print the tokens, every 15 tokens in a new line\n",
        "for i in range(0, len(stopped_tokens), 15):\n",
        "    print(stopped_tokens[i:i+15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pk3C-23DSc2p",
        "outputId": "fc181f0a-ec0d-473c-fa98-2c8de219656b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of tokens before removing stopwords is 110\n",
            "The number of tokens after removing stopwords is 73\n",
            "The different tokens are {'or', 'and', 'a', 'no', 'this', 'the', 'in', 'now', 'of', 'which', 'to', 'with', 'is', 'there'}\n"
          ]
        }
      ],
      "source": [
        "# compare the difference between mwe_tokens and stopped_tokens\n",
        "print(f\"The number of tokens before removing stopwords is {len(mwe_tokens)}\")\n",
        "print(f\"The number of tokens after removing stopwords is {len(stopped_tokens)}\")\n",
        "diff_tokens = set(mwe_tokens) - set(stopped_tokens)\n",
        "print(f\"The different tokens are {diff_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7al5RmRSc2p"
      },
      "source": [
        "Of course, you can use a rich stopword list, as the one used in the lecture. You can also expand the stopword list by adding corpus specific stop words, for example those more frequent words. By more frequent, we meant that the words appear in every document, they do not help us distinguish documents. For example, the following words do appear in each diagnostic report. (In next tutorial, we will demonstrate how to use basic statistics to identify them.)\n",
        "* <font color=\"red\">ct</font>\n",
        "* <font color=\"red\">mm</font>\n",
        "* <font color=\"red\">cm</font>\n",
        "* <font color=\"red\">fungal</font>\n",
        "* <font color=\"red\">conclusion</font>\n",
        "\n",
        "You task is to expand the stopword list with the four words, and process the list of tokens again. The output should be\n",
        "```\n",
        "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'findings', 'comparison', 'made', 'prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid', 'overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper', 'lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures', '5.4', 'non-specific', 'nature', 'given', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right', 'axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule', 'right', 'lower', 'lobe', 'keeping', 'inflammatory_aetiology']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeL858ETSc2q",
        "outputId": "05fa5125-f854-411a-a53a-8ac4776c2b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of tokens after adding new stopwords is 65\n",
            "['november', 'overload', 'significant', 'presence', 'prior', 'finding', '4.2', 'question', 'ectasia', 'study']\n",
            "['noted', 'fluid', 'interval', 'within', 'mediastinal', 'change', 'axillary_node', 'comparison', 'likely', 'hilar_adenopathy']\n",
            "['features', 'new', 'non-specific', 'lobe_nodule', 'previously', 'thoracic_aorta_measuring', 'resolution', 'measures', 'keeping', 'status']\n",
            "['lobe', '2004', '5.4', 'inflammatory_aetiology', 'right', 'nodule', 'dated', 'nature', 'lower', 'made']\n",
            "['given', 'development', 'medial', 'since', 'previous', 'upper', 'findings', 'segment', '13', '30']\n",
            "['generalised_centrilobular_emphysema']\n"
          ]
        }
      ],
      "source": [
        "# add the following words to the stopwords set\n",
        "stopwords_set.add('ct')\n",
        "stopwords_set.add('mm')\n",
        "stopwords_set.add('cm')\n",
        "stopwords_set.add('fungal')\n",
        "stopwords_set.add('conclusion')\n",
        "stopped_tokens = [w for w in mwe_tokens if w not in stopwords_set]\n",
        "print(f\"The number of tokens after adding new stopwords is {len(stopped_tokens)}\")\n",
        "# print the tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(set(stopped_tokens)), 10):\n",
        "    print(list(set(stopped_tokens))[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ntigit-0Sc2q"
      },
      "source": [
        "Again, we should inspect the output, which is a very good practice in data preprocessing. You will find that we have words like \"find\" and \"findings\", \"previous\" and \"previously\", \"noted\", etc. Should we keep them as they are? or Should we reduce them to the base form?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQiAR0HqSc2q"
      },
      "source": [
        "### 1.3 Stemming, Lemmatization, setence segmentation and POS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BtFTp9gSc2q"
      },
      "source": [
        "The task of stemming and lemmatization is to reduce the same word in different lexical forms to its base form in the lexicon without significantly loosing the meaning (a.k.a. word normalisation). In English, nouns are inflected in the plural, verbs are inflected in the various tenses, and adjectives are inflected in the comparative/superlative. In morphology, the derivation process creates a new word out of an existing one often by adding either a prefix or a suffix. In this exercise, you are going to apply the <b>PorterStemmer</b> and <b>WordNetLemmatizer</b> provided in the NLTK's <a href=\"http://www.nltk.org/api/nltk.stem.html\">stem</a> package to perform this task.\n",
        "\n",
        "Note that the <b>stemming</b> and the <b>lemmatization</b> can serve as alternative methods for word normalisation.\n",
        "\n",
        "\n",
        "- `Stemming` involves removing suffixes or prefixes from words to obtain their root or base form, irrespective of the word's context or meaning. This results in words that may not be actual words in the language but are linguistically related. For example, the word \"running\" would be stemmed to \"run,\" but \"runs\" and \"ran\" would also be stemmed to \"run.\"\n",
        "- `Lemmatization` takes into account the context and meaning of words, mapping them to their dictionary form or lemma. This process considers factors such as part of speech and grammatical rules, ensuring that the resulting word is a valid word in the language. For instance, \"ran\" would be lemmatized to \"run,\" and \"better\" would be lemmatized to \"good.\" Therefore, the sentence-level structure need to be kept in order to considering the context and meaning of the words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD55GVeXSc2q"
      },
      "source": [
        "#### 1.3.1 Stemming\n",
        "Normally, stemming process happens after stop word removal. However, it depends on the text analysis tasks and the context. If you need to remove the stop words after stemming, you need to stem the stop words as well.\n",
        "\n",
        "`nltk.stem` is a module that provides a number of stemmers. In this tutorial, we are going to use the `PorterStemmer` to stem the list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bhY3SmkHSc2q"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "# create an instance of the PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUO5q0gjSc2q",
        "outputId": "5d5d6af6-0f5f-4070-b85c-3b948a1131de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previou', 'right', 'upper', 'lobe_nodul', 'question', 'resolut', 'chang', 'find', 'comparison', 'made']\n",
            "['prior', 'date', 'novemb', '30', '2004', 'signific', 'resolut', 'previous', 'note', 'fluid']\n",
            "['overload', 'statu', 'ectasia', 'thoracic_aorta_measur', '4.2', 'featur', 'generalised_centrilobular_emphysema', 'resolut', 'right', 'upper']\n",
            "['lobe_nodul', 'presenc', 'nodul', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measur']\n",
            "['5.4', 'non-specif', 'natur', 'given', 'interv', 'develop', 'inflammatory_aetiolog', 'like', '13', 'right']\n",
            "['axillary_nod', 'new', 'find', 'sinc', 'prior', 'studi', 'signific', 'mediastin', 'hilar_adenopathi', 'nodul']\n",
            "['right', 'lower', 'lobe', 'keep', 'inflammatory_aetiolog']\n"
          ]
        }
      ],
      "source": [
        "final_tokens_stemmed = []\n",
        "for token in stopped_tokens:\n",
        "    final_tokens_stemmed = final_tokens_stemmed + [stemmer.stem(token)]\n",
        "# print the tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(final_tokens_stemmed), 10):\n",
        "    print(final_tokens_stemmed[i:i+10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSPRPp-gSc2r",
        "outputId": "de224116-9224-4edd-f8c4-e1c0dfffc2e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of tokens before stemming is 51\n",
            "The number of tokens after stemming is 50\n"
          ]
        }
      ],
      "source": [
        "print(f\"The number of tokens before stemming is {len(set(stopped_tokens))}\")\n",
        "print(f\"The number of tokens after stemming is {len(set(final_tokens_stemmed))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6GF0zo-Sc2r"
      },
      "source": [
        "<span style='color:red'>Whats the difference before and after stemming?</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEQc11eZSc2r"
      },
      "source": [
        "#### 1.3.2 Sentence Segmentation\n",
        "\n",
        "`Lemmatization`, unlike stemming, it operates on sentence level, necessitating prior steps such as sentence segmentation and part-of-speech (POS) tagging to provide the necessary linguistic context. By leveraging this linguistic information, lemmatization ensures that words are transformed to their correct dictionary form, accounting for variations in inflection and tense.\n",
        "\n",
        "Note that the <b>WordNetLemmatizer</b> can take the POS tag of each word as one argument, specifying which can give us more accurate base form of the word.\n",
        "\n",
        "Therefore, first you should carry out sentence segmentation. You code should produce\n",
        "```\n",
        "previous right upper lobe nodule?\n",
        "fungal question resolution change.\n",
        "findings: comparison is made to prior ct dated november 30, 2004. significant resolution in the previously noted fluid overload status.\n",
        "ectasia of the thoracic aorta measuring 4.2 cm.\n",
        "features of generalised centrilobular emphysema.\n",
        "resolution of right upper lobe nodule.\n",
        "there is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature.\n",
        "given the interval development of this fungal/inflammatory aetiology is likely.\n",
        "there is a 13 mm right axillary node which is a new finding since the prior study.\n",
        "no significant mediastinal or hilar adenopathy.\n",
        "conclusion: nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\n",
        "```\n",
        "\n",
        "In order to segment the given text into sentences, you can refer to the Jupyter Notebook (chapter 1) or search \"<b>Punkt Sentence Tokenizer</a>\" on the http://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_UcCy6lzSc2r"
      },
      "outputs": [],
      "source": [
        "# nltk.download('punkt')\n",
        "# package name changed to punkt_tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpR7fRg2Wtfy",
        "outputId": "1ff72774-0761-4d5c-ceea-b796d432f336"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKdRRtCxSc2r",
        "outputId": "48003816-c113-4ae2-ab57-f9e2a3782339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "previous right upper lobe nodule?\n",
            "fungal question resolution change.\n",
            "findings: comparison is made to prior ct dated november 30, 2004. significant resolution in the previously noted fluid overload status.\n",
            "ectasia of the thoracic aorta measuring 4.2 cm.\n",
            "features of generalised centrilobular emphysema.\n",
            "resolution of right upper lobe nodule.\n",
            "there is now presence of a nodule within the medial segment of the right lower lobe which measures 5.4 mm and is non-specific in nature.\n",
            "given the interval development of this fungal/inflammatory aetiology is likely.\n",
            "there is a 13 mm right axillary node which is a new finding since the prior study.\n",
            "no significant mediastinal or hilar adenopathy.\n",
            "conclusion: nodule in the right lower lobe in keeping with fungal/inflammatory aetiology.\n"
          ]
        }
      ],
      "source": [
        "import nltk.data\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "### write your code below\n",
        "sentences = sent_detector.tokenize(raw_text.strip())\n",
        "for sent in sentences:\n",
        "    print (sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkASLc_HSc2r"
      },
      "source": [
        "#### 1.3.3 POS Tagging\n",
        "\n",
        "Then, we will use the POS tagger to assign POS tag to each word in each sentence. Please refer to Section 1 of http://www.nltk.org/book/ch05.html. The step you are going to use is:\n",
        "\n",
        "For each sentence\n",
        "1. use the unigram tokenizer you developed in Exercise 1 to tokenize the sentence\n",
        "2. use the <b>MWETokenizer</b> used in Exercise 1 to tokenize the sentence with multi-word expressions\n",
        "3. use the information in Section 1 of http://www.nltk.org/book/ch05.html to help you finish the POS tagging.\n",
        "4. remove the stop words in each sentence\n",
        "\n",
        "Finally save the tagged sentences in a list. The output you will derive should be\n",
        "```\n",
        "[[('previous', 'JJ'), ('right', 'JJ'), ('upper', 'NN'), ('lobe_nodule', 'NN')], [('question', 'NN'), ('resolution', 'NN'), ('change', 'NN')], [('findings', 'NNS'), ('comparison', 'NN'), ('made', 'VBN'), ('prior', 'VB'), ('dated', 'JJ'), ('november', 'RB'), ('30', 'CD'), ('2004', 'CD'), ('significant', 'JJ'), ('resolution', 'NN'), ('previously', 'RB'), ('noted', 'VBN'), ('fluid', 'NN'), ('overload', 'NN'), ('status', 'NN')], [('ectasia', 'NN'), ('thoracic_aorta_measuring', 'VBG'), ('4.2', 'CD')], [('features', 'NNS'), ('generalised_centrilobular_emphysema', 'NN')], [('resolution', 'NN'), ('right', 'JJ'), ('upper', 'JJ'), ('lobe_nodule', 'NN')], [('presence', 'NN'), ('nodule', 'NN'), ('within', 'IN'), ('medial', 'JJ'), ('segment', 'NN'), ('right', 'JJ'), ('lower', 'JJR'), ('lobe', 'NN'), ('measures', 'VBZ'), ('5.4', 'CD'), ('non-specific', 'JJ'), ('nature', 'NN')], [('given', 'VBN'), ('interval', 'NN'), ('development', 'NN'), ('inflammatory_aetiology', 'NN'), ('likely', 'JJ')], [('13', 'CD'), ('right', 'NN'), ('axillary_node', 'NN'), ('new', 'JJ'), ('finding', 'NN'), ('since', 'IN'), ('prior', 'JJ'), ('study', 'NN')], [('significant', 'JJ'), ('mediastinal', 'NN'), ('hilar_adenopathy', 'NN')], [('nodule', 'NN'), ('right', 'NN'), ('lower', 'JJR'), ('lobe', 'NN'), ('keeping', 'VBG'), ('inflammatory_aetiology', 'NN')]]\n",
        "```\n",
        "Now, your task is to fill the for loop below by following the steps above. If you would like to know the meaning of each tag, you can type for example\n",
        "```\n",
        "print(nltk.help.upenn_tagset('NNP'))\n",
        "```\n",
        "Replacing \"NNP\" with the tag you want, you should see the explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWCCxd5rSc2s"
      },
      "source": [
        "`nltk.download('tagsets')` is a module that provides a number of tagsets. In this tutorial, we are going to use the `upenn_tagset` to get the meaning of each tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM9PZosFSc2t",
        "outputId": "fa02cad8-12df-4da4-8146-82f58387acca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# nltk.download('tagsets')\n",
        "# package name changed to tagsets_json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9KocKP_XEKg",
        "outputId": "247b1818-dbe3-4b79-c135-d98623fc1e6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('tagsets_json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXICf7nGSc2t"
      },
      "source": [
        "`nltk.help.upenn_tagset('NNP')` will show you the explanation of the tag \"NNP\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uqx11thSc2t",
        "outputId": "de7bd2e0-c711-46e0-ef71-43c1f8276b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n"
          ]
        }
      ],
      "source": [
        "nltk.help.upenn_tagset('NNP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fR8mlyySc2t"
      },
      "source": [
        "`nltk.download('averaged_perceptron_tagger')` is a module that provides a number of taggers. In this tutorial, we are going to use the `averaged_perceptron_tagger` to tag the list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m56RuKj3Sc2t",
        "outputId": "981004f1-3205-4d48-ecdb-c3623b758762"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFn8tW__Sc2t"
      },
      "source": [
        "Now we can have a look at the tagged sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lbaX0FgSc2t",
        "outputId": "16a74046-2869-4096-8f45-d56096c67052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('previous', 'JJ'), ('right', 'JJ'), ('upper', 'NN'), ('lobe_nodule', 'NN')]\n",
            "[('question', 'NN'), ('resolution', 'NN'), ('change', 'NN')]\n",
            "[('findings', 'NNS'), ('comparison', 'NN'), ('made', 'VBN'), ('prior', 'VB'), ('dated', 'JJ'), ('november', 'RB'), ('30', 'CD'), ('2004', 'CD'), ('significant', 'JJ'), ('resolution', 'NN'), ('previously', 'RB'), ('noted', 'VBN'), ('fluid', 'NN'), ('overload', 'NN'), ('status', 'NN')]\n",
            "[('ectasia', 'NN'), ('thoracic_aorta_measuring', 'VBG'), ('4.2', 'CD')]\n",
            "[('features', 'NNS'), ('generalised_centrilobular_emphysema', 'NN')]\n",
            "[('resolution', 'NN'), ('right', 'JJ'), ('upper', 'JJ'), ('lobe_nodule', 'NN')]\n",
            "[('presence', 'NN'), ('nodule', 'NN'), ('within', 'IN'), ('medial', 'JJ'), ('segment', 'NN'), ('right', 'JJ'), ('lower', 'JJR'), ('lobe', 'NN'), ('measures', 'VBZ'), ('5.4', 'CD'), ('non-specific', 'JJ'), ('nature', 'NN')]\n",
            "[('given', 'VBN'), ('interval', 'NN'), ('development', 'NN'), ('inflammatory_aetiology', 'NN'), ('likely', 'JJ')]\n",
            "[('13', 'CD'), ('right', 'NN'), ('axillary_node', 'NN'), ('new', 'JJ'), ('finding', 'NN'), ('since', 'IN'), ('prior', 'JJ'), ('study', 'NN')]\n",
            "[('significant', 'JJ'), ('mediastinal', 'NN'), ('hilar_adenopathy', 'NN')]\n",
            "[('nodule', 'NN'), ('right', 'NN'), ('lower', 'JJR'), ('lobe', 'NN'), ('keeping', 'VBG'), ('inflammatory_aetiology', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "tagged_sents = []\n",
        "for sent in sentences:\n",
        "    # tokenize the sentence\n",
        "    uni_sent = tokenizer.tokenize(sent)\n",
        "    mwe_text = mwe_tokenizer.tokenize(uni_sent)\n",
        "    tagged_sent = nltk.tag.pos_tag(mwe_text)\n",
        "    stopped_tagged_sent = [x for x in tagged_sent if x[0] not in stopwords_set]\n",
        "    tagged_sents.append(stopped_tagged_sent)\n",
        "# print the tagged sentences\n",
        "for sent in tagged_sents:\n",
        "    print(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc12YFzySc2u"
      },
      "source": [
        "#### 1.3.4 Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMYKCny4Sc2u"
      },
      "source": [
        "The last step is to apply the <b>WordNetLemmatizer</b>. You code should make use the POS tags of each word to decide the lexical base form. The <font color=\"blue\">lemmatize</font> function in <b>WordNetLemmatizer</b> can accept the following wordnet tags\n",
        "* wordnet.ADJ\n",
        "* wordnet.VERB\n",
        "* wordnet.NOUN\n",
        "* wordnet.ADV\n",
        "\n",
        "The function of converting POS tags to wordnet tags is given bellow. In you code, you should think about how to call the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "pszvX_X4Sc2u"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# The code in this cell is adapted from the following website\n",
        "# http://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n",
        "#\n",
        "from nltk.corpus import wordnet\n",
        "# get the WordNet POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma_WI1KCSc2u"
      },
      "source": [
        "`nltk.download('wordnet')` is a module that provides a number of stemmers. In this tutorial, we are going to use the `WordNetLemmatizer` to lemmatize the list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzrOoEIYSc2u",
        "outputId": "38cd739c-8824-4491-99c9-2bd6c94ba809"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HyGFkdwTSc2u"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "final_tokens =[]\n",
        "### write your code below\n",
        "for tagged_set in tagged_sents:\n",
        "    final_tokens = final_tokens + [lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1])) for w in tagged_set ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtVTDYG6Sc2u"
      },
      "source": [
        "You can compare the difference between the tokenization with and without lemmatization. For example, if the list of tokens generated in Exercise 2 is \"stopped_tokens\", then you can use the following code to see the difference\n",
        "```python\n",
        "set(final_tokens) - set(stopped_tokens)\n",
        "```\n",
        "```python\n",
        "set(stopped_tokens) - set(final_tokens)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf6ny1FWSc2u"
      },
      "source": [
        "To use the stemmers discussed in the lecture, you can simply replace the following code in the above code cell\n",
        "```python\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "```\n",
        "with\n",
        "```python\n",
        "    stemmer = PorterStemmer()\n",
        "```\n",
        "and replance\n",
        "```python\n",
        "    lemmatizer.lemmatize(w[0], get_wordnet_pos(w[1]))\n",
        "```\n",
        "with\n",
        "```python\n",
        "    stemmer.stem(w[0])\n",
        "```\n",
        "Don't forget import the corresponding modules.\n",
        "\n",
        "Note that we have not yet done the preprocessing. Next tutorial, we will learn how to count the vocabulary by further removing the most and less frequent words, to generate numberical represenation of a document, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwM_POLgSc2v",
        "outputId": "590fd9aa-b0c1-4ec4-d9f4-cb7204e0b9aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'finding', 'comparison', 'make']\n",
            "['prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'note', 'fluid']\n",
            "['overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'feature', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper']\n",
            "['lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'low', 'lobe', 'measure']\n",
            "['5.4', 'non-specific', 'nature', 'give', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right']\n",
            "['axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule']\n",
            "['right', 'low', 'lobe', 'keep', 'inflammatory_aetiology']\n"
          ]
        }
      ],
      "source": [
        "# print the final_tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(final_tokens), 10):\n",
        "    print(final_tokens[i:i+10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha1Jws5tSc2v",
        "outputId": "0eeea3cf-539e-4dae-d9ec-6466e7031f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['previous', 'right', 'upper', 'lobe_nodule', 'question', 'resolution', 'change', 'findings', 'comparison', 'made']\n",
            "['prior', 'dated', 'november', '30', '2004', 'significant', 'resolution', 'previously', 'noted', 'fluid']\n",
            "['overload', 'status', 'ectasia', 'thoracic_aorta_measuring', '4.2', 'features', 'generalised_centrilobular_emphysema', 'resolution', 'right', 'upper']\n",
            "['lobe_nodule', 'presence', 'nodule', 'within', 'medial', 'segment', 'right', 'lower', 'lobe', 'measures']\n",
            "['5.4', 'non-specific', 'nature', 'given', 'interval', 'development', 'inflammatory_aetiology', 'likely', '13', 'right']\n",
            "['axillary_node', 'new', 'finding', 'since', 'prior', 'study', 'significant', 'mediastinal', 'hilar_adenopathy', 'nodule']\n",
            "['right', 'lower', 'lobe', 'keeping', 'inflammatory_aetiology']\n"
          ]
        }
      ],
      "source": [
        "# print stopped_tokens, every 10 tokens in a new line\n",
        "for i in range(0, len(stopped_tokens), 10):\n",
        "    print(stopped_tokens[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nyBJWraSc2v"
      },
      "source": [
        "Now, we have pre-processed the text data. The next step is to explore the pre-processed text data and generate features. In this tutorial, we are going to demonstrate how to count the vocabulary and generate the sparse format data. The sparse format data is often used in text analysis, where each row contains document ID, word index and the corresponding word count in the document. This practice will help you understand the basic text analysis technologies and the data structure used in text analysis.\n",
        "\n",
        "In the next Section, we are going to explore the pre-processed text data and generate features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGnwc9tsSc2v"
      },
      "source": [
        "## **2. Exploring Pre-Processed text and Generating Features**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kSHiFv0Sc2v"
      },
      "source": [
        "One of the challenges of text analysis is to convert unstructured and semi-structured text into a structured representation. This must be done prior to carrying out any text analysis tasks. This chapter will show you\n",
        "how to put some of those basic steps discussed in the previous chapter together to generate different vector\n",
        "representations for some given text. You will learn how to compute some basic statistics for text, and how to extract features rather than unigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUr3av-BSc2w"
      },
      "source": [
        "### 2.1. Counting Vocabulary by Selecting Tokens of Interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4D3ExgVSc2w"
      },
      "source": [
        "Two important concepts that should be mentioned first are **type** and **token**.\n",
        "Here are the definitions of the two terms, quoted from \"[tokenization](http://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html)\",  \n",
        ">a **token** is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing;\n",
        "\n",
        "> a **type** is the class of all tokens containing the same character sequence.\n",
        "\n",
        "A *type* is also a vocabulary entry. In other words, a vocabulary consists of a number of word types.\n",
        "The distinction between a type and its tokens is a distinction that separates a descriptive concept from\n",
        "its particular concrete instances.\n",
        "This is quite similar to the distinction in object-oriented programming between classes and objects.\n",
        "In this section, you are going to learn how to count types in a given corpus by further processing the text.\n",
        "\n",
        "The document collection that we are going to use is a set of Reuters articles that comes with NLTK.\n",
        "It contains 10788 Reuters articles in total and has been split into two subsets, training and testing.\n",
        "Although this collection has already been pre-processed (e.g., you can access the text at different levels, like raw text, tokens, and sentences),\n",
        "we would still like to demonstrate how to put some of the basis text preprocessing steps together and process the raw Reuters articles step by step.\n",
        "First, import the main Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ew3_oLBMSc2w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o748JQUjSc2w"
      },
      "source": [
        "`nltk.download('reuters')` is a module that provides a number of corpora, including the Reuters corpus. The following code shows how to use the `reuters` corpus to load the Reuters articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSKD4PgGSc2w",
        "outputId": "158029ff-3d43-4ac1-83a7-5674046a483b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV2gMYU8Sc2x"
      },
      "source": [
        "Since the tokenizer works on a per document level, we can parallelize the process of tokenization with Python's multi-processing module. Please refer to its official documentation [here](https://docs.python.org/2/library/multiprocessing.html).\n",
        "In the following code, we wrap tokenization in a Python function, and then\n",
        "create a pool of four worker processes with the Python Pool class.\n",
        "The <font color=\"blue\">`Pool.map()`</font>, a parallel equivalent of the  built-in  <font color=\"blue\">`map()`</font> function, takes one iterable argument.\n",
        "The iterable will be split into a number of chunks, each of which will be submitted to a process in the process pool.\n",
        "Each process will apply a callable function to each element in the chunk it has received.\n",
        "Note that you can replace the NLTK tokenizer with the one you implement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "n7EG2rSzSc2x"
      },
      "outputs": [],
      "source": [
        "def tokenizeRawData(fileid):\n",
        "    \"\"\"\n",
        "        This function tokenizes a raw text document.\n",
        "    \"\"\"\n",
        "    raw_article = reuters.raw(fileid).lower() # cover all words to lowercase\n",
        "    tokenised_article = nltk.tokenize.word_tokenize(raw_article) # tokenize each Reuters articles\n",
        "    return (fileid, tokenised_article)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iDosp1WCSc2x"
      },
      "outputs": [],
      "source": [
        "# import multiprocessing as mp\n",
        "# pool = mp.Pool(processes=4) # Build a pool of 4 processess\n",
        "# tokenized_reuters = dict(pool.map(tokenizeRawData, reuters.fileids()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOhCZraNSc2x"
      },
      "source": [
        "If the the above parallelized code does not work on your computer, you can try the following code running with a signle thread."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "IoFxjl9JSc2x"
      },
      "outputs": [],
      "source": [
        "tokenized_reuters =  dict(tokenizeRawData(fileid) for fileid in reuters.fileids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKKVlTuKSc2x"
      },
      "source": [
        "#### 2.1.1 Removing Words with Non-alphabetic Characters\n",
        "\n",
        "The NLTK's built-in  <font color=\"blue\">word_tokenize</font> function tokenizes a string to split off punctuation other than periods.\n",
        "Not only does it return words with alphanumerical characters, but also punctuations.\n",
        "Let's take a look at one Reuters articles,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Kmx5EDSc2x",
        "outputId": "4ca7a3aa-66d5-4257-ac6b-ad7dcfeaa274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['partnership', 'cuts', 'stake', 'in', 'erc', 'international', '&', 'lt', ';', 'erc']\n",
            "['>', 'parsow', 'partnership', 'ltd', ',', 'a', 'nevada', 'investment', 'partnership', ',']\n",
            "['said', 'it', 'lowered', 'its', 'stake', 'in', 'erc', 'international', 'inc', 'to']\n",
            "['343,500', 'shares', 'or', '8.3', 'pct', 'of', 'the', 'total', 'outstanding', 'common']\n",
            "['stock', ',', 'from', '386,300', 'shares', ',', 'or', '9.3', 'pct', '.']\n",
            "['in', 'a', 'filing', 'with', 'the', 'securities', 'and', 'exchange', 'commission', ',']\n",
            "['parsow', 'said', 'it', 'sold', '42,800', 'erc', 'common', 'shares', 'between', 'jan']\n",
            "['9', 'and', 'march', '2', 'at', 'prices', 'ranging', 'from', '12.125', 'to']\n",
            "['14.50', 'dlrs', 'each', '.', 'the', 'partnership', 'said', 'its', 'dealings', 'in']\n",
            "['erc', 'stock', 'are', 'for', 'investment', 'purposes', 'and', 'it', 'has', 'no']\n",
            "['intention', 'of', 'seeking', 'control', 'of', 'the', 'company', '.']\n"
          ]
        }
      ],
      "source": [
        "# print the first 10 tokens of the first 10 articles\n",
        "for i in range(0, len(tokenized_reuters['training/1684']), 10):\n",
        "    print(tokenized_reuters['training/1684'][i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dm8Q1K3Sc2y"
      },
      "source": [
        "Let's Assume that we are interested in words containing alphabetic characters only\n",
        "and would like to remove all the other tokens\n",
        "that contain digits, punctuation and the other symbols.\n",
        "Removing all the non-alphabetic words from the vocabulary is\n",
        "usually required in some text analysis tasks, such as Topic Modelling that\n",
        "learns the semantic meaning of documents.\n",
        "It can be easily done with the  <font color=\"blue\">`isalpha()`</font> function.\n",
        "The <font color=\"blue\">`isalpha()`</font>\n",
        "checks whether the string consists of alphabetic characters only or not.\n",
        "This method returns true if all characters in the string are in the alphabet and there\n",
        "is at least one character, false otherwise.\n",
        "If you would like to keep all words with alphanumeric characters, you can use\n",
        " <font color=\"blue\">`isalnum()`</font>. Refer to Python's [built-in types](https://docs.python.org/2/library/stdtypes.html) for more detail.\n",
        "Indeed, you can construct your tokenizer in a way such that the tokenizer only extracts words with either\n",
        "alphabetic or alphanumerical characters, as we discussed in the previous chapter.\n",
        "We will leave this as a simple exercise for you to do on your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joprA1AzSc2y",
        "outputId": "37eb9db4-a504-4834-8c13-493f33146226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['partnership', 'cuts', 'stake', 'in', 'erc', 'international', 'lt', 'erc', 'parsow', 'partnership']\n",
            "['ltd', 'a', 'nevada', 'investment', 'partnership', 'said', 'it', 'lowered', 'its', 'stake']\n",
            "['in', 'erc', 'international', 'inc', 'to', 'shares', 'or', 'pct', 'of', 'the']\n",
            "['total', 'outstanding', 'common', 'stock', 'from', 'shares', 'or', 'pct', 'in', 'a']\n",
            "['filing', 'with', 'the', 'securities', 'and', 'exchange', 'commission', 'parsow', 'said', 'it']\n",
            "['sold', 'erc', 'common', 'shares', 'between', 'jan', 'and', 'march', 'at', 'prices']\n",
            "['ranging', 'from', 'to', 'dlrs', 'each', 'the', 'partnership', 'said', 'its', 'dealings']\n",
            "['in', 'erc', 'stock', 'are', 'for', 'investment', 'purposes', 'and', 'it', 'has']\n",
            "['no', 'intention', 'of', 'seeking', 'control', 'of', 'the', 'company']\n"
          ]
        }
      ],
      "source": [
        "# remove non-alphabetic tokens\n",
        "for k, v in tokenized_reuters.items():\n",
        "    tokenized_reuters[k] = [word for word in v if word.isalpha()]\n",
        "# print the tokens, every 10 tokens in a new line\n",
        "# tokenized_reuters['training/1684']\n",
        "for i in range(0, len(tokenized_reuters['training/1684']), 10):\n",
        "    print(tokenized_reuters['training/1684'][i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzbWdNY7Sc2y"
      },
      "source": [
        "Now you should have derived much cleaner text for each Reuters article.\n",
        "Let's check how many types we have in the whole corpus and the lexical diversity (i.e., the average number\n",
        "of times a type apprearing in the collection.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXSAGlXkSc2y",
        "outputId": "1a6c0b0b-8fe5-460e-85b4-88620824717d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size:  27956 \n",
            "Total number of tokens:  1275855 \n",
            "Lexical diversity:  45.63796680497925\n"
          ]
        }
      ],
      "source": [
        "from __future__ import division\n",
        "from itertools import chain\n",
        "# get the list of all words\n",
        "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
        "vocab = set(words)\n",
        "lexical_diversity = len(words)/len(vocab)\n",
        "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
        "\"\\nLexical diversity: \", lexical_diversity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3whQDXW-Sc2y"
      },
      "source": [
        "There are about 1.27 million word tokens in the tokenized Reuters corpus.\n",
        "The vocabulary size is 27,944, which is still quite large according to our knowledge of this corpus.\n",
        "The lexical diversity tells us that words occur on average about 46 times each.\n",
        "You might think that\n",
        "there could still be words that occur very frequently, such as stopwords,\n",
        "and those that only occur once or twice.\n",
        "For example, if an article \"the\" appears in almost\n",
        "every document in a corpus,\n",
        "it might not help you at all and would only contribute noise.\n",
        "Similarly if a word appears only once in a corpus or only in one document of the corpus,\n",
        "it could carry little useful information for downstream analysis.\n",
        "Therefore, we would better remove those words from the vocabulary, which\n",
        "will benefit the text analysis algorithms in terms of reducing running time and\n",
        "memory requirement, and improving their performance.\n",
        "To do so, we need to further explore the corpus by computing some simple\n",
        "statistics.\n",
        "\n",
        "Note that we introduced two new Python libraries in the code above.\n",
        "They are\n",
        "[`__future__`](https://docs.python.org/2/library/__future__.html)\n",
        "and [`itertools`](https://docs.python.org/2/library/itertools.html).\n",
        "The first statement in the code makes sure that Python switches to\n",
        "always yielding a real result.\n",
        "Thus if you divide two integer values, you will not get for example.\n",
        "````\n",
        "    1/2 = 0\n",
        "    3/2 = 1\n",
        "````\n",
        "Instead, you will have\n",
        "```\n",
        "    1/2 = 0.5\n",
        "    3/2 = 1.5\n",
        "```\n",
        "The second statement imported a  <font color=\"blue\">chain()</font> iterator from the  <font color=\"blue\">itertools</font> module.\n",
        "We use the iterator to join all the words in all the Reuters articles together.\n",
        "It works as\n",
        "```python\n",
        "   for wordList in tokenized_reuters.values():\n",
        "       for word in wordList:\n",
        "           yield word\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f-B8ecPSc2y"
      },
      "source": [
        "#### 2.1.2. Removing the Most and Less Frequent Words\n",
        "It is quite useful for us to identify the words that are most informative about the sematic\n",
        "meaning of the text regardless of syntax.\n",
        "One common statistics often used in text processing is frequency distribution.\n",
        "It can tell us how frequent a word is in a given corpus in terms of either term frequency or document frequency.\n",
        "Term frequency counts the number of times a word occurs in the whole corpus regardless which document it is in.\n",
        "Frequency distribution based on term frequency tells us how the total number of word tokens are distributed across all the types.\n",
        "NLTK provides a built-in function `FreqDist` to compute this distribution directly from a set of word tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L53T6QJnSc2z"
      },
      "source": [
        "`nltk.probability ` is a module that provides a number of probability distributions. In this tutorial, we are going to use the `FreqDist` to compute the frequency distribution of the word tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "40_sbilkSc2z"
      },
      "outputs": [],
      "source": [
        "from nltk.probability import *\n",
        "fd_1 = FreqDist(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y4-pzPtSc2z"
      },
      "source": [
        "What are the most frequent words in the corpus?\n",
        "we can use the  <font color=\"blue\">most_common</font> function to print out the most frequent words together with their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXympzMDSc20",
        "outputId": "03a3389e-2610-4e53-8774-4d57bd99ec09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('the', 69245) ('of', 36749) ('to', 36275) ('in', 29217) ('and', 25616)\n",
            "('said', 25381) ('a', 24723) ('mln', 18598) ('vs', 14332) ('for', 13420)\n",
            "('dlrs', 12329) ('it', 11100) ('pct', 9771) ('on', 9094) ('lt', 8696)\n",
            "('cts', 8308) ('from', 8217) ('is', 7674) ('that', 7540) ('its', 7402)\n",
            "('by', 7082) ('at', 7014) ('net', 6986) ('year', 6715) ('be', 6354)\n"
          ]
        }
      ],
      "source": [
        "# print the 25 most common tokens, 5 each line\n",
        "for i, word in enumerate(fd_1.most_common(25)):\n",
        "    print(word, end = '\\n' if (i+1) % 5 == 0 else ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VpzxBf-Sc20",
        "outputId": "13cfa9cc-8e14-43e7-c33c-0d59fd5b3b17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "69245"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd_1.get('the')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pon-7s5jSc20"
      },
      "source": [
        "The list above contains the 25 most frequent words.\n",
        "You can see that it is mostly dominated by the little words of the English language which have important grammatical roles.\n",
        "Those words are articles, prepositions, pronouns, auxiliary webs, conjunctions, etc.\n",
        "They are usually referred to as function words in linguistics, which tell us nothing about\n",
        "the meaning of the text.\n",
        "What proportion of the text is taken up with such words?\n",
        "We can generate a cumulative frequency plot for them\n",
        "using  <font color=\"blue\">fd.plot(25, cumulative=True)</font>.\n",
        "If you set  <font color=\"blue\">cumulative</font> to  <font color=\"blue\">False</font>,\n",
        "it will plot the frequencies of these 25 words.\n",
        "These 25 words account for about 33% of the while Reuters corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "a-QfNr48Sc21",
        "outputId": "35f42768-918a-4ee4-9547-1b8d5b444495"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='Samples', ylabel='Cumulative Counts'>"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the frequency distribution\n",
        "fd_1.plot(25, cumulative=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNTGMbgtSc21"
      },
      "source": [
        "What are the most frequent words in terms of document frequency?\n",
        "Here we are going to count how many documents a word appears in, which is referred to as document frequency.\n",
        "Instead of writing nested FOR loops to count the document frequency for each word,\n",
        "we can use  <font color=\"blue\">`FreqDist()`</font> jointly with  <font color=\"blue\">`set()`</font> as follows:\n",
        "1. Apply  <font color=\"blue\">`set()`</font> to each Reuters article to generate a set of unique words in the article and save all sets in a list\n",
        "```python\n",
        "    [set(value) for value in tokenized_reuters.values()]\n",
        "```\n",
        "2. Similar to what we have done before, we put all the words in a list using  <font color=\"blue\">`chain.from_iterable`</font> and past\n",
        "it to  <font color=\"blue\">`FreqDist`</font>.\n",
        "\n",
        "The first step makes sure that each word in an article appears only once, thus the total number of\n",
        "times a word appears in all the sets is equal to the number of documents containing that word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOOmtjBeSc21",
        "outputId": "b5e4127a-f8e4-47ff-e2a1-5ba117e45239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('of', 7621) ('the', 6950) ('to', 6911) ('said', 6784) ('and', 6761)\n",
            "('in', 6580) ('a', 6222) ('lt', 6069) ('for', 5415) ('mln', 4845)\n",
            "('it', 4788) ('dlrs', 4193) ('from', 4006) ('on', 3987) ('its', 3761)\n",
            "('is', 3569) ('by', 3511) ('year', 3423) ('at', 3392) ('with', 3253)\n",
            "('pct', 3212) ('cts', 3068) ('inc', 3023) ('vs', 2982) ('be', 2927)\n"
          ]
        }
      ],
      "source": [
        "# get the list of all words\n",
        "words_2 = list(chain.from_iterable([set(value) for value in tokenized_reuters.values()]))\n",
        "fd_2 = FreqDist(words_2)\n",
        "# print the 25 most common tokens, 5 each line\n",
        "for i, word in enumerate(fd_2.most_common(25)):\n",
        "    print(word, end = '\\n' if (i+1) % 5 == 0 else ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-9A-tecSc21"
      },
      "source": [
        "What you will find is that the majority of the most frequent words according to their document frequecy are still functional words.\n",
        "Therefore, the next step is to remove all the stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol3lvBv5Sc21"
      },
      "source": [
        "##### a. Ignoring Stopwords\n",
        "As discussed in section 1, we often remove function words from the text completely for most text analysis tasks.\n",
        "Instead of using the built-in stopword list of `NLTK`, we use a much rich stopword list that has been downloaded before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEyObRIEZWfu",
        "outputId": "521f7148-aaec-42b1-da39-80a61de4ef9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw2XM8_oSc21"
      },
      "outputs": [],
      "source": [
        "stopwords = []\n",
        "with open('/content/drive/Shareddrives/FIT5196_S2_2025/week5/stopwords_en.txt') as f:\n",
        "    stopwords = f.read().splitlines()\n",
        "tokenized_reuters_1 = {}\n",
        "for fileid in reuters.fileids():\n",
        "    tokenized_reuters_1[fileid] = [w for w in tokenized_reuters[fileid] if w not in stopwords]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97gs3aVcSc21"
      },
      "source": [
        "The list comprehension\n",
        "```python\n",
        "    [w for w in tokenized_reuters[fileid] if w not in stopwords]\n",
        "```\n",
        "says: For each word in each Reuters article, keep the word if the word is not contained in the stopword list.\n",
        "\n",
        "Checking for membership of a value in a list takes time proportional to the list's length in the average and worst cases.\n",
        "It causes the above code to run quite slow as we need to do the check for every word in each Reuters article\n",
        "and the size of the stopword list is large.\n",
        "However, if you have hashable items, which means both the item order and duplicates are disregarded,\n",
        "Python `set` is better choice than `list`. The former runs much faster than the latter in terms of searching\n",
        "a large number of hashable items. Indeed, `set` takes constant time to check the membership.\n",
        "Let's try converting the stopword list into a stopword set, then search to remove all the stopwords.\n",
        "Please also note that if you try to perform iteration, `list` is much better than `set`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "2Ub2CqUmSc22"
      },
      "outputs": [],
      "source": [
        "uni_stopwords = set(stopwords)\n",
        "for fileid in reuters.fileids():\n",
        "    tokenized_reuters[fileid] = [w for w in tokenized_reuters[fileid] if w not in uni_stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrVzff4-Sc22"
      },
      "outputs": [],
      "source": [
        "# check how many stopwords are removed from tokenized_reuters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "367sQ3XxSc22",
        "outputId": "4921d07e-4de7-46f3-f6c2-da6b13c76f58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of stopwords removed is 481\n"
          ]
        }
      ],
      "source": [
        "words_3 = list(chain.from_iterable(tokenized_reuters.values()))\n",
        "fd_3 = FreqDist(words_3)\n",
        "print(f\"The number of stopwords removed is {len(list(vocab - set(fd_3.keys())))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdSN3moFSc22"
      },
      "source": [
        "In the above stopping process, 481 stopwords have been removed from the vocabulary. You might wonder what those removed words are. It is quite easy to check those words by differentiating the vocabulary before and after stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUeNU_gqSc22"
      },
      "outputs": [],
      "source": [
        "# print the removed stopwords, every 10 tokens in a new line\n",
        "for i in range(0, len(list(vocab - set(fd_3.keys()))), 10):\n",
        "    print(list(vocab - set(fd_3.keys()))[i:i+10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6STzaxdSc22"
      },
      "source": [
        "Beside stopwords, there might some other words that occur quite often as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkTEPTgkSc22"
      },
      "outputs": [],
      "source": [
        "fd_3.most_common(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wjVRczndSc22"
      },
      "outputs": [],
      "source": [
        "# get the list of all words\n",
        "words_4 = list(chain.from_iterable([set(tokenlist) for tokenlist in tokenized_reuters.values()]))\n",
        "fd_4 = FreqDist(words_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pXIrKuhSc22",
        "outputId": "b829d3b5-3c0e-40e2-db61-9b63d4f5c692"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1687"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd_4.get('billion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH8vDuvRSc23",
        "outputId": "95052d79-8151-411b-ea42-3b6387874564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['lt', 'mln', 'dlrs', 'year']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[v for v,k in fd_4.most_common() if k > len(tokenized_reuters.keys())*0.30]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYn34Bp9Sc23"
      },
      "source": [
        "Before we decide to remove those words from our vocabulary, it might be worth checking what\n",
        "those words mean and the context of those words. Fortunately `NLTK` provides a `concordance`\n",
        "function in the `nltk.text` module. A concordance view shows us every occurrence of a given\n",
        "word, together with the corresponding context. For example,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUxAtCdoSc23",
        "outputId": "750520c3-41e5-48c5-a1f0-70618284364f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying 25 of 18623 matches:\n",
            "e U . S . Has said it will impose 300 mln dlrs of tariffs on imports of Japanes\n",
            ". It also said that each year 1 . 575 mln tonnes , or 25 pct , of China ' s fru\n",
            "it output are left to rot , and 2 . 1 mln tonnes , or up to 30 pct , of its veg\n",
            "ergy supplies in the year 2000 to 550 mln kilolitres ( kl ) from 600 mln , they\n",
            "to 550 mln kilolitres ( kl ) from 600 mln , they said . The decision follows th\n",
            "totalled 9 , 595 tonnes , worth 6 . 9 mln dlrs FOB , plus 184 . 3 mln rupiah fo\n",
            "rth 6 . 9 mln dlrs FOB , plus 184 . 3 mln rupiah for rubber delivered locally ,\n",
            "thern Territory at a cost of about 21 mln dlrs . The mine , to be known as the \n",
            "umitomo last August agreed to pay 500 mln dlrs for a 12 . 5 pct limited partner\n",
            " . It reported a net loss of 976 . 38 mln pesos in the year ending December 198\n",
            "ld acquire Atlas ' total loans of 275 mln dlrs , to be repaid by the mining com\n",
            "increase in authorised capital to 175 mln shares from 125 mln at a general meet\n",
            "ed capital to 175 mln shares from 125 mln at a general meeting on May 1 , it sa\n",
            "WATER 1986 PRETAX PROFITS RISE 15 . 6 MLN STG Shr 27 . 7p vs 20 . 7p Div 6 . 0p\n",
            "s 1 . 29 billion Pretax profit 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9\n",
            "on Pretax profit 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company n\n",
            "t 48 . 0 mln vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company name is Bowater \n",
            "vs 32 . 4 mln Tax 14 . 4 mln vs 6 . 9 mln Company name is Bowater Industries Pl\n",
            "lt ; BWTR . L > Trading profit 63 . 4 mln vs 45 . 1 mln Trading profit includes\n",
            "> Trading profit 63 . 4 mln vs 45 . 1 mln Trading profit includes - Packaging a\n",
            "kaging and associated products 23 . 2 mln vs 14 . 2 mln Merchanting and service\n",
            "ociated products 23 . 2 mln vs 14 . 2 mln Merchanting and services 18 . 4 mln v\n",
            "2 mln Merchanting and services 18 . 4 mln vs 9 . 6 mln Tissue and timber produc\n",
            "ting and services 18 . 4 mln vs 9 . 6 mln Tissue and timber products 9 . 0 mln \n",
            " mln Tissue and timber products 9 . 0 mln vs 5 . 8 mln Interest debit 15 . 4 ml\n"
          ]
        }
      ],
      "source": [
        "nltk.Text(reuters.words()).concordance('mln')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQTnWKWJSc23",
        "outputId": "86f3d0d6-d70f-451c-df34-d7b24a9d6bc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying 25 of 6989 matches:\n",
            "ndesbank is effectively withdrawing a net 8 . 1 billion marks from the market w\n",
            "d world copper prices . It reported a net loss of 976 . 38 mln pesos in the yea\n",
            "nding December 1986 , compared with a net loss of 1 . 53 billion in 1985 . The \n",
            " U . S .- based bank , said it made a net loss of just over six mln crowns in 1\n",
            "ng program , reported a 198 mln franc net loss , after 187 mln francs in provis\n",
            "plant , compared with a 250 mln franc net profit in 1985 . VIEILLE MONTAGNE REP\n",
            "REPORTS LOSS , DIVIDEND NIL 1986 Year Net loss after exceptional charges 198 ml\n",
            "on francs vs 20 . 20 billion Proposed net dividend on ordinary shares nil vs 11\n",
            "ently reported first - half 1986 / 87 net fell to 15 . 02 mln dlrs from 17 . 09\n",
            "eso sales in 1985 . It said unaudited net profit was in the neighbourhood of 70\n",
            "id in a statement that parent company net profit last year will rise from the 7\n",
            " DSM & lt ; DSMN . AS > said its 1986 net profit rose to 412 mln guilders from \n",
            "ARCH INC & lt ; CORE > 2ND QTR FEB 28 NET Shr 14 cts vs nine cts Net 217 , 572 \n",
            "QTR FEB 28 NET Shr 14 cts vs nine cts Net 217 , 572 vs 153 , 454 Revs 2 , 530 ,\n",
            "8 , 924 1st half Shr 19 cts vs 11 cts Net 299 , 838 vs 174 , 739 Revs 4 , 865 ,\n",
            "ENT CORP & lt ; ELRC > 3RD QTR FEB 28 NET Shr 20 cts vs 32 cts Net 1 , 358 , 00\n",
            "D QTR FEB 28 NET Shr 20 cts vs 32 cts Net 1 , 358 , 000 vs 2 , 476 , 000 Revs 2\n",
            "0 Nine mths Shr 68 cts vs 1 . 05 dlrs Net 4 , 957 , 000 vs 8 , 129 , 000 Revs 8\n",
            "0 RUBBERMAID INC & lt ; RBD > 1ST QTR NET Shr 28 cts vs 22 cts Net 20 . 6 mln v\n",
            "BD > 1ST QTR NET Shr 28 cts vs 22 cts Net 20 . 6 mln vs 16 . 1 mln Sales 238 . \n",
            "IONAL INC & lt ; WAF > 4TH QTR FEB 28 NET Shr profit 13 cts vs loss 33 cts Net \n",
            " NET Shr profit 13 cts vs loss 33 cts Net profit 1 , 149 , 000 vs loss 2 , 833 \n",
            "Year Shr profit 24 cts vs loss 18 cts Net profit 2 , 050 , 000 vs loss 1 , 551 \n",
            "GA SAVINGS BANK & lt ; CAYB > 1ST QTR NET Shr 55 cts vs 41 cts Net 494 , 000 vs\n",
            "YB > 1ST QTR NET Shr 55 cts vs 41 cts Net 494 , 000 vs 204 , 000 Avg shrs 896 ,\n"
          ]
        }
      ],
      "source": [
        "nltk.Text(reuters.words()).concordance('net')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ULngbpSc23"
      },
      "source": [
        "##### b. Remove Less Frequent Words\n",
        "If the most common words do not benefit the downstream text analysis tasks, except for contributing noises,\n",
        "how about the words that occur once or twice?\n",
        "Here another interesting statistic to look at is the frequency of the frequencies of word types in a given corpus.\n",
        "We would like to see how many words appear only once, how many words appear twice, how many\n",
        "words appear three times, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "9qnq7Wc5Sc23",
        "outputId": "63e92730-1548-448e-8048-e3a106be576d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ffd = FreqDist(fd_3.values())\n",
        "from pylab import *\n",
        "y = [0]*14\n",
        "for k, v in ffd.items():\n",
        "     if k <= 10:\n",
        "        y[k-1] = v\n",
        "     elif k >10 and k <= 50:\n",
        "        y[10] =  y[10] + v\n",
        "     elif k >50 and k <= 100:\n",
        "        y[11] =  y[11] + v\n",
        "     elif k > 100 and k <= 500:\n",
        "        y[12] =  y[12] + v\n",
        "     else:\n",
        "        y[13] =  y[13] + v\n",
        "x = range(1, 15) # generate integer from 1 to 14\n",
        "ytks =list(map(str, range(1, 11))) # covert a integer list to a string list\n",
        "ytks.append('10-50')\n",
        "ytks.append('51-100')\n",
        "ytks.append('101-500')\n",
        "ytks.append('>500')\n",
        "barh(x,y, align='center')\n",
        "yticks(x, ytks)\n",
        "xlabel('Frequency of Frequency')\n",
        "ylabel('Word Frequency')\n",
        "grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjWa3CLKSc23"
      },
      "source": [
        "The horizontal bar chart generated above shows how many word types occur with a certain frequency.\n",
        "There are 241 types occurring over 500 times and therefore individually accounting for about 1% of\n",
        "the vocabulary.\n",
        "However, on the other extreme, more than one-third of the word types occur only once in the Reuters corpus.\n",
        "Note that the majority of word types occur quite infrequently given the size of the whole corpus (i.e., 721,371 word tokens):\n",
        "about 78% of the word types occur 10 times or less.\n",
        "Similarly, you can also look at the bar chart based on the document frequency. Try it by yourself!\n",
        "\n",
        "Let's further remove those words that occur only once.\n",
        "To get those words, you can write the code like\n",
        "```python\n",
        "    lessFreqWords = set([k for k, v in fdist.items() if v < 2])\n",
        "```\n",
        "or choose to use `hapaxes()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "pcgM3m3-Sc23"
      },
      "outputs": [],
      "source": [
        "lessFreqWords = set(fd_3.hapaxes())\n",
        "def removeLessFreqWords(fileid):\n",
        "    return (fileid, [w for w in tokenized_reuters[fileid] if w not in lessFreqWords])\n",
        "#pool = mp.Pool(4)\n",
        "#tokenized_reuters = dict(pool.map(removeLessFreqWords, reuters.fileids()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "iD7h9hh-Sc24"
      },
      "outputs": [],
      "source": [
        "tokenized_reuters = dict(removeLessFreqWords(fileid) for fileid in reuters.fileids())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4QBCKolSc24"
      },
      "source": [
        "Now, you should have a pretty clean set of Reuters articles, each of which is stored as a list of word tokens.\n",
        "Let's further print out some statistics that summarize this corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syRvWI3rSc24",
        "outputId": "ea8f971c-b010-49e0-c085-e2cac88105e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size:  17408\n",
            "Total number of tokens:  712329\n",
            "Lexical diversity:  45.63796680497925\n",
            "Total number of articles: 10788\n",
            "Average document length: 66.0297552836485\n",
            "Maximun document length: 705\n",
            "Minimun document length: 0\n",
            "Standard deviation of document length: 65.84580999696115\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "words = list(chain.from_iterable(tokenized_reuters.values()))\n",
        "vocab = set(words)\n",
        "print (\"Vocabulary size: \",len(vocab))\n",
        "print (\"Total number of tokens: \", len(words))\n",
        "print (\"Lexical diversity: \", lexical_diversity)\n",
        "print (\"Total number of articles:\", len(tokenized_reuters))\n",
        "lens = [len(value) for value in tokenized_reuters.values()]\n",
        "print (\"Average document length:\", np.mean(lens))\n",
        "print (\"Maximun document length:\", np.max(lens))\n",
        "print (\"Minimun document length:\", np.min(lens))\n",
        "print (\"Standard deviation of document length:\", np.std(lens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7OENAdnSc24"
      },
      "source": [
        "It is interesting that the minimun document length is 0. There must be some Reuters articles that are extremely short,\n",
        "after tokenization and stopping, there are no words left. Can you check those documents to see what they look like?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljzYr5hLSc24"
      },
      "source": [
        "### 2.2. Building Vector Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2tIFBg-Sc25"
      },
      "source": [
        "After text pre-processing has been completed, each individual document needs to be transformed into\n",
        "some kind of numeric representation that can be input into most NLP and text mining algorithms.\n",
        "For example, classification algorithms, such as Support Vector Machine, can only take data in a\n",
        "structured and numerical form. They do not accept free languge text.\n",
        "The most popular structured representation of text is the vector-space model, which represents text\n",
        "as a vector where the elements of the vector indicate the occurence of words within the text.\n",
        "The vector-space model makes an implicit assumption that\n",
        "the order of words in a text document are not as\n",
        "important as words themselves, and thus disregarded.\n",
        "This assumpiton is called [**Bag-of-words**](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
        "\n",
        "Given a set of documents and a pre-defined list of words appearing\n",
        "in those documents (i.e., a vocabulary), you can compute a vector representation for each document.\n",
        "This vector representation can take one of the following three forms:\n",
        "* a binary representation,\n",
        "* an integer count,\n",
        "* and a float-valued weighted vector.\n",
        "\n",
        "To highlight the difference among the three approaches, we use a very simple example as follows:\n",
        "```\n",
        "    document_1: \"Data analysis is important.\"\n",
        "    document_2: \"Data wrangling is as important as data analysis.\"\n",
        "    document_3: \"Data science contains data analysis and data wrangling.\"\n",
        "```\n",
        "The three documents contain 20 tokens and 9 unique words.\n",
        "Those unique words are sorted alphabetically with total counts:\n",
        "```\n",
        "     'analysis': 3,\n",
        "     'and': 1,\n",
        "     'as': 2,\n",
        "     'contains': 1,\n",
        "     'data': 6,\n",
        "     'important': 2,\n",
        "     'is': 2,\n",
        "     'science': 1,\n",
        "     'wrangling': 2\n",
        "```\n",
        "Given the vocabulary above,\n",
        "both the binary and the integer count vectors are easy to compute.\n",
        "A binary vector stores 1s for the word that appears in a document and 0s for the other words in\n",
        "the vocabulary,\n",
        "whereas a count vector stores the frequency of each word appearing in the document.\n",
        "Thus, the binary vector representations for the three documents above are\n",
        "   \n",
        "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
        "   |-|-|-|-|-|-|-|-|-|\n",
        "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
        "   |document 2:|1|0|1|0|1|1|1|0|1|\n",
        "   |document 3:|1|1|0|1|1|0|0|1|1|\n",
        "\n",
        "The count vector representations for the same documents would look as follows:\n",
        "\n",
        "   ||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
        "   |-|-|-|-|-|-|-|-|-|\n",
        "   |document 1:|1|0|0|0|1|1|1|0|0|\n",
        "   |document 2:|1|0|2|0|2|1|1|0|1|\n",
        "   |document 3:|1|1|0|1|3|0|0|1|1|\n",
        "\n",
        "Instead of using the two vector representations above,\n",
        "most existing text analysis algorithms, like document classification and information retrieval,\n",
        "prefer representing documents as weighted vectors.\n",
        "The raw term frequency is often replaced with a weighted term frequency\n",
        "that indicates how important a word is in a particular document.\n",
        "There are many different term weighting schemes online.\n",
        "To store each document as a weighted vector, we first need to choose a weighting scheme.\n",
        "The most popular scheme is the TF-IDF weighting approach.\n",
        "TF-IDF stands for term frequency-inverse document frequency.\n",
        "The term frequency for a word is the number of times the word appears in a document.\n",
        "In the preceding example, the term frequency in Document 2 for data is 2, since it appears twice in the document. Document frequency for a word is the number of documents that contain the word;\n",
        "it would also be 3 for data in the collection of the three preceding documents.\n",
        "The Wikipidia entry on [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) lists\n",
        "a number of variants of TF-IDF.\n",
        "One variant is reproduced here\n",
        "$$tf\\cdot idf(w,d) = tf(w, d) * idf(w)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$tf(w,d)\\,=\\, \\sum_{i}^{|d|} 1_{w = w_{d,i}}$$\n",
        "and\n",
        "$$idf(w) = log\\left(\\frac{|D|}{|d \\in D: w \\in d |}\\right)$$\n",
        "\n",
        "The assumption behind TF-IDF is that words with high term frequency should receive high weight unless they also have high document frequency.\n",
        "Stopwords are the most commonly occurring words in the English language. They often occur many times within a single document, but they also occur in nearly every document.\n",
        "These two competing effects cancel out to give them low weights,\n",
        "as those very common words carry very little meaningful information about the actual contents of the document.\n",
        "Therefore, the TF-IDF weights for stopwords are almost always 0.\n",
        "With the TF-DF formulas above,\n",
        "the weighted vector representations for the example documents are computed as\n",
        "\n",
        "||'analysis'|'and'|'as'|'contains'|'data'|'important'|'is'|'science'|'wrangling'|\n",
        "   |-|-|-|-|-|-|-|-|-|\n",
        "   |document 1:|0|0|0|0|0|0.176|0.176|0|0|\n",
        "   |document 2:|0|0|0.954|0|0|0.176|0.176|0|0.176|\n",
        "   |document 3:|0|0.477|0|0.477|0|0|0|0.477|0.176|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_vpmkfVSc25"
      },
      "source": [
        "Given the cleaned up Reuters documents, how can we generate those vectors for each documents?\n",
        "Unfortunately, NLTK does not implement methods that directly produce those vectors.\n",
        "Therefore, we will either write our own code to compute them or appeal to other data analysis libraries.\n",
        "Here we are going to use [scikit-learn](http://scikit-learn.org/stable/index.html), an open source machine\n",
        "learning library for Python.\n",
        "If you use Anaconda, you should already have scikit-learn installed, otherwise you will need to\n",
        "[install it](http://scikit-learn.org/stable/install.html) by following the instruction on its official website.\n",
        "\n",
        "Although scikit-learn features various classification, regression and clustering algorithms\n",
        "we are particularly interested in its feature extraction module, [sklearn.feature_extraction](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction).\n",
        "This module is often used to \"extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image.\" Please refer to its documentation on text feature extraction,\n",
        "section 4.2.3 of [Feature Extraction](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction). We will demonstrate the usage of the following two classes:\n",
        "* [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer): It converts a collection of text documents to a matrix of token counts.\n",
        "* [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\n",
        "It converts a collection of raw documents to a matrix of TF-IDF features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PTHwTLqaK6H"
      },
      "source": [
        "#### 2.2.1 Creating Count Vectors\n",
        "Let's start with generating the count vector representation for each Reuters document.\n",
        "Initialise the \"CountVector\" object: since we have pre-processed all the Reuters documents,\n",
        "the parameters, \"tokenizer\", \"preprocessor\" and \"stop_words\" are set to their default value, i.e., None."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "u5rFlrlSSc25"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(analyzer = \"word\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XNOYT3oSc25"
      },
      "source": [
        "Next, transform Reuters articles into feature vectors. `fit_transform` does two things: First, it fits the model and learns the vocabulary; second it transforms the text data into feature vectors.\n",
        "Please note the input to `fit_transform` should be a list of strings.\n",
        "Since we have stored each tokenised article as a list of words, we concatenate all the words in the list and separate\n",
        "them with white spaces.\n",
        "The following code will do that:\n",
        "```python\n",
        "[' '.join(value) for value in tokenized_reuters.values()]\n",
        "```\n",
        "Then, we input this list of strings into `fit_transform`,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeHTR6QRSc25",
        "outputId": "0f4ea3e3-71a2-406f-b8d9-1d9eb6663443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10788, 17408)\n"
          ]
        }
      ],
      "source": [
        "data_features = vectorizer.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\n",
        "print (data_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAUfbQCBSc25"
      },
      "source": [
        "The shape of document-by-word matrix should be 10788 * 17403.\n",
        "However, in order to save such a matrix in memory but also to speed up algebraic operations on the matrix,\n",
        "scikit-learn implements matrix/vector in a sparse representation.\n",
        "Let's check the count vector for the first article, i.e., 'training/1684'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KFB16khSc26",
        "outputId": "37404e48-0328-4d18-835b-3ae0360ab3a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "barry : 1\n",
            "appeal : 1\n",
            "korkman : 1\n",
            "await : 2\n",
            "committees : 1\n",
            "fostering : 1\n",
            "jobless : 1\n",
            "allowance : 1\n",
            "grease : 2\n",
            "taher : 1\n",
            "diversity : 1\n",
            "yaik : 2\n",
            "anita : 1\n",
            "pickup : 1\n",
            "cotton : 1\n",
            "aluminium : 1\n",
            "feeding : 1\n",
            "protected : 1\n",
            "petroleos : 5\n",
            "dls : 1\n",
            "gilts : 2\n",
            "faction : 1\n",
            "republicans : 1\n",
            "worse : 1\n",
            "eep : 4\n",
            "gazeta : 2\n",
            "land : 1\n",
            "signup : 1\n",
            "ideologically : 1\n",
            "upturn : 1\n",
            "pursuit : 1\n",
            "morse : 1\n",
            "mtech : 1\n",
            "empty : 1\n",
            "nge : 1\n",
            "stowed : 1\n",
            "official : 1\n",
            "volcker : 1\n",
            "exporting : 1\n",
            "zimbabwean : 1\n",
            "thermo : 1\n",
            "muskogee : 1\n",
            "westpac : 2\n",
            "cede : 1\n",
            "onward : 2\n",
            "dalton : 1\n",
            "flexibility : 2\n",
            "heimat : 1\n",
            "interrupt : 1\n",
            "voluntary : 1\n",
            "milex : 2\n",
            "numerous : 1\n",
            "canned : 1\n",
            "regency : 1\n",
            "formally : 2\n",
            "recall : 6\n",
            "ibrahim : 1\n",
            "englebright : 1\n",
            "adm : 2\n",
            "cairns : 2\n",
            "sown : 1\n",
            "semi : 1\n",
            "tyres : 4\n",
            "regulated : 1\n",
            "brnf : 1\n",
            "goodrich : 1\n",
            "fewer : 1\n",
            "fragmented : 1\n",
            "usage : 1\n",
            "privileged : 1\n",
            "markey : 3\n",
            "atco : 1\n",
            "britannia : 6\n",
            "med : 1\n",
            "warranted : 1\n",
            "considerations : 1\n",
            "touched : 1\n",
            "global : 1\n",
            "iel : 1\n",
            "krutikhin : 1\n",
            "contrary : 1\n",
            "viner : 1\n",
            "honduras : 1\n",
            "importance : 1\n",
            "sjt : 1\n",
            "deforestation : 1\n",
            "marked : 2\n",
            "fmd : 1\n",
            "assertion : 1\n",
            "occupied : 1\n",
            "leftist : 1\n",
            "buckeye : 1\n",
            "swl : 1\n",
            "gemina : 4\n",
            "equa : 1\n",
            "sroka : 1\n",
            "insert : 1\n",
            "fetched : 5\n",
            "loophole : 1\n",
            "programmes : 1\n",
            "skylights : 1\n",
            "ridden : 3\n",
            "presenting : 1\n",
            "cfc : 1\n",
            "feinberg : 1\n",
            "sociedad : 12\n",
            "milton : 4\n",
            "excessively : 1\n",
            "registered : 1\n",
            "participating : 4\n",
            "banque : 3\n",
            "metz : 1\n",
            "leeway : 1\n",
            "soared : 2\n",
            "ralph : 1\n",
            "including : 2\n",
            "intermark : 1\n",
            "watanabe : 1\n",
            "trace : 1\n",
            "realistically : 1\n",
            "alleghney : 3\n",
            "kaneb : 2\n",
            "bonuses : 1\n",
            "awful : 1\n",
            "magic : 1\n",
            "explained : 1\n",
            "tons : 1\n",
            "similarly : 2\n",
            "terming : 1\n",
            "chile : 1\n",
            "coastal : 1\n",
            "recruitment : 2\n",
            "exovir : 1\n",
            "resettle : 1\n",
            "pequiven : 1\n",
            "industrial : 3\n",
            "bacterial : 1\n",
            "conference : 1\n",
            "bilion : 1\n",
            "relate : 1\n",
            "feasability : 1\n",
            "pulling : 1\n",
            "logistics : 1\n",
            "dravo : 1\n",
            "fortunately : 1\n",
            "atwell : 1\n",
            "precluded : 1\n",
            "outlines : 1\n",
            "ub : 2\n",
            "divi : 2\n",
            "alpha : 1\n",
            "stephens : 1\n",
            "sights : 1\n",
            "heatley : 1\n",
            "canton : 1\n",
            "mla : 1\n",
            "backers : 1\n",
            "spokeswoman : 1\n",
            "losing : 1\n",
            "ml : 2\n",
            "nawg : 1\n",
            "copies : 2\n",
            "conditioning : 1\n",
            "sees : 1\n",
            "quarrel : 1\n",
            "cgic : 1\n",
            "georgia : 1\n",
            "ascs : 3\n",
            "est : 1\n",
            "anodes : 1\n",
            "downtime : 1\n",
            "tomaque : 1\n",
            "groupe : 1\n",
            "debtor : 1\n",
            "azuma : 1\n",
            "failure : 1\n",
            "fine : 1\n",
            "flick : 1\n",
            "aqazadeh : 1\n",
            "turnaround : 1\n",
            "expertise : 1\n",
            "gob : 1\n",
            "cajamarquilla : 1\n",
            "png : 1\n",
            "itel : 1\n",
            "spree : 2\n",
            "americas : 1\n",
            "habits : 1\n",
            "feeling : 1\n",
            "monroe : 1\n",
            "levied : 1\n",
            "fleming : 1\n",
            "grisanti : 1\n",
            "temp : 1\n",
            "aggressively : 3\n",
            "inherent : 2\n",
            "rainbow : 1\n",
            "alitalia : 1\n",
            "impatience : 1\n",
            "amo : 1\n",
            "opening : 1\n",
            "proceeding : 1\n",
            "metropolitan : 1\n",
            "sierra : 1\n",
            "needy : 1\n",
            "fshg : 3\n",
            "dai : 2\n",
            "commissioner : 1\n",
            "group : 1\n",
            "encroachments : 1\n",
            "commission : 1\n",
            "constantine : 1\n",
            "reading : 1\n",
            "verify : 1\n",
            "fried : 3\n",
            "cbtb : 1\n",
            "economist : 4\n",
            "method : 1\n",
            "revise : 1\n",
            "assaults : 5\n",
            "resurgence : 1\n",
            "americus : 1\n",
            "capitalized : 1\n",
            "projected : 1\n",
            "semiannual : 2\n",
            "treatment : 1\n",
            "antilles : 1\n",
            "nederlandse : 1\n",
            "labor : 15\n",
            "abilene : 1\n",
            "comon : 1\n",
            "curbed : 2\n",
            "intermediate : 1\n",
            "maintain : 1\n",
            "erbamont : 1\n",
            "obolensky : 1\n",
            "nicotine : 1\n",
            "topic : 2\n",
            "zealand : 1\n",
            "escape : 1\n",
            "welcomed : 4\n",
            "bec : 1\n"
          ]
        }
      ],
      "source": [
        "# vocab2 = vectorizer.get_feature_names()\n",
        "vocab2 = vectorizer.get_feature_names_out()\n",
        "for word, count in zip(vocab, data_features.toarray()[0]):\n",
        "    if count > 0:\n",
        "        print (word, \":\", count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--0EIl02Sc26"
      },
      "source": [
        "Another way to get the count list above is to use `FreqDist`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH80_1GBSc26",
        "outputId": "595553b4-b912-4e62-b258-68fcb0ae5ddb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'erc': 5, 'partnership': 4, 'shares': 3, 'stake': 2, 'international': 2, 'parsow': 2, 'investment': 2, 'pct': 2, 'common': 2, 'stock': 2, ...})"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "FreqDist(tokenized_reuters['training/1684'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhiVzko0Sc26"
      },
      "source": [
        "Note that the vocabulary you just got with `vectorizer.get_feature_names()` or `vectorizer.get_feature_names_out()`  shoud be exactly the same as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOAxpNrLSc26",
        "outputId": "53d9a1e6-ff35-4001-f884-5630a7155c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(vocab-set(vocab2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZiFsrsjSc27"
      },
      "source": [
        "#### 2.2.2 Creating TF-IDF Vectors\n",
        "Similar to the use of `CountVector`, we first initialise a `TfidfVectorizer` object by only specifying\n",
        "the value of \"analyzer\", and then covert the Reuters data into a list of strings, each of which corresponds\n",
        "to a Reuters articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atN-hQWRSc27",
        "outputId": "857bb61a-d6c4-4bd9-ff21-424de7aab2fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10788, 17408)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(analyzer = \"word\")\n",
        "tfs = tfidf.fit_transform([' '.join(value) for value in tokenized_reuters.values()])\n",
        "tfs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O5EqInhSc27",
        "outputId": "ddbd1703-f6b0-4d09-9e4e-9070b5c8d4e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accounting : 0.03824380547132344\n",
            "action : 0.030610220218964906\n",
            "advantage : 0.04392411496013759\n",
            "alleged : 0.08927471732522489\n",
            "american : 0.027677085531154454\n",
            "analyst : 0.03472473724338243\n",
            "april : 0.020866650380173362\n",
            "asia : 0.04365822325459151\n",
            "asian : 0.08812199459067033\n",
            "asked : 0.030570833605031753\n",
            "association : 0.031404654310321495\n",
            "australia : 0.07293404873879264\n",
            "australian : 0.037043385736673595\n",
            "awaiting : 0.04885226076212608\n",
            "aware : 0.04595799589769544\n",
            "barriers : 0.043529017024617124\n",
            "beef : 0.04203796774468898\n",
            "biggest : 0.03992164056026284\n",
            "billion : 0.09867294201265053\n",
            "block : 0.040726307565469196\n",
            "boost : 0.06988607911167637\n",
            "broker : 0.04595799589769544\n",
            "budget : 0.033247064776165465\n",
            "business : 0.026613390843530576\n",
            "businessmen : 0.18106474320049826\n",
            "button : 0.13262187160486735\n",
            "call : 0.03679736930639484\n",
            "canberra : 0.06350822634328575\n",
            "capel : 0.050749259364486356\n",
            "capitals : 0.05672840886885183\n",
            "centred : 0.0559142534173469\n",
            "chairman : 0.027047696666605902\n",
            "chief : 0.03296373281652167\n",
            "coal : 0.04235461846847909\n",
            "commercial : 0.033542508550586735\n",
            "complete : 0.03772729150857683\n",
            "concern : 0.03524526614342489\n",
            "concerns : 0.043529017024617124\n",
            "conflict : 0.04714588193527\n",
            "continue : 0.028601944851173566\n",
            "correspondents : 0.05672840886885183\n",
            "cost : 0.031382534828908175\n",
            "countries : 0.05477226032257796\n",
            "country : 0.029382947567457185\n",
            "curbs : 0.09386635602609497\n",
            "cut : 0.027651293217486033\n",
            "damage : 0.07747550711565242\n",
            "day : 0.030590498858816546\n",
            "defuse : 0.05518596654888442\n",
            "democratic : 0.04595799589769544\n",
            "deputy : 0.08111541148492105\n",
            "deterioration : 0.04714588193527\n",
            "diplomatic : 0.0473653394799576\n",
            "disadvantage : 0.0559142534173469\n",
            "dispute : 0.07697273665138131\n",
            "dlrs : 0.08066161307778365\n",
            "domestically : 0.0503947030820935\n",
            "due : 0.02556686621601832\n",
            "economic : 0.05430936489052692\n",
            "economy : 0.061950357617757384\n",
            "effort : 0.03836402297175659\n",
            "electric : 0.03992164056026284\n",
            "electronics : 0.16030101704303476\n",
            "emergency : 0.04327763179248158\n",
            "end : 0.025769758451649312\n",
            "erosion : 0.05392569940970391\n",
            "estimates : 0.03447847036175333\n",
            "exchange : 0.023346592230024582\n",
            "expand : 0.03756335500168817\n",
            "export : 0.02716667261660139\n",
            "exporters : 0.1069279080121248\n",
            "exporting : 0.04257407601316668\n",
            "exports : 0.16075765218319057\n",
            "extended : 0.03745618491629293\n",
            "failure : 0.03984609519582153\n",
            "fear : 0.04420064511648406\n",
            "fears : 0.04183464920346451\n",
            "federation : 0.0426864741326267\n",
            "financial : 0.026194333327546095\n",
            "firm : 0.029203396380455996\n",
            "firms : 0.03381965475185525\n",
            "fiscal : 0.03089239539975181\n",
            "foreign : 0.02526871403478771\n",
            "friction : 0.048581154169842176\n",
            "friday : 0.03304751968533576\n",
            "gain : 0.02741095005166527\n",
            "goods : 0.06543659759231096\n",
            "government : 0.023907358932947632\n",
            "group : 0.021962001272099733\n",
            "half : 0.029862211990354076\n",
            "halt : 0.04434317247612207\n",
            "helped : 0.03893248001336623\n",
            "hit : 0.03532293361270684\n",
            "hong : 0.1618996893333346\n",
            "hurt : 0.04183464920346451\n",
            "impact : 0.03388278151062303\n",
            "import : 0.03263838655302757\n",
            "imports : 0.14213469877890952\n",
            "impose : 0.04203796774468898\n",
            "include : 0.029601338311919946\n",
            "industrial : 0.029652731351959206\n",
            "industry : 0.08093184542811584\n",
            "interest : 0.02462483683588759\n",
            "international : 0.023230185670478767\n",
            "james : 0.03486950341503778\n",
            "japan : 0.3149599210557148\n",
            "japanese : 0.12097924090475128\n",
            "john : 0.03812564307185476\n",
            "kind : 0.04378989070305126\n",
            "kong : 0.1635964994712949\n",
            "korea : 0.1190923114930395\n",
            "kuroda : 0.057651422192854324\n",
            "large : 0.03179193728509643\n",
            "largest : 0.06237316317248436\n",
            "lawrence : 0.051122989950555975\n",
            "lead : 0.067957131870672\n",
            "leading : 0.03447847036175333\n",
            "length : 0.05286015872606341\n",
            "liberal : 0.046726824419285516\n",
            "loss : 0.02109716454394746\n",
            "lt : 0.032664262911335014\n",
            "major : 0.05064729352794748\n",
            "makoto : 0.057651422192854324\n",
            "malaysia : 0.0419355609524946\n",
            "manoeuvres : 0.06151967233564276\n",
            "manufacturers : 0.04098718124390333\n",
            "market : 0.020837768727947334\n",
            "markets : 0.05574731596542218\n",
            "matsushita : 0.06151967233564276\n",
            "matter : 0.04183464920346451\n",
            "measure : 0.03926973382854425\n",
            "measures : 0.06815139264201935\n",
            "meet : 0.033103947199044866\n",
            "michael : 0.041076373862616075\n",
            "mills : 0.044060997295335164\n",
            "minister : 0.083942484204319\n",
            "miti : 0.048320280491408046\n",
            "mln : 0.012444779255143326\n",
            "months : 0.025250499710564346\n",
            "mounting : 0.04942862212940336\n",
            "move : 0.030339113626681\n",
            "murtha : 0.059977230015675335\n",
            "nakasone : 0.04510353518888525\n",
            "named : 0.0396974371643465\n",
            "nations : 0.03193397650292828\n",
            "newspapers : 0.04973588747219542\n",
            "office : 0.03524526614342489\n",
            "officers : 0.046726824419285516\n",
            "official : 0.05201834101012212\n",
            "officials : 0.05474750711258751\n",
            "open : 0.03133850708742738\n",
            "outcome : 0.04478928416379764\n",
            "outlined : 0.046933178013047486\n",
            "outweighed : 0.057651422192854324\n",
            "package : 0.039065413572653425\n",
            "pact : 0.033363731377588156\n",
            "partners : 0.03447847036175333\n",
            "party : 0.03933920508710864\n",
            "paul : 0.0396242901109371\n",
            "pct : 0.03057078406377925\n",
            "place : 0.03552100825530342\n",
            "pressure : 0.06726732300567559\n",
            "prevent : 0.038866959787800595\n",
            "prime : 0.033788306253484945\n",
            "problems : 0.03414121654657531\n",
            "produced : 0.03633014203419874\n",
            "producers : 0.0316757609026054\n",
            "products : 0.08107243577089014\n",
            "program : 0.03129475800440456\n",
            "promotion : 0.0503947030820935\n",
            "proposed : 0.029284432854218757\n",
            "protectionist : 0.04055770574246052\n",
            "public : 0.030415493763424888\n",
            "purpose : 0.04633172648376506\n",
            "put : 0.032128019617847586\n",
            "quickly : 0.04023235947896642\n",
            "raised : 0.03176854527616286\n",
            "record : 0.022802346533809272\n",
            "reform : 0.04163714056553913\n",
            "relations : 0.04144511899441756\n",
            "remain : 0.03220221351845618\n",
            "remove : 0.045266185800124564\n",
            "representative : 0.038610918218646165\n",
            "reserves : 0.030434721403202823\n",
            "restraining : 0.05286015872606341\n",
            "retaliation : 0.07999604139401345\n",
            "reuter : 0.04183464920346451\n",
            "rift : 0.06151967233564276\n",
            "row : 0.042800730156154654\n",
            "ruling : 0.0396242901109371\n",
            "safe : 0.05518596654888442\n",
            "sales : 0.021761468309785398\n",
            "sell : 0.027794356936545698\n",
            "selling : 0.033542508550586735\n",
            "semiconductors : 0.13097466976377453\n",
            "senior : 0.0675141985676117\n",
            "sentiment : 0.043529017024617124\n",
            "seriousness : 0.06151967233564276\n",
            "serves : 0.04885226076212608\n",
            "share : 0.019616842454599975\n",
            "significant : 0.03560185516573626\n",
            "similar : 0.03615174937096513\n",
            "smith : 0.04365822325459151\n",
            "solve : 0.04577844471069425\n",
            "sources : 0.02827031997119515\n",
            "south : 0.09408145771252265\n",
            "spending : 0.07321334438109044\n",
            "spokesman : 0.026438385748138917\n",
            "spokesmen : 0.05151808788607644\n",
            "stick : 0.050749259364486356\n",
            "stimulate : 0.041076373862616075\n",
            "stock : 0.02108719722853169\n",
            "subject : 0.03062999800654392\n",
            "supplementary : 0.0559142534173469\n",
            "surplus : 0.09343134189566207\n",
            "swell : 0.06350822634328575\n",
            "taiwan : 0.15520821913881486\n",
            "taiwanese : 0.05337241763663309\n",
            "talks : 0.029482886639071598\n",
            "tariffs : 0.178213180020208\n",
            "taxes : 0.03913285149334667\n",
            "textile : 0.0473653394799576\n",
            "threat : 0.0426864741326267\n",
            "time : 0.026558224293387462\n",
            "tokyo : 0.07161641751899647\n",
            "told : 0.022221136108428446\n",
            "tom : 0.04973588747219542\n",
            "tough : 0.04378989070305126\n",
            "trade : 0.34042028247458994\n",
            "trading : 0.028062310332374194\n",
            "unofficial : 0.05672840886885183\n",
            "view : 0.07449340855579611\n",
            "virtually : 0.044637358662612446\n",
            "warning : 0.04714588193527\n",
            "washington : 0.03301947773189486\n",
            "week : 0.024089472615638277\n",
            "works : 0.046526452939305074\n",
            "world : 0.0529200412157522\n",
            "worried : 0.046526452939305074\n",
            "yasuhiro : 0.04560343961530258\n",
            "year : 0.0593829498029965\n",
            "yesterday : 0.027498528280749005\n"
          ]
        }
      ],
      "source": [
        "# vocab = vectorizer.get_feature_names()\n",
        "vocab = vectorizer.get_feature_names_out()\n",
        "for word, weight in zip(vocab, tfs.toarray()[0]):\n",
        "    if weight > 0:\n",
        "        print (word, \":\", weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3kK1YJgSc27"
      },
      "source": [
        "So now we have converted all the Reuters articles into feature vectors.\n",
        "We can use those vectors to, for example,\n",
        "* compute the similarity between two articles,\n",
        "* search articles for a given query\n",
        "* do other advance text analysis, such as document classification and clustering.\n",
        "\n",
        "Assume that we have a new document, how can we get its TF-IDF vector.\n",
        "We do this by using the transform function as follows.\n",
        "We have randomly chosen a sentence from\n",
        "[a recent Reuters news](http://www.reuters.com/article/us-usa-election-idUSKCN0W346T)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiotMgB1Sc27",
        "outputId": "baadb3f3-64fd-48b9-f471-b8f16dcfc194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "big  -  0.25208797149625073\n",
            "democratic  -  0.2923827260361501\n",
            "fight  -  0.3013361833184753\n",
            "hoped  -  0.2667921738023468\n",
            "nomination  -  0.3913854194683875\n",
            "secretary  -  0.20108220692083675\n",
            "senator  -  0.32524218850047387\n",
            "state  -  0.18714392191939005\n",
            "states  -  0.17542767104552628\n",
            "step  -  0.25697922435891357\n",
            "vermont  -  0.4040365116291299\n",
            "win  -  0.3074114755647501\n"
          ]
        }
      ],
      "source": [
        "str = \"\"\"\n",
        "the former secretary of state hoped to win enough states to take a big step toward wrapping up her nomination fight\n",
        "with a democratic senator from Vermont.\n",
        "\"\"\"\n",
        "response = tfidf.transform([str])\n",
        "for col in response.nonzero()[1]:\n",
        "    print (vocab[col], ' - ', response[0, col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ6LqmyDSc27"
      },
      "source": [
        "Note that the text above is not included in the trained TF-IDF model with the 'transform' function, unless the `fit_transform` function is called,\n",
        "\n",
        "Both `CountVectorizer` and `TfidfVectorizer` come with their own options to automatically do pre-processing, tokenization, and stop word removal -- for each of these, instead of using their default value (i.e., None),\n",
        "we could customise the two vectorizer classes by either using a built-in method or specifying our own function.\n",
        "See the function documentation for more details.\n",
        "However, we wanted to write our own function for clean the text data in this chapter to show you how\n",
        "it's done step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcL5xIpSSc27"
      },
      "source": [
        "Let's print out the weighted vector for the first document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ervHRuQFSc28"
      },
      "source": [
        "### 2.3. Saving Pre-processed Text to a File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCjHr359Sc28"
      },
      "source": [
        "The pre-processed text needs to be saved in a proper format so that it can be easily used by the downstream analysis algorithm. There are a couple of ways of dumping the pre-processed text data into txt files.\n",
        "For example, use one txt file to store the tokenized documents. The tokens in a document are stored in one row in the txt file, and are separated with a given delimiter, e.g., whitespace. In this case, the downstream text analyser needs to re-construct the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "n7raddgvSc28"
      },
      "outputs": [],
      "source": [
        "out_file = open(\"./reuters_1.txt\", 'w')\n",
        "for d in tokenized_reuters.values():\n",
        "    out_file.write(' '.join(d) + '\\n')\n",
        "out_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E8XrsMUSc28"
      },
      "source": [
        "You can also save vocabulary in a separate file, and assign a fixed integer id to each word in the vocabulary. What text analysers usually do is to use the index of each word in the vocabulary as its integer id.\n",
        "Given the vocabulary, each document can be represented as a sequence of integers that correspond to the tokens,\n",
        "or in the following sparse form:\n",
        "```\n",
        "    word_index:word count\n",
        "```\n",
        "for example,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "dUe1GXe-Sc28"
      },
      "outputs": [],
      "source": [
        "out_file = open(\"./reuters_2.txt\", 'w')\n",
        "vocab = list(vocab)\n",
        "vocab_dict = {}\n",
        "i = 0\n",
        "for w in vocab:\n",
        "    vocab_dict[w] = i\n",
        "    i = i + 1\n",
        "for d in tokenized_reuters.values():\n",
        "    d_idx = [vocab_dict[w] for w in d]\n",
        "    for k, v in FreqDist(d_idx).items():\n",
        "        out_file.write(\"{}:{} \".format(k,v))\n",
        "    out_file.write('\\n')\n",
        "out_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuaPwdwSc28"
      },
      "source": [
        "### 2.4. Extracting Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87Ee_ZbSc28"
      },
      "source": [
        "It is common for most text analysis tasks to treat documents as bags-of-words, which can significantly simplify the inference procedure of text analysis algorithms.\n",
        "However, things always have pros and cons.\n",
        "The bag-of-words representation loses lots of information encoded in either syntax or word order (i.e., dependencies between adjacent words in sentences.).\n",
        "For example, representing a document as a collection of unigrams effectively disregards any word order dependence,\n",
        "which fails to capture phrases and multi-word expressions. A similar issue has been mentioned in section 2.1. of Chapter 2.\n",
        "In this section, we are going to show you how to\n",
        "* use Part-of-Speeching (POS) tagging to extract specific word groups, such as all nouns, verbs, etc.,\n",
        "* extract n-grams,\n",
        "*  and extract collocations\n",
        "\n",
        "These features can be further used to enrich the representation of a document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST4E-ZzabY4y"
      },
      "source": [
        "#### 2.4.1 Extracting Nouns and Verbs\n",
        "\n",
        "It is easy for human to tell the difference between nouns, verbs,\n",
        "adjectives and adverbs, as we have learnt them back in elementary school.\n",
        "However, how can we automatically classify words into their parts of speech (i.e., lexical categories or word classes)\n",
        "and label them accordingly with computer program?\n",
        "This section is not going to discuss how to determine the category of a word from a linguistic perspective.\n",
        "Instead it demonstrates the use of some existing POS taggers to extract words in a specific lexical category.\n",
        "It has been proven that words together with their part-of-speech (POS) are quite useful for many language processing tasks.\n",
        "\n",
        "In NLP, the process of labelling words with their corresponding part-of-speech (POS) tags is known as [POS tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging).\n",
        "A POS tagger processes a sequence of words and attaches a POS tag to each word based on both its definition and its context. There are many POS taggers available online, such as [Sandford POS tagger](http://nlp.stanford.edu/software/tagger.shtml).\n",
        "We are going to use the one implemented by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xnm3OO-ySc28",
        "outputId": "479550b7-c101-4319-bb45-a4e2a7ebe96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('A', 'DT'), ('POS', 'NNP'), ('tagger', 'NN'), ('processes', 'VBZ'), ('a', 'DT')]\n",
            "[('sequence', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('and', 'CC'), ('attaches', 'VBZ')]\n",
            "[('a', 'DT'), ('POS', 'NNP'), ('tag', 'NN'), ('to', 'TO'), ('each', 'DT')]\n",
            "[('word', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('both', 'DT'), ('its', 'PRP$')]\n",
            "[('definition', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('context', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "# nltk.download('averaged_perceptron_tagger') download the tagger if you haven't\n",
        "example_sent = 'A POS tagger processes a sequence of words and attaches a POS tag to each \\\n",
        "word based on both its definition and its context'\n",
        "text = nltk.word_tokenize(example_sent)\n",
        "tagged_sent = nltk.tag.pos_tag(text)\n",
        "# print the tagged sentence, every 5 tokens in a new line\n",
        "for i in range(0, len(tagged_sent), 5):\n",
        "    print(tagged_sent[i:i+5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BH89F9Sc28"
      },
      "source": [
        "If you are seeing these tags for the first time, you will wonder what these tags mean.\n",
        "You can find the specification of all the tags [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n",
        "NLTK provides documentation for each tag, which can be queried using the tag, e.g.,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNvnuu1VSc28",
        "outputId": "d46ab52a-116b-4edd-9d4f-bd411350d0f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "None\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "None\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# nltk.download('tagsets') download the tagsets if you haven't\n",
        "print (nltk.help.upenn_tagset('NNP'))\n",
        "print (nltk.help.upenn_tagset('IN'))\n",
        "print (nltk.help.upenn_tagset('PRP$'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1frvZgASc29"
      },
      "source": [
        "The example sentence has been processed by `pos_tag` into a list of tuples, each of which is a pair of a word and its POS tag. We see that 'a' is 'DT', a determiner; 'its' is 'PRP$', a possessive pronoun; 'and' is 'CC', a coordinating conjunction, 'words' is 'NNS', a noun in the plural form, and so on. Note that several of the corpora included in NLTK have been tagged for their POS. Please click [here](http://www.nltk.org/howto/corpus.html#tagged-corpora) to see how to access those tagged corpora.\n",
        "Here is an example of using the `tagged_words` function to retrieve all words in Brown corpus with their tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIRb0JU9Sc29"
      },
      "source": [
        "`nltk.download('brown')` is a module that provides a number of corpora, including the Brown corpus. The following code shows how to use the `brown` corpus to load the Brown articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1NIRyR2Sc29",
        "outputId": "bb621b1e-5364-459d-8eec-edbd4dec4068"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('brown')\n",
        "nltk.corpus.brown.tagged_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvLaGjgrSc29"
      },
      "source": [
        "Please note that the collection of tags is known as a tag set.\n",
        "There are many different conventions for tagging words.\n",
        "Therefore, tag sets can vary among different tasks.\n",
        "What we used above is the Penn Treebank tag set.\n",
        "Let's change the tag set to the Universal POS tag set, and print the Brown corpus again.\n",
        "You will find different tags are used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqYgNh0Sc29"
      },
      "source": [
        "`nltk.download('universal_tagset')` is a module that provides a number of tagsets. In this tutorial, we are going to use the `universal_tagset` to get the meaning of each tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsBpKLXmSc29",
        "outputId": "98c2a46d-a2de-4712-ded6-0b85263aa1ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('universal_tagset')\n",
        "nltk.corpus.brown.tagged_words(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjORqSwxSc29"
      },
      "source": [
        "If you would like to learn more about POS tagging, please refer to [1].\n",
        "\n",
        "Given the tagged text, you can easily identify all the nouns, verbs, etc.\n",
        "Nouns generally refer to people, places, things, or concepts, e.g., Monash, Melbourne, university, data, and science.\n",
        "Nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
        "Now how can we extract all the nouns from a text?\n",
        "Assume we use the Penn Treebank tag set.\n",
        "Here are all the tags for nouns:\n",
        "```\n",
        "    NN    Noun, singular or mass\n",
        "    NNS   Noun, plural\n",
        "    NNP   Proper noun, singular\n",
        "    NNPS  Proper noun, plural\n",
        "```\n",
        "It is not hard to see all the tags above start with 'NN'.\n",
        "Thus, we can iterate over all the words and check if their tag string starts with 'NN'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtHtbwdRSc2-",
        "outputId": "3dd0f5d2-1e57-4b1a-c4af-2dbe41193dc4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['POS',\n",
              " 'tagger',\n",
              " 'sequence',\n",
              " 'words',\n",
              " 'POS',\n",
              " 'tag',\n",
              " 'word',\n",
              " 'definition',\n",
              " 'context']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_nouns = [w for w,t in tagged_sent if t.startswith('NN')]\n",
        "all_nouns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0CYW3vZSc2-"
      },
      "source": [
        "Similarly, you will find that all the verb tags start with 'VB', see\n",
        "```\n",
        "    VB\tVerb, base form\n",
        "    VBD   Verb, past tense\n",
        "    VBG   Verb, gerund or present participle\n",
        "    VBN   Verb, past participle\n",
        "    VBP   Verb, non-3rd person singular present\n",
        "    VBZ   Verb, 3rd person singular present\n",
        "```\n",
        "Thus,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d39nTP9-Sc2-",
        "outputId": "0deed372-8158-467c-8c16-b7cbde9f8958"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['processes', 'attaches', 'based']"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_verbs = [w for w,t in tagged_sent if t.startswith('VB')]\n",
        "all_verbs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hURBhgbSSc2-"
      },
      "source": [
        "Unfortunately, the Reuters corpus that we have been using, has no built-in POS tags. But you can get sentences from Reuters corpus, and then you can get the POS tags."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI3DlZF9Sc2-"
      },
      "source": [
        "#### 2.4.2 Extracting N-grams and Collocations\n",
        "\n",
        "Besides unigrams that we have been working on so far,\n",
        "N-grams of texts are also extensively used in various text analysis tasks.\n",
        "They are basically contiguous sequences of `n` words from a given sequence of text.\n",
        "When computing the n-grams you typically move a fixed size window of size n\n",
        "words forward.\n",
        "For example, for the sentence\n",
        "\"Laughter is like a windshield wiper.\"\n",
        "if N = 2 (known as bigrams), the n-grams would be:\n",
        "```\n",
        "    Laughter is\n",
        "    is like\n",
        "    like a\n",
        "    a windshield\n",
        "    windshield wiper\n",
        "```\n",
        "So you have 5 bigrams in this case. Notice that the generative process above\n",
        "essentially moves one word forward to generate the next bigram.\n",
        "If N = 3 (known as trigrams), the n-grams would be:\n",
        "```\n",
        "    Laughter is like\n",
        "    is like a\n",
        "    like a  windshield\n",
        "    a  windshield wiper\n",
        "```\n",
        "What are N-grams used for? They can be used to build n-gram language model that\n",
        "can be further used for speech recognition, spelling correction, entity detection, etc.\n",
        "In terms of text mining tasks, n-grams is used for developing features for\n",
        "classification algorithms, such as SVMs, MaxEnt models, Naive Bayes, etc.\n",
        "The idea is to expand the unigram feature space with n-grams.\n",
        "But please notice that\n",
        "the use of bigrams and trigrams in your feature space may not necessarily yield significant performance\n",
        "improvement. The only way to know this is to try it!\n",
        "Extracting from a text a list of n-gram can be easily accomplished with function `ngram()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zoF5nDMQSc2-"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "bigrams = ngrams(reuters.words(), n = 2)\n",
        "fdbigram = FreqDist(bigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi_nDTYDSc2-",
        "outputId": "47fdcafc-f8b5-459a-b560-dbb2b929acff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[((',', '000'), 10266),\n",
              " ((\"'\", 's'), 9220),\n",
              " (('lt', ';'), 8693),\n",
              " (('&', 'lt'), 8688),\n",
              " (('.', 'The'), 8530),\n",
              " (('said', '.'), 7888),\n",
              " (('of', 'the'), 6803),\n",
              " (('in', 'the'), 6487),\n",
              " (('U', '.'), 6350),\n",
              " (('.', 'S'), 5833),\n",
              " (('S', '.'), 5459),\n",
              " (('1', '.'), 4414),\n",
              " ((',', 'the'), 4296),\n",
              " (('mln', 'dlrs'), 4192),\n",
              " (('said', 'it'), 4003),\n",
              " (('mln', 'vs'), 3916),\n",
              " (('said', 'the'), 3604),\n",
              " (('.', '\"'), 3512),\n",
              " (('cts', 'vs'), 3209),\n",
              " (('.', '5'), 2779),\n",
              " (('for', 'the'), 2665),\n",
              " (('000', 'vs'), 2578),\n",
              " (('to', 'the'), 2465),\n",
              " (('1', ','), 2425),\n",
              " (('2', '.'), 2375),\n",
              " (('cts', 'Net'), 2179),\n",
              " (('.', '2'), 1983),\n",
              " (('the', 'U'), 1959),\n",
              " (('the', 'company'), 1941),\n",
              " (('.', '1'), 1924),\n",
              " (('on', 'the'), 1907),\n",
              " ((',', 'which'), 1896),\n",
              " (('.', '3'), 1841),\n",
              " ((',', 'and'), 1828),\n",
              " (('.', '4'), 1815),\n",
              " (('.', '6'), 1798),\n",
              " (('.', '8'), 1789),\n",
              " (('vs', 'loss'), 1746),\n",
              " (('.', '9'), 1726),\n",
              " (('.', '7'), 1719),\n",
              " (('.', 'It'), 1707),\n",
              " (('3', '.'), 1667),\n",
              " (('he', 'said'), 1633),\n",
              " (('will', 'be'), 1623),\n",
              " (('billion', 'dlrs'), 1596),\n",
              " (('to', 'be'), 1586),\n",
              " (('.', '0'), 1586),\n",
              " (('dlrs', 'in'), 1583),\n",
              " (('and', 'the'), 1554),\n",
              " (('by', 'the'), 1543),\n",
              " ((',', 'a'), 1533),\n",
              " (('000', 'dlrs'), 1526),\n",
              " (('2', ','), 1525),\n",
              " (('pct', 'of'), 1498),\n",
              " (('with', 'the'), 1435),\n",
              " (('dlrs', ','), 1404),\n",
              " (('.', 'He'), 1385),\n",
              " ((',', 'but'), 1359),\n",
              " (('that', 'the'), 1346),\n",
              " (('5', 'mln'), 1338),\n",
              " (('from', 'the'), 1318),\n",
              " (('.', 'In'), 1315),\n",
              " (('dlrs', '.'), 1269),\n",
              " (('last', 'year'), 1243),\n",
              " (('at', 'the'), 1228),\n",
              " (('company', 'said'), 1217),\n",
              " (('4', '.'), 1215),\n",
              " (('0', '.'), 1213),\n",
              " (('in', 'a'), 1203),\n",
              " (('000', 'Revs'), 1198),\n",
              " (('The', 'company'), 1185),\n",
              " (('a', 'share'), 1167),\n",
              " (('dlrs', 'vs'), 1167),\n",
              " (('1', 'mln'), 1145),\n",
              " (('vs', '1'), 1145),\n",
              " (('2', 'mln'), 1139),\n",
              " (('of', 'its'), 1133),\n",
              " ((',', 'or'), 1119),\n",
              " (('3', 'mln'), 1117),\n",
              " (('pct', 'in'), 1115),\n",
              " (('of', 'a'), 1114),\n",
              " (('would', 'be'), 1112),\n",
              " ((',', 'said'), 1102),\n",
              " (('3', ','), 1092),\n",
              " (('6', 'mln'), 1086),\n",
              " ((',', 'it'), 1084),\n",
              " (('8', 'mln'), 1065),\n",
              " (('4', 'mln'), 1060),\n",
              " (('NET', 'Shr'), 1060),\n",
              " (('INC', '&'), 1059),\n",
              " (('9', 'mln'), 1054),\n",
              " (('NOTE', ':'), 1054),\n",
              " (('year', '.'), 1037),\n",
              " ((',', 'he'), 1031),\n",
              " (('Avg', 'shrs'), 1027),\n",
              " (('to', 'a'), 1026),\n",
              " (('this', 'year'), 1020),\n",
              " (('5', '.'), 1012),\n",
              " (('7', 'mln'), 1007),\n",
              " (('.', 'But'), 989),\n",
              " (('vs', 'profit'), 979),\n",
              " ((',\"', 'he'), 968),\n",
              " (('0', 'mln'), 966),\n",
              " (('per', 'share'), 952),\n",
              " (('000', 'tonnes'), 948),\n",
              " (('dlrs', 'a'), 930),\n",
              " (('in', '1986'), 926),\n",
              " (('.', 'A'), 908),\n",
              " (('It', 'said'), 901),\n",
              " (('year', ','), 900),\n",
              " (('the', 'dollar'), 878),\n",
              " (('in', 'January'), 873),\n",
              " (('He', 'said'), 871),\n",
              " (('4', ','), 865),\n",
              " (('/', '87'), 837),\n",
              " (('said', 'that'), 830),\n",
              " (('a', 'year'), 829),\n",
              " (('the', 'first'), 810),\n",
              " (('it', 'has'), 806),\n",
              " (('7', '.'), 800),\n",
              " (('expected', 'to'), 798),\n",
              " (('QTR', 'NET'), 797),\n",
              " (('6', '.'), 794),\n",
              " (('mln', 'stg'), 794),\n",
              " (('mln', 'tonnes'), 788),\n",
              " (('1986', '.'), 785),\n",
              " (('pct', '.'), 782),\n",
              " (('-', '1'), 776),\n",
              " (('Inc', 'said'), 776),\n",
              " (('in', 'February'), 771),\n",
              " (('said', 'its'), 743),\n",
              " (('4TH', 'QTR'), 738),\n",
              " (('\"', 'The'), 728),\n",
              " (('1986', ','), 726),\n",
              " (('for', 'a'), 724),\n",
              " (('CORP', '&'), 724),\n",
              " (('has', 'been'), 723),\n",
              " (('said', 'in'), 715),\n",
              " (('Bank', 'of'), 715),\n",
              " (('Net', 'loss'), 706),\n",
              " (('the', 'year'), 704),\n",
              " (('in', '1985'), 702),\n",
              " (('Shr', 'loss'), 696),\n",
              " (('5', ','), 694),\n",
              " (('10', '.'), 693),\n",
              " (('.', 'K'), 692),\n",
              " (('it', 'said'), 687),\n",
              " (('5', 'pct'), 676),\n",
              " (('1', '/'), 672),\n",
              " (('as', 'a'), 661),\n",
              " (('tonnes', 'of'), 658),\n",
              " (('compared', 'with'), 657),\n",
              " (('K', '.'), 656),\n",
              " ((',', 'with'), 655),\n",
              " (('pct', ','), 654),\n",
              " (('due', 'to'), 650),\n",
              " (('it', 'is'), 645),\n",
              " (('mln', 'Avg'), 638),\n",
              " (('>', '4TH'), 634),\n",
              " (('vs', '2'), 632),\n",
              " (('8', '.'), 627),\n",
              " (('said', 'he'), 625),\n",
              " (('United', 'States'), 621),\n",
              " (('the', 'government'), 620),\n",
              " (('said', ','), 614),\n",
              " (('it', 'will'), 603),\n",
              " (('\"', 'We'), 601),\n",
              " (('added', '.'), 601),\n",
              " (('.', '50'), 601),\n",
              " (('cts', 'a'), 597),\n",
              " (('the', 'United'), 595),\n",
              " (('sources', 'said'), 594),\n",
              " (('company', \"'\"), 591),\n",
              " (('.', 'O'), 591),\n",
              " (('mln', 'NOTE'), 588),\n",
              " (('also', 'said'), 586),\n",
              " (('Corp', 'said'), 585),\n",
              " ((',', 'in'), 584),\n",
              " ((',\"', 'said'), 584),\n",
              " (('dlrs', 'per'), 583),\n",
              " (('did', 'not'), 578),\n",
              " (('and', 'a'), 577),\n",
              " (('9', '.'), 576),\n",
              " (('.', 'U'), 573),\n",
              " (('dlrs', 'Net'), 571),\n",
              " (('at', 'a'), 564),\n",
              " (('6', ','), 564),\n",
              " (('billion', 'vs'), 556),\n",
              " (('more', 'than'), 552),\n",
              " (('end', 'of'), 550),\n",
              " (('sale', 'of'), 549),\n",
              " (('11', '.'), 549),\n",
              " (('share', ','), 548),\n",
              " (('12', '.'), 545),\n",
              " (('with', 'a'), 540),\n",
              " (('the', 'market'), 537),\n",
              " ((',', 'they'), 536),\n",
              " (('15', '.'), 533),\n",
              " (('the', 'same'), 533),\n",
              " (('pct', 'from'), 529),\n",
              " (('mths', 'Shr'), 523),\n",
              " (('on', 'a'), 521),\n",
              " (('to', 'buy'), 521),\n",
              " (('dlrs', 'from'), 521),\n",
              " (('3RD', 'QTR'), 519),\n",
              " (('the', 'end'), 518),\n",
              " (('1985', '.'), 513),\n",
              " (('1ST', 'QTR'), 513),\n",
              " (('agreed', 'to'), 511),\n",
              " (('have', 'been'), 511),\n",
              " (('O', '>'), 510),\n",
              " (('non', '-'), 509),\n",
              " (('Oper', 'shr'), 508),\n",
              " (('Oper', 'net'), 505),\n",
              " (('spokesman', 'said'), 503),\n",
              " (('1986', '/'), 503),\n",
              " (('7', ','), 500),\n",
              " (('over', 'the'), 497),\n",
              " (('of', '1'), 497),\n",
              " (('is', 'expected'), 492),\n",
              " (('it', 'was'), 492),\n",
              " (('16', '.'), 492),\n",
              " (('13', '.'), 489),\n",
              " (('had', 'been'), 487),\n",
              " (('>', '3RD'), 487),\n",
              " (('2', 'pct'), 487),\n",
              " (('share', '.'), 485),\n",
              " (('first', 'quarter'), 483),\n",
              " (('000', 'Sales'), 482),\n",
              " (('vs', '3'), 482),\n",
              " (('mln', 'Revs'), 482),\n",
              " (('8', ','), 479),\n",
              " (('4', 'pct'), 478),\n",
              " (('it', 'would'), 473),\n",
              " (('up', 'to'), 469),\n",
              " (('17', '.'), 468),\n",
              " (('common', 'stock'), 468),\n",
              " (('the', 'sale'), 467),\n",
              " ((',', 'to'), 458),\n",
              " (('that', 'it'), 450),\n",
              " (('loss', '1'), 450),\n",
              " (('/', '2'), 449),\n",
              " (('Net', 'profit'), 445),\n",
              " (('rise', 'in'), 444),\n",
              " (('New', 'York'), 444),\n",
              " (('because', 'of'), 442),\n",
              " (('pct', 'to'), 440),\n",
              " (('last', 'month'), 440),\n",
              " (('.', 'They'), 436),\n",
              " (('.', 'This'), 436),\n",
              " ((',', '\"'), 436),\n",
              " (('billion', 'in'), 434),\n",
              " (('told', 'Reuters'), 433),\n",
              " (('in', 'its'), 433),\n",
              " (('>', 'said'), 432),\n",
              " (('Year', 'Shr'), 428),\n",
              " (('is', 'a'), 427),\n",
              " (('cts', 'prior'), 427),\n",
              " (('in', '1987'), 426),\n",
              " (('14', '.'), 424),\n",
              " (('from', 'a'), 421),\n",
              " (('Nine', 'mths'), 421),\n",
              " (('to', 'acquire'), 420),\n",
              " (('-', 'year'), 419),\n",
              " (('central', 'bank'), 417),\n",
              " (('the', 'new'), 416),\n",
              " (('>', '1ST'), 416),\n",
              " (('Qtly', 'div'), 415),\n",
              " (('to', 'sell'), 414),\n",
              " ((',', 'while'), 414),\n",
              " (('8', 'pct'), 412),\n",
              " (('cts', 'per'), 409),\n",
              " (('the', 'current'), 408),\n",
              " (('stake', 'in'), 406),\n",
              " (('However', ','), 405),\n",
              " (('/', '4'), 405),\n",
              " (('part', 'of'), 404),\n",
              " (('loss', 'of'), 401),\n",
              " (('shares', 'of'), 401),\n",
              " ((\"'\", 't'), 399),\n",
              " ((',', 'up'), 394),\n",
              " (('tonnes', ','), 393),\n",
              " (('dlrs', 'and'), 392),\n",
              " (('\"', 'I'), 392),\n",
              " (('said', 'they'), 391),\n",
              " (('last', 'week'), 390),\n",
              " (('a', 'new'), 389),\n",
              " (('Shr', 'profit'), 389),\n",
              " (('they', 'said'), 388),\n",
              " (('9', ','), 387),\n",
              " (('the', 'previous'), 387),\n",
              " (('likely', 'to'), 383),\n",
              " (('18', '.'), 383),\n",
              " (('cts', 'Oper'), 383),\n",
              " (('dlrs', 'or'), 383),\n",
              " ((\"'\", 'S'), 378),\n",
              " (('mln', 'dlr'), 377),\n",
              " (('.', 'However'), 376),\n",
              " ((',', 'including'), 375),\n",
              " (('sales', 'of'), 375),\n",
              " (('>', 'TO'), 375),\n",
              " (('-', 'term'), 373),\n",
              " (('.', '25'), 373),\n",
              " (('of', 'about'), 372),\n",
              " (('March', '31'), 371),\n",
              " (('shares', ','), 371),\n",
              " (('Inc', ','), 370),\n",
              " (('interest', 'rates'), 368),\n",
              " (('by', 'a'), 366),\n",
              " (('under', 'the'), 366),\n",
              " (('and', 'other'), 365),\n",
              " (('rose', 'to'), 364),\n",
              " ((',', '500'), 364),\n",
              " (('mln', 'in'), 363),\n",
              " (('a', 'statement'), 362),\n",
              " ((',', 'for'), 362),\n",
              " (('as', 'the'), 360),\n",
              " (('officials', 'said'), 358),\n",
              " (('West', 'Germany'), 358),\n",
              " (('would', 'not'), 357),\n",
              " (('analysts', 'said'), 356),\n",
              " (('plans', 'to'), 351),\n",
              " (('in', 'December'), 351),\n",
              " (('1', 'pct'), 350),\n",
              " (('during', 'the'), 349),\n",
              " (('Corp', '&'), 349),\n",
              " (('year', '-'), 349),\n",
              " (('he', 'added'), 348),\n",
              " (('if', 'the'), 347),\n",
              " (('after', 'a'), 347),\n",
              " (('vs', '4'), 347),\n",
              " (('19', '.'), 347),\n",
              " (('dlrs', 'for'), 346),\n",
              " (('is', 'not'), 346),\n",
              " (('.', '00'), 345),\n",
              " (('-', '3'), 344),\n",
              " (('subject', 'to'), 343),\n",
              " ((',', 'as'), 342),\n",
              " (('to', '1'), 342),\n",
              " (('3', 'pct'), 342),\n",
              " (('increase', 'in'), 341),\n",
              " (('the', 'next'), 341),\n",
              " (('the', 'two'), 340),\n",
              " (('which', 'is'), 339),\n",
              " (('1987', '.'), 337),\n",
              " (('>', 'YEAR'), 336),\n",
              " (('against', 'the'), 334),\n",
              " (('year', 'earlier'), 334),\n",
              " (('.', '75'), 333),\n",
              " (('>', 'SETS'), 332),\n",
              " ((',', 'who'), 331),\n",
              " (('Corp', ','), 330),\n",
              " ((',', 'is'), 327),\n",
              " (('9', 'pct'), 327),\n",
              " (('to', 'take'), 326),\n",
              " (('year', \"'\"), 325),\n",
              " (('prior', 'Pay'), 323),\n",
              " (('common', 'shares'), 323),\n",
              " (('6', 'pct'), 322),\n",
              " (('official', 'said'), 321),\n",
              " (('the', 'EC'), 321),\n",
              " (('7', 'pct'), 320),\n",
              " (('is', 'the'), 319),\n",
              " (('1987', ','), 319),\n",
              " ((',', 'an'), 319),\n",
              " (('20', '.'), 318),\n",
              " (('quarter', 'and'), 318),\n",
              " (('31', ','), 317),\n",
              " (('after', 'the'), 317),\n",
              " (('foreign', 'exchange'), 316),\n",
              " (('mln', '.'), 314),\n",
              " (('it', 'had'), 313),\n",
              " (('could', 'be'), 313),\n",
              " (('oil', 'and'), 313),\n",
              " (('-', 'for'), 312),\n",
              " (('the', 'world'), 311),\n",
              " (('for', '-'), 309),\n",
              " (('company', '.'), 308),\n",
              " (('3', '/'), 306),\n",
              " (('LOSS', 'Shr'), 306),\n",
              " ((',', 'compared'), 305),\n",
              " (('The', 'U'), 303),\n",
              " (('the', 'country'), 303),\n",
              " (('have', 'to'), 302),\n",
              " (('does', 'not'), 302),\n",
              " (('30', ','), 301),\n",
              " (('it', 'expects'), 301),\n",
              " (('according', 'to'), 300),\n",
              " (('rate', 'of'), 300),\n",
              " ((',', '1986'), 300),\n",
              " (('said', 'a'), 299),\n",
              " (('would', 'have'), 299),\n",
              " ((',', 'will'), 297),\n",
              " (('/', '8'), 297),\n",
              " (('Note', ':'), 297),\n",
              " (('crude', 'oil'), 296),\n",
              " (('Japan', \"'\"), 295),\n",
              " (('stock', '.'), 295),\n",
              " (('tender', 'offer'), 292),\n",
              " (('and', 'Co'), 291),\n",
              " (('31', '.'), 291),\n",
              " (('mln', 'Year'), 291),\n",
              " (('dlrs', 'of'), 290),\n",
              " (('Inc', '&'), 290),\n",
              " (('January', ','), 290),\n",
              " (('vs', '5'), 289),\n",
              " (('acquisition', 'of'), 289),\n",
              " (('oil', 'prices'), 288),\n",
              " (('1985', ','), 288),\n",
              " (('the', 'second'), 288),\n",
              " (('West', 'German'), 288),\n",
              " (('the', 'Bank'), 288),\n",
              " (('of', 'U'), 287),\n",
              " (('gain', 'of'), 287),\n",
              " (('dlrs', 'to'), 286),\n",
              " (('were', 'not'), 286),\n",
              " (('its', 'board'), 286),\n",
              " (('continue', 'to'), 285),\n",
              " (('Securities', 'and'), 284),\n",
              " (('fourth', 'quarter'), 284),\n",
              " (('value', 'of'), 283),\n",
              " (('mln', 'shares'), 283),\n",
              " (('number', 'of'), 283),\n",
              " (('/', '86'), 283),\n",
              " (('5', 'billion'), 282),\n",
              " (('and', 'Exchange'), 282),\n",
              " (('a', 'barrel'), 282),\n",
              " (('In', 'a'), 282),\n",
              " ((',', 'was'), 281),\n",
              " (('1985', '/'), 280),\n",
              " ((',', '1987'), 280),\n",
              " (('/', '09'), 280),\n",
              " (('-', 'based'), 279),\n",
              " (('\"', 'It'), 278),\n",
              " (('such', 'as'), 278),\n",
              " (('mln', 'Nine'), 278),\n",
              " (('09', '/'), 278),\n",
              " (('year', 'ago'), 277),\n",
              " (('price', 'of'), 277),\n",
              " (('Exchange', 'Commission'), 277),\n",
              " (('buffer', 'stock'), 276),\n",
              " (('shares', '.'), 275),\n",
              " (('the', 'past'), 275),\n",
              " (('interest', 'in'), 275),\n",
              " (('this', 'week'), 274),\n",
              " (('told', 'the'), 274),\n",
              " (('two', '-'), 272),\n",
              " (('Shr', '1'), 272),\n",
              " (('not', 'be'), 271),\n",
              " (('about', 'the'), 271),\n",
              " (('was', 'not'), 270),\n",
              " (('.', 'Agriculture'), 270),\n",
              " (('The', 'Bank'), 270),\n",
              " (('company', ','), 269),\n",
              " (('Net', '1'), 269),\n",
              " (('M', '-'), 269),\n",
              " (('at', 'least'), 268),\n",
              " (('the', 'bank'), 267),\n",
              " (('10', 'pct'), 267),\n",
              " (('in', 'an'), 266),\n",
              " (('on', 'its'), 266),\n",
              " (('the', 'acquisition'), 266),\n",
              " (('based', 'on'), 266),\n",
              " (('N', '.'), 266),\n",
              " (('the', 'offer'), 266),\n",
              " (('50', 'pct'), 265),\n",
              " (('fell', 'to'), 265),\n",
              " (('to', 'make'), 264),\n",
              " (('net', 'profit'), 264),\n",
              " (('50', 'dlrs'), 264),\n",
              " (('net', 'includes'), 264),\n",
              " (('the', 'Securities'), 262),\n",
              " (('billion', 'marks'), 262),\n",
              " (('dealers', 'said'), 262),\n",
              " (('up', 'from'), 262),\n",
              " (('and', 'that'), 262),\n",
              " (('one', 'of'), 260),\n",
              " (('will', 'not'), 260),\n",
              " (('the', 'total'), 260),\n",
              " (('.', 'L'), 258),\n",
              " (('of', 'Japan'), 257),\n",
              " (('declined', 'to'), 257),\n",
              " (('two', 'cts'), 257),\n",
              " (('of', '1986'), 256),\n",
              " (('for', 'an'), 255),\n",
              " (('should', 'be'), 255),\n",
              " (('QTR', 'LOSS'), 254),\n",
              " ((',', 'according'), 253),\n",
              " (('money', 'market'), 252),\n",
              " (('25', '.'), 252),\n",
              " (('the', 'agreement'), 252),\n",
              " ((',', 'has'), 251),\n",
              " (('European', 'Community'), 251),\n",
              " (('In', 'the'), 251),\n",
              " ((',', 'from'), 250),\n",
              " (('the', 'merger'), 249),\n",
              " ((':', '1986'), 249),\n",
              " (('to', '2'), 248),\n",
              " (('22', '.'), 248),\n",
              " (('on', 'March'), 247),\n",
              " (('month', '.'), 247),\n",
              " (('Agriculture', 'Department'), 247),\n",
              " (('billion', '.'), 246),\n",
              " (('today', '.'), 246),\n",
              " (('to', 'have'), 246),\n",
              " (('discontinued', 'operations'), 246),\n",
              " (('21', '.'), 244),\n",
              " (('.', '&'), 244),\n",
              " (('mln', 'barrels'), 244),\n",
              " (('20', 'pct'), 243),\n",
              " (('of', '2'), 242),\n",
              " (('loss', '2'), 242),\n",
              " (('prices', ','), 242),\n",
              " (('share', 'in'), 241),\n",
              " (('the', 'Fed'), 241),\n",
              " (('told', 'a'), 241),\n",
              " (('be', 'a'), 241),\n",
              " ((',\"', 'the'), 240),\n",
              " (('10', 'cts'), 239),\n",
              " (('24', '.'), 238),\n",
              " (('between', 'the'), 237),\n",
              " (('stock', 'split'), 237),\n",
              " (('was', 'a'), 236),\n",
              " (('one', '-'), 236),\n",
              " (('tonnes', 'in'), 235),\n",
              " (('27', '.'), 235),\n",
              " (('a', 'major'), 235),\n",
              " (('into', 'the'), 234),\n",
              " (('the', 'economy'), 233),\n",
              " (('net', 'loss'), 233),\n",
              " ((',', 'down'), 233),\n",
              " (('2ND', 'QTR'), 233),\n",
              " (('Federal', 'Reserve'), 232),\n",
              " (('five', 'cts'), 232),\n",
              " (('when', 'the'), 232),\n",
              " (('>', 'and'), 231),\n",
              " (('week', ','), 231),\n",
              " (('30', '.'), 230),\n",
              " (('Inc', '.'), 230),\n",
              " (('.', 'Under'), 230),\n",
              " (('five', 'pct'), 229),\n",
              " (('expects', 'to'), 228),\n",
              " (('before', 'the'), 228),\n",
              " (('in', 'cash'), 228),\n",
              " (('qtr', 'and'), 228),\n",
              " (('the', 'last'), 227),\n",
              " (('-', 'tax'), 227),\n",
              " (('the', 'Gulf'), 226),\n",
              " (('1987', '/'), 226),\n",
              " (('dlrs', 'on'), 226),\n",
              " (('JAN', '31'), 226),\n",
              " (('the', 'week'), 225),\n",
              " (('in', 'March'), 225),\n",
              " (('/', '88'), 225),\n",
              " (('of', 'record'), 225),\n",
              " (('of', 'this'), 224),\n",
              " (('six', 'cts'), 224),\n",
              " (('vs', '6'), 223),\n",
              " (('of', '3'), 222),\n",
              " (('Ltd', '>'), 222),\n",
              " (('three', '-'), 222),\n",
              " (('and', 'its'), 222),\n",
              " (('four', 'cts'), 222),\n",
              " (('They', 'said'), 221),\n",
              " (('traders', 'said'), 221),\n",
              " (('the', 'group'), 221),\n",
              " (('Pay', 'April'), 221),\n",
              " (('Ltd', '&'), 220),\n",
              " (('QTR', 'JAN'), 220),\n",
              " (('Soviet', 'Union'), 220),\n",
              " (('23', '.'), 219),\n",
              " (('to', 'its'), 219),\n",
              " (('year', 'and'), 219),\n",
              " (('owned', 'by'), 218),\n",
              " (('-', 'owned'), 218),\n",
              " (('trade', 'deficit'), 217),\n",
              " (('2', 'billion'), 217),\n",
              " (('but', 'the'), 216),\n",
              " (('pct', 'stake'), 216),\n",
              " (('today', ','), 216),\n",
              " (('mln', 'bpd'), 216),\n",
              " (('.', 'For'), 216),\n",
              " (('there', 'is'), 216),\n",
              " (('may', 'be'), 216),\n",
              " (('the', 'price'), 215),\n",
              " (('0', 'pct'), 215),\n",
              " (('three', 'cts'), 215),\n",
              " (('the', 'Soviet'), 215),\n",
              " (('pct', 'and'), 214),\n",
              " (('the', 'fourth'), 214),\n",
              " (('January', '.'), 214),\n",
              " (('down', 'from'), 212),\n",
              " (('stock', ','), 212),\n",
              " (('vs', '7'), 211),\n",
              " (('an', 'agreement'), 211),\n",
              " (('and', 'to'), 210),\n",
              " (('CO', '&'), 210),\n",
              " ((',', 'however'), 209),\n",
              " (('a', 'result'), 208),\n",
              " (('a', 'total'), 208),\n",
              " (('than', 'the'), 208),\n",
              " (('to', 'increase'), 208),\n",
              " (('MLN', 'DLRS'), 208),\n",
              " (('market', '.'), 208),\n",
              " (('which', 'has'), 208),\n",
              " (('told', 'reporters'), 208),\n",
              " (('from', '1'), 207),\n",
              " (('billion', 'dlr'), 207),\n",
              " (('to', 'an'), 207),\n",
              " (('offer', 'for'), 207),\n",
              " (('quarter', '.'), 207),\n",
              " (('the', 'yen'), 206),\n",
              " (('since', 'the'), 206),\n",
              " (('February', ','), 206),\n",
              " (('the', 'Federal'), 205),\n",
              " (('added', 'that'), 205),\n",
              " (('Record', 'April'), 205),\n",
              " (('31', 'NET'), 205),\n",
              " (('and', 'will'), 205),\n",
              " (('to', 'reduce'), 204),\n",
              " (('growth', 'in'), 204),\n",
              " (('the', 'European'), 204),\n",
              " (('trade', 'surplus'), 203),\n",
              " (('.', 'Last'), 203),\n",
              " (('half', 'of'), 203),\n",
              " (('less', 'than'), 203),\n",
              " (('exchange', 'rate'), 203),\n",
              " (('vs', '8'), 203),\n",
              " (('>', '2ND'), 203),\n",
              " (('long', '-'), 202),\n",
              " (('quarter', 'of'), 202),\n",
              " (('of', 'an'), 202),\n",
              " (('amount', 'of'), 201),\n",
              " (('100', ','), 201),\n",
              " (('.', 'On'), 201),\n",
              " (('28', '.'), 201),\n",
              " (('as', 'well'), 200),\n",
              " (('.', 'And'), 199),\n",
              " (('Department', 'said'), 199),\n",
              " (('however', ','), 199),\n",
              " (('.', 'As'), 199),\n",
              " (('1986', 'net'), 199),\n",
              " (('10', ','), 199),\n",
              " (('on', 'April'), 198),\n",
              " (('market', ','), 198),\n",
              " (('the', '1986'), 198),\n",
              " (('Co', 'said'), 198),\n",
              " (('vs', '10'), 198),\n",
              " (('the', 'trade'), 197),\n",
              " (('the', 'central'), 196),\n",
              " (('of', 'England'), 196),\n",
              " (('to', 'help'), 196),\n",
              " (('is', 'to'), 195),\n",
              " (('of', '1987'), 195),\n",
              " (('L', '>'), 195),\n",
              " (('department', 'said'), 194),\n",
              " (('.', 'Analysts'), 194),\n",
              " (('government', \"'\"), 194),\n",
              " (('have', 'a'), 194),\n",
              " (('7', '-'), 193),\n",
              " (('billion', 'stg'), 193),\n",
              " (('.', '10'), 193),\n",
              " ((',', '600'), 193),\n",
              " (('the', 'stock'), 193),\n",
              " (('25', 'pct'), 192),\n",
              " (('.\"', 'The'), 192),\n",
              " (('subsidiary', 'of'), 191),\n",
              " (('and', '1'), 191),\n",
              " (('profit', '1'), 191),\n",
              " (('operations', '.'), 191),\n",
              " (('1', 'billion'), 190),\n",
              " (('the', 'Japanese'), 190),\n",
              " (('country', \"'\"), 190),\n",
              " (('share', 'of'), 190),\n",
              " (('vs', '9'), 190),\n",
              " (('.', '20'), 190),\n",
              " (('a', 'record'), 190),\n",
              " (('total', 'of'), 190),\n",
              " (('and', 'gas'), 190),\n",
              " (('Record', 'March'), 190),\n",
              " (('out', 'of'), 189),\n",
              " (('week', '.'), 189),\n",
              " (('years', '.'), 189),\n",
              " (('year', 'to'), 189),\n",
              " (('will', 'have'), 188),\n",
              " (('Plc', '&'), 188),\n",
              " (('vs', '11'), 188),\n",
              " (('.', 'At'), 188),\n",
              " (('to', 'cut'), 187),\n",
              " (('.', '30'), 187),\n",
              " (('mln', ','), 186),\n",
              " (('decline', 'in'), 186),\n",
              " (('prices', '.'), 186),\n",
              " (('end', '-'), 186),\n",
              " (('pre', '-'), 186),\n",
              " (('fall', 'in'), 185),\n",
              " (('result', 'of'), 185),\n",
              " (('terms', 'of'), 185),\n",
              " (('exchange', 'rates'), 185),\n",
              " (('-', '7'), 185),\n",
              " (('this', 'month'), 184),\n",
              " (('26', '.'), 184),\n",
              " (('.\"', 'He'), 184),\n",
              " (('assets', 'of'), 184),\n",
              " ((',', 'effective'), 184),\n",
              " (('eight', 'cts'), 184),\n",
              " (('-', 'day'), 183),\n",
              " (('share', 'for'), 183),\n",
              " (('compared', 'to'), 183),\n",
              " (('not', 'to'), 182),\n",
              " (('fiscal', 'year'), 182),\n",
              " ((',', 'told'), 182),\n",
              " (('are', 'not'), 182),\n",
              " (('well', 'as'), 182),\n",
              " (('TO', 'SELL'), 182),\n",
              " (('was', 'the'), 182),\n",
              " (('month', ','), 181),\n",
              " (('ago', '.'), 181),\n",
              " ((',', 'adding'), 181),\n",
              " (('the', 'proposed'), 181),\n",
              " (('000', 'NOTE'), 181),\n",
              " ((',', 'dealers'), 180),\n",
              " ((',', 'analysts'), 180),\n",
              " (('.', '35'), 180),\n",
              " (('the', 'third'), 180),\n",
              " (('pct', 'rise'), 179),\n",
              " (('Reuters', '.'), 179),\n",
              " (('a', 'further'), 179),\n",
              " (('TO', 'BUY'), 179),\n",
              " (('1986', 'and'), 178),\n",
              " (('drop', 'in'), 178),\n",
              " (('a', 'loss'), 178),\n",
              " (('all', 'of'), 178),\n",
              " (('January', 'and'), 178),\n",
              " (('tonnes', '.'), 177),\n",
              " (('But', 'the'), 177),\n",
              " (('the', 'official'), 177),\n",
              " ((',', 'after'), 177),\n",
              " (('will', 'continue'), 177),\n",
              " (('years', ','), 176),\n",
              " (('vs', '12'), 176),\n",
              " (('current', 'account'), 176),\n",
              " (('natural', 'gas'), 175),\n",
              " (('cost', 'of'), 175),\n",
              " (('don', \"'\"), 175),\n",
              " (('500', ','), 175),\n",
              " (('meeting', 'of'), 175),\n",
              " ((',', '700'), 175),\n",
              " (('earlier', '.'), 175),\n",
              " (('agreement', ','), 174),\n",
              " (('months', 'of'), 174),\n",
              " (('said', 'there'), 174),\n",
              " (('for', 'its'), 174),\n",
              " (('operations', 'of'), 174),\n",
              " (('.', 'F'), 174),\n",
              " ((',', 'respectively'), 174),\n",
              " (('statement', '.'), 173),\n",
              " (('agreement', 'to'), 173),\n",
              " (('>', 'SEES'), 173),\n",
              " ((',', '300'), 173),\n",
              " (('Industries', 'Inc'), 173),\n",
              " (('.', '15'), 173),\n",
              " (('net', 'income'), 173),\n",
              " (('Net', '2'), 173),\n",
              " ((',', '200'), 173),\n",
              " (('seven', 'cts'), 173),\n",
              " (('mln', 'Note'), 173),\n",
              " (('Co', ','), 173),\n",
              " (('.', '80'), 172),\n",
              " (('Saudi', 'Arabia'), 172),\n",
              " (('the', 'International'), 172),\n",
              " (('.', '60'), 172),\n",
              " (('15', 'cts'), 172),\n",
              " (('do', 'not'), 171),\n",
              " (('3', 'billion'), 171),\n",
              " (('8', 'billion'), 171),\n",
              " (('20', 'cts'), 171),\n",
              " (('from', 'discontinued'), 171),\n",
              " (('level', 'of'), 170),\n",
              " (('and', 'is'), 170),\n",
              " (('in', 'quarter'), 170),\n",
              " (('short', '-'), 169),\n",
              " (('6', 'billion'), 169),\n",
              " (('the', 'state'), 169),\n",
              " (('has', 'not'), 169),\n",
              " (('quarter', ','), 169),\n",
              " (('comment', 'on'), 169),\n",
              " (('money', 'supply'), 169),\n",
              " (('for', 'all'), 169),\n",
              " (('one', 'ct'), 169),\n",
              " (('NET', 'Oper'), 169),\n",
              " (('nine', 'mths'), 169),\n",
              " (('Japan', ','), 168),\n",
              " (('In', 'addition'), 168),\n",
              " (('000', 'shares'), 168),\n",
              " (('to', '3'), 168),\n",
              " (('Under', 'the'), 168),\n",
              " ((',', 'of'), 167),\n",
              " ((',', 'traders'), 167),\n",
              " (('the', 'Bundesbank'), 167),\n",
              " (('to', 'shareholders'), 167),\n",
              " (('.', 'If'), 167),\n",
              " (('nine', 'cts'), 167),\n",
              " (('sell', 'its'), 167),\n",
              " (('the', 'department'), 167),\n",
              " (('in', 'April'), 166),\n",
              " (('April', '30'), 166),\n",
              " (('that', 'a'), 166),\n",
              " (('7', 'billion'), 165),\n",
              " (('.', 'Some'), 165),\n",
              " (('which', 'was'), 165),\n",
              " (('Inc', '>'), 165),\n",
              " (('Revs', '1'), 165),\n",
              " (('response', 'to'), 164),\n",
              " (('of', 'oil'), 164),\n",
              " (('there', 'was'), 164),\n",
              " (('QTR', 'FEB'), 164),\n",
              " ((',', '800'), 164),\n",
              " (('to', 'meet'), 163),\n",
              " (('to', 'rise'), 163),\n",
              " (('29', '.'), 163),\n",
              " (('a', 'letter'), 163),\n",
              " (('third', 'quarter'), 163),\n",
              " (('is', 'likely'), 162),\n",
              " (('\"', 'There'), 162),\n",
              " ((',', '100'), 162),\n",
              " (('control', 'of'), 162),\n",
              " (('one', 'mln'), 162),\n",
              " (('nil', 'nil'), 162),\n",
              " (('an', 'average'), 161),\n",
              " (('A', '.'), 161),\n",
              " (('April', '15'), 161),\n",
              " (('not', 'disclosed'), 161),\n",
              " (('billion', 'francs'), 160),\n",
              " (('the', 'board'), 160),\n",
              " (('.', '05'), 160),\n",
              " (('February', '1986'), 160),\n",
              " ((',', 'vs'), 160),\n",
              " (('9', 'billion'), 159),\n",
              " (('Ltd', ','), 159),\n",
              " (('.', 'There'), 159),\n",
              " (('the', 'transaction'), 159),\n",
              " (('FEB', '28'), 159),\n",
              " (('seasonally', 'adjusted'), 159),\n",
              " (('noted', 'that'), 158),\n",
              " (('for', 'about'), 158),\n",
              " (('revenues', 'of'), 158),\n",
              " ((',', '400'), 158),\n",
              " (('able', 'to'), 157),\n",
              " (('South', 'Korea'), 157),\n",
              " (('.', 'Dlrs'), 157),\n",
              " (('are', 'expected'), 157),\n",
              " (('stg', 'vs'), 157),\n",
              " (('the', 'money'), 157),\n",
              " (('plan', 'to'), 157),\n",
              " (('is', 'subject'), 157),\n",
              " (('12', 'cts'), 157),\n",
              " ((',', 'would'), 156),\n",
              " (('G', '-'), 156),\n",
              " (('completed', 'the'), 156),\n",
              " (('interest', 'rate'), 156),\n",
              " (('S', '.,'), 155),\n",
              " (('It', 'is'), 155),\n",
              " (('stg', 'in'), 155),\n",
              " (('two', 'pct'), 155),\n",
              " (('31', 'Shr'), 155),\n",
              " (('the', 'Paris'), 155),\n",
              " (('.', '1986'), 155),\n",
              " (('per', 'tonne'), 155),\n",
              " (('.', 'Trade'), 154),\n",
              " (('to', 'pay'), 154),\n",
              " (('4', 'billion'), 154),\n",
              " (('34', '.'), 154),\n",
              " (('at', 'its'), 154),\n",
              " (('a', 'two'), 153),\n",
              " (('that', 'would'), 153),\n",
              " (('from', 'its'), 153),\n",
              " (('said', '\"'), 153),\n",
              " (('vs', '13'), 153),\n",
              " (('disclosed', '.'), 153),\n",
              " (('E', '.'), 153),\n",
              " (('The', 'government'), 152),\n",
              " (('s', '&'), 152),\n",
              " (('previously', 'announced'), 152),\n",
              " (('president', 'of'), 152),\n",
              " (('holders', 'of'), 152),\n",
              " (('16', 'pct'), 151),\n",
              " ((',', 'when'), 151),\n",
              " (('it', 'agreed'), 151),\n",
              " (('shr', 'loss'), 151),\n",
              " (('stg', '.'), 150),\n",
              " (('has', 'a'), 150),\n",
              " (('/', '16'), 150),\n",
              " (('he', 'was'), 150),\n",
              " (('15', 'pct'), 150),\n",
              " (('000', 'Avg'), 150),\n",
              " (('200', ','), 150),\n",
              " (('YEAR', 'NET'), 150),\n",
              " (('while', 'the'), 149),\n",
              " (('30', 'pct'), 149),\n",
              " (('rates', '.'), 149),\n",
              " (('.', 'Earlier'), 149),\n",
              " (('year', 'net'), 149),\n",
              " (('s', 'stock'), 149),\n",
              " (('at', '1'), 149),\n",
              " (('mid', '-'), 148),\n",
              " (('a', 'meeting'), 148),\n",
              " (('I', 'think'), 148),\n",
              " (('PCT', 'IN'), 148),\n",
              " (('the', 'National'), 148),\n",
              " (('000', 'dlr'), 148),\n",
              " (('Corp', '.'), 148),\n",
              " (('Group', 'Inc'), 148),\n",
              " (('has', 'agreed'), 148),\n",
              " (('000', 'bpd'), 148),\n",
              " ((',', 'against'), 147),\n",
              " (('of', '&'), 147),\n",
              " (('an', 'increase'), 147),\n",
              " (('to', '7'), 147),\n",
              " (('-', 'one'), 147),\n",
              " (('with', 'its'), 147),\n",
              " (('to', 'continue'), 147),\n",
              " (('dollar', ','), 147),\n",
              " (('to', 'raise'), 147),\n",
              " (('exports', 'to'), 146),\n",
              " (('change', 'in'), 146),\n",
              " (('BANK', 'OF'), 146),\n",
              " (('at', 'about'), 146),\n",
              " (('000', 'Year'), 146),\n",
              " (('000', ','), 146),\n",
              " (('1', '-'), 146),\n",
              " (('which', 'would'), 146),\n",
              " (('-', 'ago'), 146),\n",
              " (('is', 'also'), 145),\n",
              " (('need', 'to'), 145),\n",
              " (('in', 'which'), 145),\n",
              " (('prices', 'and'), 145),\n",
              " (('vs', '15'), 145),\n",
              " (('37', '.'), 145),\n",
              " (('the', 'report'), 145),\n",
              " (('to', 'keep'), 144),\n",
              " (('per', 'day'), 144),\n",
              " (('300', ','), 144),\n",
              " (('F', '.'), 144),\n",
              " (('in', '1988'), 144),\n",
              " (('10', 'mln'), 144),\n",
              " (('Hong', 'Kong'), 143),\n",
              " (('in', 'New'), 143),\n",
              " (('because', 'the'), 143),\n",
              " (('they', 'are'), 143),\n",
              " (('.', 'N'), 143),\n",
              " (('three', 'pct'), 143),\n",
              " (('6', '-'), 143),\n",
              " (('17', 'cts'), 143),\n",
              " (('25', 'cts'), 143),\n",
              " (('had', 'a'), 142),\n",
              " (('It', 'also'), 142),\n",
              " ((',', 'although'), 142),\n",
              " (('MONEY', 'MARKET'), 142),\n",
              " (('C', '.'), 142),\n",
              " (('week', \"'\"), 142),\n",
              " (('group', 'of'), 142),\n",
              " (('offer', '.'), 142),\n",
              " (('its', 'stake'), 142),\n",
              " (('Commission', ','), 142),\n",
              " (('April', '1'), 142),\n",
              " (('ended', 'March'), 141),\n",
              " (('today', \"'\"), 141),\n",
              " (('80', 'pct'), 141),\n",
              " (('a', '1'), 141),\n",
              " (('chief', 'executive'), 141),\n",
              " (('the', 'purchase'), 141),\n",
              " (('-', 'month'), 141),\n",
              " (('going', 'to'), 141),\n",
              " (('to', '10'), 141),\n",
              " (('03', '/'), 141),\n",
              " (('32', '.'), 140),\n",
              " (('when', 'it'), 140),\n",
              " (('talks', 'with'), 140),\n",
              " (('.', '40'), 140),\n",
              " (('14', 'cts'), 140),\n",
              " (('net', 'excludes'), 140),\n",
              " (('Finance', 'Minister'), 140),\n",
              " (('led', 'by'), 140),\n",
              " (('offer', 'to'), 140),\n",
              " (('preferred', 'stock'), 140),\n",
              " (('04', '/'), 140),\n",
              " (('87', '03'), 140),\n",
              " (('chairman', 'of'), 139),\n",
              " (('is', 'still'), 139),\n",
              " (('so', 'far'), 139),\n",
              " (('mln', 'marks'), 139),\n",
              " (('signed', 'a'), 139),\n",
              " (('pct', 'interest'), 139),\n",
              " (('on', '-'), 139),\n",
              " (('spokesman', 'for'), 138),\n",
              " (('\"', 'This'), 138),\n",
              " (('pct', 'increase'), 138),\n",
              " (('it', 'to'), 138),\n",
              " (('rates', ','), 138),\n",
              " ...]"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fdbigram.most_common()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov8TZjx7Sc2-"
      },
      "source": [
        "Collocations are expressions of multiple words that commonly co-occur.\n",
        "\n",
        ">Finding collocations requires first calculating the frequencies of words and\n",
        "their appearance in the context of other words. Often the collection of words\n",
        "will then requiring filtering to only retain useful content terms. Each ngram\n",
        "of words may then be scored according to some association measure, in order\n",
        "to determine the relative likelihood of each ngram being a collocation. (Quoted from [here](http://www.nltk.org/_modules/nltk/collocations.html))\n",
        "\n",
        "For example, to extract bigram collocations, we can firstly extract bigrams then get the commonly co-occurring ones by ranking the bigrams by some measures. A commonly used measure is [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PMI). The following code will find the best 100 bigrams using the PMI scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hhf0E8emSc2_",
        "outputId": "7e0c33cf-e557-42c2-b278-ea18770582d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('...............', 'MAX'),\n",
              " ('11895', '2289'),\n",
              " ('11TH', 'LICENCE'),\n",
              " ('12664', '11895'),\n",
              " ('2867', '2073'),\n",
              " ('3198', '2867'),\n",
              " ('35000', '32876'),\n",
              " ('6000', 'QMS'),\n",
              " ('6834', '2292'),\n",
              " ('7289', '6834'),\n",
              " ('8440', '1409'),\n",
              " ('9009', '8440'),\n",
              " (';(', 'MWW'),\n",
              " ('ACUIRES', 'SUPERMAC'),\n",
              " ('ADELAIDE', 'STEAMSHIP'),\n",
              " ('ADVENTURE', 'Americanture'),\n",
              " ('ARMISTICE', 'ELUDES'),\n",
              " ('ARTICLES', 'HALLMARKED'),\n",
              " ('ARTILLERY', 'SHELLS'),\n",
              " ('ASPEN', 'RIBBONS'),\n",
              " ('ASSUMPTIONS', 'FLAWED'),\n",
              " ('AUTOCLAVE', 'ENGINEERS'),\n",
              " ('AVIAN', 'INFLUENZA'),\n",
              " ('Acquired', 'Immune'),\n",
              " ('Addis', 'Ababa'),\n",
              " ('Addressograph', 'Farrington'),\n",
              " ('Adventist', 'Church'),\n",
              " ('Afobaka', 'dam'),\n",
              " ('Aggregate', 'judgments'),\n",
              " ('Aghia', 'Efthymia'),\n",
              " ('Ahmad', 'Sarji'),\n",
              " ('Alfieri', 'Maserati'),\n",
              " ('Allgemeine', 'Hypothekenbank'),\n",
              " ('Almy', 'Hafild'),\n",
              " ('Alois', 'Schwietert'),\n",
              " ('Alsthom', 'Inudstrial'),\n",
              " ('Anders', 'Carlberg'),\n",
              " ('Anno', '1720'),\n",
              " ('Anwar', 'Sadat'),\n",
              " ('Aproveitamentos', 'Florestais'),\n",
              " ('Arie', 'Guldemond'),\n",
              " ('Artistic', 'Greetings'),\n",
              " ('Attilio', 'Petrocelli'),\n",
              " ('Augusto', 'Pinochet'),\n",
              " ('Ausimont', 'Compo'),\n",
              " ('BALANCED', 'PHYSICALS'),\n",
              " ('BANCO', 'SANTANDER'),\n",
              " ('BEACONS', 'Huge'),\n",
              " ('BED', 'BOILER'),\n",
              " ('BERGEN', 'BRUNSWIG')]"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(reuters.words())\n",
        "finder.nbest(bigram_measures.pmi, 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JIY_cSjSc2_"
      },
      "source": [
        "The `collocations` module implements a number of measures to score collocations or other associations.\n",
        "They include Student's t test, Chi-Square, likelihood ratios, PMI and so on.\n",
        "Here we used PMI scores for finding bigrams.\n",
        "Please read [2] for a detailed tutorial on finding collocations with NLTK.\n",
        "If you would like to know more about collocations, please refer to [3]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ypg3eU3Sc2_"
      },
      "source": [
        "### 2.5. Conclusion\n",
        "This section has show you how to\n",
        "* generate vocabulary be further exploring the tokenized text with some simple statistics.\n",
        "* convert unstructured text to structured form using the bag-of-words model\n",
        "* compute TF-IDF\n",
        "* extract words in specific lexical categories, n-grams and collocations.\n",
        "\n",
        "In summary\n",
        "- In section 2.1, we have shown you how to count the vocabulary by selecting tokens of interest.\n",
        "- In section 2.2, we have demonstrated how to count the vocabulary and generate the sparse format data.\n",
        "- In section 2.3, we have shown you how to save pre-processed text to a file.\n",
        "- In section 2.4, we have demonstrated how to extract other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reeL39KjSc2_"
      },
      "source": [
        "### 2.6. Reference Reading Materials\n",
        "1. \"[Categorizaing and Tagging Words](http://www.nltk.org/book/ch05.html)\",\n",
        "Chapter 5 of \"Natural Language Processing with Python\".\n",
        "2. \"[Collocations](http://www.nltk.org/howto/collocations.html)\": An NTLK tutorial on how to extract collocations  .\n",
        "3. \"[Collocations](http://nlp.stanford.edu/fsnlp/promo/colloc.pdf)\": An introduction to collocation by Manning and Schutze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVpdeX15Sc2_"
      },
      "source": [
        "## **3. XML Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpj6TnOYSc2_"
      },
      "source": [
        "We have shown you how to process text data in the previous sections. In this section, we are going to show you how to process a set of patents documents stored in XML format and generating the sparse representations for those patents. The final output file should be exactly the same as the one stored in \"patents.txt\".\n",
        "\n",
        "In order to finish this task, you should\n",
        "1. Exatract the abstract and claims for each patent from its xml file. Use Beautiful soup\n",
        "2. Tokenise the patents\n",
        "3. Generate 100 bigram collocations\n",
        "4. Re-tokenize the patents with those bigram collocations\n",
        "5. Generate the TF-IDF vectors for those re-tokenized patents\n",
        "6. save the vectors in the form shown in \"patents.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aoPi0S2Sc2_"
      },
      "source": [
        "### 3.1 Import libraries\n",
        "Here we will focus on using the existing packages as possible as we can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk-JqbdMSc2_"
      },
      "source": [
        "`bs4` is a module that helps to extract information from an XML file. The following code shows how to use the `bs4` module to load the XML file.\n",
        "`BeautifulSoup` is a class that helps to parse the XML file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "mfYb82eJSc2_"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bsoup\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from nltk.collocations import *\n",
        "from itertools import chain\n",
        "import itertools\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import MWETokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB8eL8PMSc3A"
      },
      "source": [
        "### 3.2 Exatract Patent's abstract and Claims\n",
        "\n",
        "The first task is to parse each patent stored in the \"xml_files\" folder. The information to be extracted includes\n",
        "1. patent document number (doc-number) stored in \"publication-reference\"\n",
        "2. patent's abstract\n",
        "3. patent's claims\n",
        "\n",
        "Hint: you can use a dictionary to save patents, where the key is the doc-number, the value is a long string contains both abstracts and all claims."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "e1j9UobdSc3B"
      },
      "outputs": [],
      "source": [
        "def parsing(t):\n",
        "\n",
        "    xmlSoup = bsoup(t,\"lxml-xml\")\n",
        "\n",
        "    pid = xmlSoup.find(\"publication-reference\").find('doc-number').string\n",
        "\n",
        "    text = \"\"\n",
        "\n",
        "    #Extract text in \"abstract\"\n",
        "    abt = xmlSoup.find('abstract')\n",
        "    for p in abt.findAll('p'):\n",
        "        text = text + p.text + \" \"\n",
        "\n",
        "    #Extract Claims\n",
        "    for tag in xmlSoup.find_all('claim-text'):\n",
        "        text = text + tag.text\n",
        "\n",
        "    return (pid, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoG6MUMNSc3B",
        "outputId": "7b1dfd67-72db-4ac7-c768-6cbaeae0332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.3.1)\n"
          ]
        }
      ],
      "source": [
        "# !pip install lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHimTG3tSc3B",
        "outputId": "96058b7c-162e-4061-ba7d-c40af359d75e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/Icon_\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891177-20110222.XML\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-83-53f9a3994fd9>:11: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  for p in abt.findAll('p'):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891027-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891221-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891266-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891262-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891140-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891121-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891165-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891057-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891271-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891071-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891166-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891076-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891084-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891097-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891206-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891148-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891104-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891019-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891173-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891070-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891268-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891248-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891098-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891231-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891210-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891141-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891044-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891273-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891083-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891037-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891163-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891055-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891162-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891029-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891087-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891058-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891118-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891252-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891107-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891178-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891039-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891117-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891067-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891020-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891032-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891188-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891199-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891078-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891158-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891053-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891139-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891021-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891116-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891234-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891018-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891272-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891056-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891255-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891198-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891033-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891160-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891169-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891123-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891146-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891063-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891170-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891030-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891239-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891082-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891025-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891115-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891242-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891026-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891152-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891036-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891114-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891159-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891192-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891183-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891167-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891259-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891260-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891060-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891269-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891201-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891136-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891161-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891111-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891034-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891086-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891219-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891249-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891023-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891203-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891129-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891038-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891059-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891041-20110222.XML\n",
            "/content/drive/Shareddrives/FIT5196_S1_2025/week5/xml_files/US07891133-20110222.XML\n"
          ]
        }
      ],
      "source": [
        "xml_file_path = \"/content/drive/Shareddrives/FIT5196_S2_2025/week5/xml_files\"\n",
        "patents_raw = {}\n",
        "# read all the XML files in the xml_file_path\n",
        "for xfile in os.listdir(xml_file_path):\n",
        "    xfile = os.path.join(xml_file_path, xfile)\n",
        "    print(xfile)\n",
        "    if os.path.isfile(xfile) and xfile.endswith('.XML'):\n",
        "        (pid, text) = parsing(open(xfile))\n",
        "        patents_raw[pid] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4_QD4B8Sc3C",
        "outputId": "54993cb5-86de-4890-9fed-d27811086be2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['07891177', '07891027', '07891221', '07891266', '07891262', '07891140', '07891121', '07891165', '07891057', '07891271', '07891071', '07891166', '07891076', '07891084', '07891097', '07891206', '07891148', '07891104', '07891019', '07891173', '07891070', '07891268', '07891248', '07891098', '07891231', '07891210', '07891141', '07891044', '07891273', '07891083', '07891037', '07891163', '07891055', '07891162', '07891029', '07891087', '07891058', '07891118', '07891252', '07891107', '07891178', '07891039', '07891117', '07891067', '07891020', '07891032', '07891188', '07891199', '07891078', '07891158', '07891053', '07891139', '07891021', '07891116', '07891234', '07891018', '07891272', '07891056', '07891255', '07891198', '07891033', '07891160', '07891169', '07891123', '07891146', '07891063', '07891170', '07891030', '07891239', '07891082', '07891025', '07891115', '07891242', '07891026', '07891152', '07891036', '07891114', '07891159', '07891192', '07891183', '07891167', '07891259', '07891260', '07891060', '07891269', '07891201', '07891136', '07891161', '07891111', '07891034', '07891086', '07891219', '07891249', '07891023', '07891203', '07891129', '07891038', '07891059', '07891041', '07891133'])"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "patents_raw.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Me3xduBjSc3C",
        "outputId": "1e25747d-049b-4441-8a7c-bd99fff3f7f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A seal assembly for a drain vent is provided to direct the pressure exerted by a plunger to the face of a drain clog rather than escaping through the drain vent. The seal assembly of the invention comprises a tubular body having a first closed end and a second open end to define a substantially rigid a cup-like structure or cavity. A sealing member concentric with a longitudinal axis of the tubular body is attached to the second open end of the tubular body to form a seal against a surface containing the drain vent. A handle is attached to the closed end of the cup-like structure and extends away from the closed first end of the tubular body to provide a mechanism for the user to hold the sealing member against the surface containing the drain vent during the plunging process. 1. A seal for a drain vent, comprising: a tubular body having a first closed end and a second open end for forming a rigid cavity within said tubular body; a sealing member concentric with a longitudinal axis of said tubular body and attached to said second open end of said tubular body for forming a seal against a surface containing the drain vent; a handle and a recess extending into an exterior surface of said first closed end of said tubular body for receiving said handle; said handle and said recess being located peripherally with respect to said tubular body and spaced from said longitudinal axis of said tubular body, and said handle attached to said tubular body via said recess and extending away from said closed first end of said tubular body for holding said sealing member against said surface containing the drain vent.2. The seal for a drain vent as defined in claim 1, wherein said tubular body includes a modulus of elasticity greater than that for said sealing member.3. The seal for a drain vent as defined in claim 1, wherein said sealing member includes a modulus of elasticity lesser than that for said tubular body.4. The seal for a drain vent as defined in claim 1, wherein said tubular body includes a polygonal-shaped tubular body.5. The seal for a drain vent as defined in claim 1, wherein said tubular body includes a cylindrical-shaped tubular body.6. The seal for a drain vent as defined in claim 1, wherein said tubular body is comprised of thermoplastic material.7. The seal for a drain vent as defined in claim 1, wherein said tubular body is comprised of a thermosetting material.8. The seal for a drain vent as defined in claim 1, wherein said tubular body is comprised of a metallic material.9. The seal for a drain vent as defined in claim 1, wherein said tubular body is comprised of an oriented stand material.10. The seal for a drain vent as defined in claim 1, wherein said tubular body is comprised of a random oriented strand material.11. The seal for a drain vent as defined in claim 1, wherein said tubular body is substantially impermeable to fluids.12. The seal for a drain vent as defined in claim 1, wherein said sealing member is substantially impermeable to fluids.13. The seal for a drain vent as defined in claim 1, wherein said sealing member is substantially non-porous.14. The seal for a drain vent as defined in claim 1, wherein said handle and said recess are parallel to said longitudinal axis of said tubular body.15. The seal for a drain vent as defined in claim 1, wherein said recess includes a threaded interior wall.16. The seal for a drain vent as defined in claim 1, wherein said handle is retained within said recess by a mechanical fastener.17. The seal for a drain vent as defined in claim 1, wherein said handle is retained within said recess by compression of said recess about said handle. '"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "patents_raw['07891027']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1tuPbJQSc3D"
      },
      "source": [
        "### 3.3 Tokenize the patents\n",
        "After finish extract the texts, you now need to tokenize the patents with regular expression tokenizer implemented in NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "-HN3yQwFSc3D"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'[a-zA-Z]{2,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "8PNIgK_ISc3D"
      },
      "outputs": [],
      "source": [
        "def tokenizePatent(pid):\n",
        "    \"\"\"\n",
        "        the tokenization function is used to tokenize each patent.\n",
        "        The one argument is patent_id.\n",
        "        First, normalize the case.\n",
        "        Then, use the regular expression tokenizer to tokenize the patent with the specified id\n",
        "    \"\"\"\n",
        "    raw_patent = patents_raw[pid].lower()\n",
        "    tokenized_patents = tokenizer.tokenize(raw_patent)\n",
        "    return (pid, tokenized_patents) # return a tupel of patent_id and a list of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "paBRStRzSc3D"
      },
      "outputs": [],
      "source": [
        "patents_tokenized = dict(tokenizePatent(pid) for pid in patents_raw.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHaG4oPLSc3D"
      },
      "source": [
        "### 3.4.  Generate the 100 bigram collocations\n",
        "The next task is go generate the bigram collocations, given the tokenized patents.\n",
        "\n",
        "The first step is to concatenate all the tokenized patents using the chain.frome_iterable function. The returned list\n",
        "by the function contains a list of all the words seprated by while space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "TCuHXPH7Sc3D"
      },
      "outputs": [],
      "source": [
        "all_words = list(chain.from_iterable(patents_tokenized.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af-YYu4GSc3E"
      },
      "source": [
        "The second step is to generate the 100 bigram cllocations. The functions you need include\n",
        "* BigramAssocMeasures()\n",
        "* BigramCollocationFinder.from_words()\n",
        "* apply_freq_filter(20)\n",
        "* apply_word_filter(lambda w: len(w) < 3)\n",
        "* nbest(bigram_measures.pmi, 100)\n",
        "\n",
        "Please do not change the parameters given in the last three function. More information about generating collocation with NLTK can be found http://www.nltk.org/howto/collocations.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnxNpgLhSc3E",
        "outputId": "d02f8821-0d2d-4712-b1b6-4ca9ca337261"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('harmonic', 'flex'),\n",
              " ('centrifugally', 'balanced'),\n",
              " ('robotic', 'harmonic'),\n",
              " ('expandable', 'chuck'),\n",
              " ('group', 'consisting'),\n",
              " ('charge', 'consistent'),\n",
              " ('walk', 'behind'),\n",
              " ('improperly', 'swapped'),\n",
              " ('saw', 'resonator'),\n",
              " ('behind', 'mowing'),\n",
              " ('jute', 'fibers'),\n",
              " ('actuator', 'flag'),\n",
              " ('elastic', 'band'),\n",
              " ('drain', 'vent'),\n",
              " ('lead', 'frames'),\n",
              " ('fresh', 'food'),\n",
              " ('high', 'humidity'),\n",
              " ('paper', 'particles'),\n",
              " ('fringe', 'maker'),\n",
              " ('ultrasonic', 'test'),\n",
              " ('foot', 'pedal'),\n",
              " ('elastomeric', 'mat'),\n",
              " ('capacitor', 'devices'),\n",
              " ('loaded', 'bag'),\n",
              " ('hammermilled', 'straw'),\n",
              " ('flash', 'tank'),\n",
              " ('tank', 'receiver'),\n",
              " ('hip', 'joint'),\n",
              " ('does', 'not'),\n",
              " ('duty', 'belt'),\n",
              " ('drier', 'solid'),\n",
              " ('solid', 'phase'),\n",
              " ('removable', 'joining'),\n",
              " ('cooler', 'box'),\n",
              " ('not', 'exceed'),\n",
              " ('cross', 'sectional'),\n",
              " ('case', 'packer'),\n",
              " ('vacuum', 'electronic'),\n",
              " ('driver', 'pulley'),\n",
              " ('mowing', 'machine'),\n",
              " ('fastened', 'together'),\n",
              " ('storage', 'capacity'),\n",
              " ('bus', 'bars'),\n",
              " ('provide', 'circulating'),\n",
              " ('set', 'forth'),\n",
              " ('force', 'translation'),\n",
              " ('relatively', 'drier'),\n",
              " ('lock', 'guard'),\n",
              " ('outwardly', 'protruding'),\n",
              " ('hand', 'crank'),\n",
              " ('well', 'plug'),\n",
              " ('coil', 'wires'),\n",
              " ('threaded', 'stud'),\n",
              " ('condemned', 'condition'),\n",
              " ('ending', 'point'),\n",
              " ('living', 'hinge'),\n",
              " ('magazine', 'well'),\n",
              " ('spaced', 'apart'),\n",
              " ('test', 'probe'),\n",
              " ('incline', 'angle'),\n",
              " ('retainer', 'tab'),\n",
              " ('liquid', 'dielectric'),\n",
              " ('shift', 'drum'),\n",
              " ('wringing', 'tube'),\n",
              " ('flex', 'drive'),\n",
              " ('timing', 'pulley'),\n",
              " ('starting', 'point'),\n",
              " ('relatively', 'wet'),\n",
              " ('shift', 'spindle'),\n",
              " ('bus', 'bar'),\n",
              " ('moisture', 'removal'),\n",
              " ('photoreceptor', 'substrate'),\n",
              " ('cat', 'oxygen'),\n",
              " ('fuel', 'ratio'),\n",
              " ('elongate', 'strength'),\n",
              " ('securing', 'connector'),\n",
              " ('notch', 'cut'),\n",
              " ('time', 'period'),\n",
              " ('directly', 'opposite'),\n",
              " ('both', 'sides'),\n",
              " ('less', 'than'),\n",
              " ('multi', 'sensing'),\n",
              " ('selectively', 'fastened'),\n",
              " ('post', 'cat'),\n",
              " ('catalytic', 'converter'),\n",
              " ('joining', 'means'),\n",
              " ('sectional', 'shape'),\n",
              " ('winding', 'arms'),\n",
              " ('mainspring', 'pulley'),\n",
              " ('clamping', 'face'),\n",
              " ('active', 'state'),\n",
              " ('about', 'percent'),\n",
              " ('sixth', 'strap'),\n",
              " ('shutter', 'panel'),\n",
              " ('composite', 'rods'),\n",
              " ('upright', 'post'),\n",
              " ('food', 'compartment'),\n",
              " ('pivotally', 'coupled'),\n",
              " ('determining', 'whether'),\n",
              " ('arm', 'rests')]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(all_words)\n",
        "bigram_finder.apply_freq_filter(20)\n",
        "bigram_finder.apply_word_filter(lambda w: len(w) < 3)# or w.lower() in ignored_words)\n",
        "top_100_bigrams = bigram_finder.nbest(bigram_measures.pmi, 100) # Top-100 bigrams\n",
        "top_100_bigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e83wUXgFSc3E"
      },
      "source": [
        "### 3.5 Re-tokenize the patents again.\n",
        "\n",
        "Task in Section 4 takenise the patents with only unigrams. Now, we introduce 100 collcations. we need to make sure those collocations are not split into two individual words. The tokenizer that you need is <a href=\"http://www.nltk.org/api/nltk.tokenize.html\">MWEtokenizer</a>.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K20XTqxeSc3E",
        "outputId": "a3fd6f92-bb6c-421d-b55b-abce46bfa0a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3372\n"
          ]
        }
      ],
      "source": [
        "mwetokenizer = MWETokenizer(top_100_bigrams)\n",
        "colloc_patents =  dict((pid, mwetokenizer.tokenize(patent)) for pid,patent in patents_tokenized.items())\n",
        "all_words_colloc = list(chain.from_iterable(colloc_patents.values()))\n",
        "colloc_voc = list(set(all_words_colloc))\n",
        "print(len(colloc_voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_6ua5RESc3E"
      },
      "source": [
        "You can check the difference between th output of MWEtokenizer and RegexpTokenizer by <font size=3>adpating</font> the following code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFRJXUIGSc3E"
      },
      "source": [
        "```python\n",
        "for pid in patents_tokenized.keys():\n",
        "    diff = set(colloc_patents[pid])-set(patents_tokenized[pid])\n",
        "    if len(diff) != 0:\n",
        "        print (pid, diff)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiYJdsjSSc3E"
      },
      "source": [
        "### 3.6 Generate the TF-IDF vectors for all the patents.\n",
        "Please refer to\n",
        "* http://scikit-learn.org/stable/modules/feature_extraction.html\n",
        "* http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "tJXvCYtPSc3E"
      },
      "outputs": [],
      "source": [
        "pids = []\n",
        "patent_words = []\n",
        "for pid, tokens in colloc_patents.items():\n",
        "    pids.append(pid)\n",
        "    txt = ' '.join(tokens)\n",
        "    patent_words.append(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEoM9PkoSc3E",
        "outputId": "a0f34a1c-4296-43dd-8565-5a7ff89a45c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 3372)"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(patent_words)\n",
        "tfidf_vectors.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSCsAmsgSc3F"
      },
      "source": [
        "### 3.7 Save the TF-IDF vector into the specified format\n",
        "Hint: you can use\n",
        "* the <a href=\"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csc_matrix.tocoo.html\">tocoo()</a> function\n",
        "* itertools.zip_longest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "0gTBedDwSc3F"
      },
      "outputs": [],
      "source": [
        "save_file = open(\"patent_student.txt\", 'w')\n",
        "vocab = tfidf_vectorizer.get_feature_names_out()\n",
        "#########please write the missing code below#######\n",
        "cx = tfidf_vectors.tocoo() # return the coordinate representation of a sparse matrix\n",
        "for i,j,v in itertools.zip_longest(cx.row, cx.col, cx.data):\n",
        "  save_file.write(pids[i] + ',' + vocab[j] + ',' + format(v) + '\\n')\n",
        "save_file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
