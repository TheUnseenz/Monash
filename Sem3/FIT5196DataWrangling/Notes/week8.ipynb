{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb8963c",
   "metadata": {},
   "source": [
    "# Week 8: Data Cleansing üßº\n",
    "\n",
    "## 1. What is Data Cleansing?\n",
    "\n",
    "**Data cleansing**, also known as data cleaning, is a core part of data wrangling. It's the process of **detecting and correcting (or removing) corrupt or inaccurate records** from a dataset. In the age of big data, clean data is absolutely essential for effective machine learning models, statistical analysis, and business intelligence tools. The goal is to improve data quality, which leads to more trustworthy insights and decisions.\n",
    "\n",
    "This process is a key component of the **Data Pre-processing** stage in the overall data wrangling workflow.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. The Data Cleansing Process\n",
    "\n",
    "Effective data cleansing follows a structured, iterative process to ensure thoroughness and prevent data loss.\n",
    "\n",
    "\n",
    "The typical steps are:\n",
    "1.  **Data Audit**: Review and diagnose the current state of data quality.\n",
    "2.  **Defining Cleansing Goals**: Set clear, measurable objectives for the cleaning process.\n",
    "3.  **Data Cleansing Plan**: Create a detailed strategy and timeline.\n",
    "4.  **Backup Data**: **Crucially**, create a backup of the original data to prevent accidental loss.\n",
    "5.  **Data Cleansing Operations**: Execute the actual cleaning tasks (e.g., removing duplicates, handling missing values).\n",
    "6.  **Verification**: Check that the cleaning operations were successful and the data meets the defined quality standards.\n",
    "7.  **Documentation & Reporting**: Record the steps taken and the outcomes.\n",
    "8.  **Review**: Analyze the results and the effectiveness of the process.\n",
    "9.  **Implementation of Preventative Measures**: Use insights from the audit to improve data entry processes and prevent future errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. A Closer Look at the Process Steps\n",
    "\n",
    "### Data Audit (The \"What's Wrong?\" Phase)\n",
    "A **data audit** is the critical first step where you perform a comprehensive review of your data to understand its overall health.\n",
    "\n",
    "* **Objectives**:\n",
    "    * **Identify Quality Issues**: Find inaccuracies, inconsistencies, duplicates, and missing values.\n",
    "    * **Assess Completeness**: Check if critical data is missing.\n",
    "    * **Evaluate Consistency**: Ensure data is consistent across different systems.\n",
    "    * **Check Compliance**: Verify that data management practices meet regulatory standards (like GDPR).\n",
    "* **Methods**:\n",
    "    * This can involve a mix of automated scanning using software tools and manual review for more complex issues.\n",
    "    * Popular tools for this include Tableau Prep, Talend, Alteryx, SAS Data Management, and Google Cloud Dataprep.\n",
    "\n",
    "### Defining Goals, Planning, and Backup\n",
    "* **Defining Cleansing Goals** involves understanding the business requirements and setting specific, measurable targets for data quality.\n",
    "* The **Data Cleansing Plan** is a formal document that outlines the strategies, tools, timeline, and monitoring procedures for the project.\n",
    "* **Backing up data** is a non-negotiable safety net. It mitigates the risk of irreversible errors during the cleaning process and ensures operational continuity.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Cleansing Operations\n",
    "\n",
    "These are the hands-on tasks performed to fix data quality problems.\n",
    "\n",
    "| Problem | Cleansing Operation |\n",
    "| :--- | :--- |\n",
    "| Duplicated records | Removing duplicates |\n",
    "| Inaccurate data | Validating and correcting errors |\n",
    "| Inconsistent data | Running consistency checks |\n",
    "| Incomplete data | Filling missing values |\n",
    "| Irrelevant data | Handling outliers |\n",
    "\n",
    "### Deep Dive: Removing Duplicates\n",
    "There are many techniques to find and remove duplicate records:\n",
    "* **Manual Review**: For very small datasets.\n",
    "* **Sorting**: Sort data to bring duplicates next to each other for easier identification.\n",
    "* **Database Queries (SQL)**: Use `GROUP BY` and `HAVING COUNT(*)>1` to identify duplicate records based on specific columns. You can then delete them, often by keeping the record with the minimum ID.\n",
    "* **Hashing Techniques**: A hash function converts a row's data into a unique string (a hash). Duplicate rows will produce the same hash, making them easy to find.\n",
    "* **Scripting**: Use libraries like **Pandas** in Python to easily identify and drop duplicate rows (`df.duplicated()` and `df.drop_duplicates()`).\n",
    "* **Machine Learning**: Advanced techniques like fuzzy matching, record linkage, and text similarity measures can find \"near duplicates\" that simple methods might miss (e.g., \"John Smith\" vs. \"Jon Smith\").\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Handling Missing Data ‚ùì\n",
    "\n",
    "Missing data is a very common problem that can arise from equipment errors, users skipping survey questions, or changes in circumstances.\n",
    "\n",
    "### 5.1. Why Missing Data is a Problem\n",
    "Ignoring or mishandling missing data is risky because:\n",
    "* Most standard statistical methods assume complete data.\n",
    "* It can lead to **biased estimations** (e.g., an incorrect sample mean) and **incorrect inferences**‚Äîa classic \"garbage in, garbage out\" scenario.\n",
    "\n",
    "### 5.2. Missing Data Mechanisms\n",
    "To handle missing data correctly, we first need to understand *why* it's missing. There are three key mechanisms:\n",
    "\n",
    "1.  **Missing Completely at Random (MCAR)**: The probability of a value being missing is completely random and unrelated to any other variable or the missing value itself. This is the ideal but rarest scenario.\n",
    "    * The probability of missingness, $p(B)$, is only dependent on some unknown parameter $\\eta$: $$p(B|Y_{obs}, Y_{miss}) = p(B|\\eta)$$ \n",
    "\n",
    "2.  **Missing at Random (MAR)**: The probability of a value being missing is related to *other observed variables* in the dataset, but not to the missing value itself. For example, men might be less likely to fill out a depression survey, so missingness on the \"depression\" variable is related to the \"gender\" variable.\n",
    "    * The probability of missingness depends only on the observed data, $Y_{obs}$: $$p(B|Y_{obs}, Y_{miss}) = p(B|Y_{obs}, \\eta)$$ \n",
    "\n",
    "3.  **Missing Not at Random (MNAR)**: The probability of a value being missing is related to the value of that variable itself. For example, people with very high incomes might be less likely to disclose their income. This is the most difficult type to handle.\n",
    "\n",
    "### 5.3. Methods for Handling Missing Data: Deletion\n",
    "These methods involve removing data.\n",
    "\n",
    "* **List-wise Deletion**: Any row with one or more missing values is completely discarded.\n",
    "    * **Pro**: It's simple and convenient.\n",
    "    * **Con**: It can dramatically reduce the sample size and will produce biased results if the data is not MCAR.\n",
    "\n",
    "* **Pairwise Deletion**: For a specific analysis (e.g., a correlation between two variables), only rows missing data for *those specific variables* are ignored.\n",
    "    * **Pro**: It uses more of the available data than list-wise deletion.\n",
    "    * **Con**: It requires MCAR data and can lead to issues like inconsistent sample sizes and invalid covariance matrices.\n",
    "\n",
    "### 5.4. Methods for Handling Missing Data: Single Imputation\n",
    "**Imputation** means filling in missing values with a replacement value.\n",
    "\n",
    "* **Mean/Median Imputation**: Replace missing values with the mean or median of the non-missing values for that variable.\n",
    "    * **Con**: This method artificially reduces the variance of the data and can distort relationships between variables.\n",
    "\n",
    "* **Regression Imputation**: Use a regression model to predict the missing value based on other variables in the dataset.\n",
    "    * **Con**: While better than mean imputation, it still artificially reduces variance because all the imputed points fall perfectly on the regression line. The regression equation is: $$JP_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 IQ_i$$ \n",
    "\n",
    "* **Stochastic Regression Imputation**: This is an improvement on standard regression imputation. It first predicts the missing value and then **adds a random residual term** to that prediction.\n",
    "    * **Pro**: This method restores the lost variability and can produce **unbiased parameter estimates** if the data is MAR, making it a much stronger choice. The equation becomes: $$P_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 IQ_i + z_i$$ where $z_i$ is a random value drawn from the residual variance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Handling Outliers üéØ\n",
    "\n",
    "An **outlier** is a data point that deviates so much from other observations that it seems to have been generated by a different mechanism.\n",
    "\n",
    "### 6.1. Why Do Outliers Matter?\n",
    "Outliers aren't always errors; they can contain useful information about abnormal events like credit card fraud, system intrusion, or medical conditions. However, they can also negatively impact statistical analysis by:\n",
    "* Increasing error variance and reducing the power of tests.\n",
    "* Violating assumptions like normality.\n",
    "* Biasing or distorting estimates like the **mean** and **standard deviation**.\n",
    "\n",
    "### 6.2. Univariate Outlier Detection Methods\n",
    "These methods are used to find outliers in a single variable.\n",
    "\n",
    "* **The 3œÉ Edit Rule**: Assumes the data is normally distributed. Any data point that falls more than **three standard deviations** away from the mean is flagged as an outlier.\n",
    "    * **Weakness**: The mean and standard deviation are themselves very sensitive to outliers, so this method can fail if outliers are present.\n",
    "\n",
    "* **The Hampel Identifier (MAD Method)**: This is a more robust method that uses the **median** as the central point and the **Median Absolute Deviation (MAD)** as the measure of variation.\n",
    "    * Since the median and MAD are less sensitive to extreme values, this method is better at detecting outliers without being skewed by them.\n",
    "\n",
    "* **The Boxplot Rule (IQR Method)**: This is a very common and effective non-parametric method. It flags any data point as an outlier if it falls outside of the following range:\n",
    "    * Lower Bound: $Q1 - 1.5 \\times IQR$\n",
    "    * Upper Bound: $Q3 + 1.5 \\times IQR$\n",
    "    * Where $Q1$ is the 25th percentile, $Q3$ is the 75th percentile, and $IQR = Q3 - Q1$.\n",
    "\n",
    "\n",
    "### 6.3. Multivariate Outlier Detection Methods\n",
    "These methods detect outliers in an n-dimensional space (i.e., by considering multiple variables at once).\n",
    "\n",
    "* **Linear Models**: A regression model is fitted to the data. Points that have a large **residual** (i.e., are far from the fitted regression line or plane) are considered outliers.\n",
    "* **Proximity-Based Models**: These methods are based on distance. Outliers are defined as points that are far from other points or located in sparse regions of the data space. This includes techniques like clustering and density-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Always make a backup of your main dataframe before modifying it!  \n",
    "You don't want to delete corrupt data only to find it could have been helpful  \n",
    "\n",
    "## Data audit\n",
    "Some metrics matter more to some companies than others  \n",
    "E.g. instagram won't care if a few entries are missing, but it would care about inaccurate data\n",
    "\n",
    "## Missing data\n",
    "Know why the data is missing: Errors? Filter question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60f9b1",
   "metadata": {},
   "source": [
    "Q1: Slide 31 - What missing data mechanisms are these 3 columns shown?\n",
    "Column 1 - Missing Completely At Random\n",
    "Column 2 - Missing Not At Random, lower IQ (below 90) are missing ratings, which is related to the x value measured - a direct inference.\n",
    "Column 3 - Missing At Random, lower ratings (7 and 8) are missing ratings, which is related to the y value measured - an indirect inference.\n",
    "\n",
    "Q2: How can you determine what imputation method is the best for your project?\n",
    "It depends on the project, but mean/median/other simple statistics are typically not as good as machine learning methods like regression or random forest.\n",
    "\n",
    "A good R^2 value for assignment is 0.98 or 0.99"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
