{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892380e9",
   "metadata": {},
   "source": [
    "# Week 9: Data Transformation ‚öôÔ∏è\n",
    "\n",
    "## 1. What is Data Transformation?\n",
    "\n",
    "**Data transformation** is the process of cleaning and converting raw data into a format that is more usable and suitable for analysis. This is a key step in the data pre-processing stage of data wrangling.\n",
    "\n",
    "The main goals of transforming data are to:\n",
    "* **Fix Skewness**: Correct data distributions that are asymmetrical.\n",
    "* **Enhance Visualisation**: Make data easier to plot and understand visually.\n",
    "* **Improve Interpretability**: Make the results of analysis easier to explain.\n",
    "* **Meet Model Assumptions**: Ensure the data is compatible with the requirements of a specific statistical model or machine learning algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Normalisation: Creating a Common Scale\n",
    "\n",
    "**Data normalisation** is a fundamental transformation technique that changes the values of numeric columns in a dataset to a common scale. This is extremely important when your features have different units or scales (e.g., age in years, income in dollars, and height in centimeters). Normalisation prevents features with larger ranges from dominating the analysis.\n",
    "\n",
    "There are two main types of normalisation: **scaling** and **standardisation**.\n",
    "\n",
    "### 2.1. Scaling (Rescaling to a Specific Interval)\n",
    "Scaling techniques focus on adjusting the range of your data to fall within a specific interval, like [0, 1] or [-1, 1].\n",
    "\n",
    "#### Min-Max Scaling\n",
    "This is one of the simplest methods, rescaling features to fit within a `[0, 1]` range.\n",
    "* **Formula**:\n",
    "    $$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "    \n",
    "* **Pros**: Easy to implement and preserves the original shape of the distribution.\n",
    "* **Cons**: Highly **sensitive to outliers**. A single extreme value can squash all other data points into a very narrow range, reducing their distinctiveness.\n",
    "\n",
    "\n",
    "#### MaxAbs Scaling\n",
    "This method scales each feature by its maximum absolute value, resulting in a range of `[-1, 1]`.\n",
    "* **Formula**:\n",
    "    $$x_{scaled} = \\frac{x}{\\max(|x|)}$$\n",
    "    \n",
    "* **Pros**: It doesn't shift or center the data, which is useful for sparse datasets (data with many zeros).\n",
    "* **Cons**: Also sensitive to outliers.\n",
    "\n",
    "#### Robust Scaling (Outlier-Resistant) üí™\n",
    "This scaler uses statistics that are robust to outliers: the **median** and the **Interquartile Range (IQR)**.\n",
    "* **Formula**:\n",
    "    $$x_{scaled} = \\frac{x - x_{median}}{IQR(x)}$$\n",
    "    where $IQR = Q3 - Q1$ (the difference between the 75th and 25th percentiles).\n",
    "* **Pros**: Because it ignores extreme values in its calculation, it's **very effective at scaling data with outliers** without squashing the inlying data points.\n",
    "* **Cons**: Calculating quartiles can be more computationally intensive than calculating the mean and standard deviation.\n",
    "\n",
    "\n",
    "#### Log Scaling (Log Transform)\n",
    "A logarithmic transform is useful for data that exhibits exponential growth or is highly right-skewed. It helps to stabilize the variance and make the data's distribution more \"normal\".\n",
    "* **Formula**:\n",
    "    $$x_{scaled} = \\log(x)$$\n",
    "    \n",
    "* **Pros**: Very effective at reducing skewness.\n",
    "* **Cons**: Can only be applied to **positive values** and can sometimes obscure small differences in the original data.\n",
    "\n",
    "### 2.2. Standardisation (Z-score Normalisation)\n",
    "Standardisation is a different approach that rescales data to have a **mean ($\\mu$) of 0** and a **standard deviation ($\\sigma$) of 1**. The resulting value is called a Z-score.\n",
    "* **Formula**:\n",
    "    $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "    \n",
    "* **Pros**: Handles outliers better than Min-Max scaling and is required by many algorithms that assume a normal distribution of the input data.\n",
    "* **Cons**: Doesn't scale the data to a specific, bounded range like Min-Max scaling does.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Power Transformation\n",
    "\n",
    "**Power transformations** are a family of functions used to make data more linear or to stabilize its variance. A key application is transforming a non-linear relationship between two variables into a linear one, which is often easier to model.\n",
    "\n",
    "A well-known example is the **Box-Cox Transformation**, which can transform a continuous variable into an almost normal distribution by finding an optimal exponent, $\\lambda$.\n",
    "* **Formula**:\n",
    "    $$y = \\begin{cases} \\frac{x^\\lambda - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0 \\\\ \\log(x), & \\text{if } \\lambda = 0 \\end{cases}$$\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Data Discretisation (Binning)\n",
    "\n",
    "**Discretisation** is the process of converting continuous numerical variables into discrete, categorical variables (or \"bins\"). This can help reduce noise, smooth data, and make it compatible with algorithms that require categorical inputs.\n",
    "\n",
    "There are two common unsupervised approaches:\n",
    "1.  **Equal-Width Binning**: Divides the data range into a predefined number of intervals, each with the same width. This method is simple but can be negatively affected by outliers and skewed data.\n",
    "2.  **Equal-Depth (Frequency) Binning**: Divides the data into intervals that each contain approximately the same number of data points. This method handles skewed data much better.\n",
    "\n",
    "After binning, the values within each bin can be replaced by the bin's **mean**, **median**, or **boundary values** to smooth the data.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Data Construction: Creating Better Features\n",
    "\n",
    "### 5.1. Feature Subset Selection\n",
    "Often, not all features in a dataset are useful. **Feature subset selection** is the process of reducing the dataset by removing irrelevant or redundant features. The goal is to find the minimum set of attributes that can still produce a good model.\n",
    "Common methods include:\n",
    "* **Stepwise Forward Selection**: Start with no features and add them one by one.\n",
    "* **Stepwise Backward Elimination**: Start with all features and remove them one by one.\n",
    "* **Decision Tree Induction**: Use a decision tree to identify the most important features.\n",
    "\n",
    "### 5.2. Data Sampling\n",
    "Sampling involves selecting a representative subset of data from a larger dataset. This is useful for reducing data volume, fixing class imbalances, or creating training and testing sets.\n",
    "\n",
    "* **Simple Random Sample (SRS)**: Every data point has an equal chance of being selected. This can be done with or without replacement.\n",
    "* **Stratified Sampling**: The data is first divided into subgroups (strata), and then a simple random sample is taken from each stratum. This ensures that all subgroups are fairly represented in the final sample, which is especially important for imbalanced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Data Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb0f21",
   "metadata": {},
   "source": [
    "Q1. Which scaling method is robust to outliers?\n",
    "Robust scaling - it scales by quartiles, which naturally filters out outliers.\n",
    "\n",
    "Q1? Which methods keep the original meaning of the data after scaling?\n",
    "All of them are linear except log scale - so Min-Max scaling, MaxAbs scaling, Decimal scaling, Robust scaling\n",
    "\n",
    "Q2 Which data sampling method is used in the Random Forest algorithm?\n",
    "Simple Random Sample with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc77629a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
