{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b8ed7f",
   "metadata": {},
   "source": [
    "We only prepare the data for data scientists, not analyze it here  \n",
    "Get the right data to solve your goal  \n",
    "Data will not be similar in:\n",
    "- Quality  \n",
    "- Completeness  \n",
    "- Coverage  \n",
    "- Formatting  \n",
    "- Structure  \n",
    "\n",
    "Example: Predictive analysis for patient readmission within 30d of discharge  \n",
    "Relevant data:  \n",
    "Electronic health records  \n",
    "- Patient Health Histories  \n",
    "- Outcomes  \n",
    "- Diagnoses  \n",
    "- Treatment plans  \n",
    "Hospital admission, discharge and transfer systems  \n",
    "- Dates and reasons for admissions, discharges, and transfers within the hospital   \n",
    "Wearable device data  \n",
    "- Patient generated vital signs  \n",
    "Insurance claim data  \n",
    "- Procedures performed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54ced3",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "To collect data that is relevant, accurate and robust for data analysis needs, we need to consider:  \n",
    "- Define objective – what to achieve with the data collection?  \n",
    "- Data requirements – what the specific data needed to meet the objective?  \n",
    "- Data sources – where to obtain the data?  \n",
    "- Data quality – ensure the data collected is of high quality  \n",
    "- Data volume – assess the volume of data needed and the capacity of handling  \n",
    "- Ethics – ensure ethical practices complying with legal standards and permissions  \n",
    "- Data format – what format to be used?  \n",
    "- Collection methods and tools – choose appropriate methods and right tools  \n",
    "- Timeframe and budget – align with project deadlines and budget constraints  \n",
    "- Data privacy and security – protect privacy from individuals and secure storage and handling of data  \n",
    "- Documentation – keep detailed documentation of the data collection process  \n",
    "- Pilot test – validate data collection process and instruments  \n",
    "- Review and adaptation – regularly review the process for any necessary adjustments  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3ec3c1",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "Selection  \n",
    "- Subsetting: Choosing only the relevant data to the analysis, filtering rows/columns on criteria needed  \n",
    "\n",
    "Sampling  \n",
    "- Stratified sampling: Sample proportionally to keep same population proportions  \n",
    "\n",
    "Formatting  \n",
    "- Date formatting, numeric/text formatting, currencies, metric/imperial, text encoding, time zones, etc  \n",
    "\n",
    "Data cleaning  \n",
    "- Anomalies, errors can reveal insights or new patterns, not just errors. E.g. \"are you still watching?\" because someone might be afk  \n",
    "\n",
    "Data transformation  \n",
    "- Normalization, standardization, aggregation, discretisation, binning/bucketing  \n",
    "\n",
    "Text data  \n",
    "- Tokenisation, stemming/lemmatisation, vectorisation  \n",
    "\n",
    "Image data  \n",
    "- Resize, crop, normalize, colour space conversion, rotate, flip, translate, noise injection, brightness/contrast adjustment  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717adeff",
   "metadata": {},
   "source": [
    "# Data Enrichment\n",
    "Enhancing the data with additional context or merging it with other data sources to make it completer and more informative.  This step can involve adding demographic information, geographical tags, or combining datasets to provide more depth.  \n",
    "\n",
    "- Data integration  \n",
    "  - Combining data from different sources to create a more comprehensive dataset.  \n",
    "- Data augmentation  \n",
    "  - Adding information to existing records to enhance the depth of data on each data point.  \n",
    "- Attribute enrichment  \n",
    "  - Enhancing existing datasets by adding new attributes or features that were not previously included.  \n",
    "- Temporal enrichment  \n",
    "  - Adding time-related data to datasets.  \n",
    "- Semantic enrichment  \n",
    "  - Adding metadata or other semantic information to make data more understandable and usable.  \n",
    "\n",
    "Data wrangling is the process of making data useful  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42194fe",
   "metadata": {},
   "source": [
    "# Data Validation\n",
    "Ensuring that the data is accurate, consistent, and usable for analysis.  \n",
    "Validation checks are performed to verify data quality and correctness after cleaning and transformation.  \n",
    "- Define Validation Rules and Criteria  \n",
    "  - Establish the standards that your data must meet.  \n",
    "- Check for accuracy  \n",
    "  - Ensure that the data accurately reflects the real-world entities or values it represents.  \n",
    "- Ensure consistency  \n",
    "  - Verify that the data is consistent within the dataset and across related datasets.  \n",
    "- Validate data completeness  \n",
    "  - Ensure no critical data is missing, which could impact analysis.  \n",
    "- Test for logical integrity  \n",
    "  - Confirm that the data makes logical sense and adheres to known constraints or relationships.  \n",
    "- Validate range and constraints   \n",
    "  - Ensure that data values fall within acceptable ranges or constraints.  \n",
    "- Format validation  \n",
    "  - Verify that the data is in the expected format or structure, which is essential for automated processing.  \n",
    "- Uniqueness checks  \n",
    "  - Ensure that entries that are supposed to be unique do not have duplicates.  \n",
    "- Cross-validation  \n",
    "  - Use related datasets or data sources to validate the data.  \n",
    "- Automate validation processes  \n",
    "  - Streamline validation to make it efficient and repeatable, especially for large datasets or ongoing data collection.  \n",
    "- Document validation issues and resolutions  \n",
    "  - Keep track of identified issues and how they were resolved for future reference and accountability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c6823",
   "metadata": {},
   "source": [
    "# Data Storing\n",
    "The final step involves saving the wrangled data in a suitable storage system for easy access and analysis.  \n",
    "This could be databases, data warehouses, or cloud storage solutions, depending on the scale and purpose of the data analysis project.  \n",
    "- Selection of Storage Solution  \n",
    "  - Choose an appropriate data storage solution that aligns with the data's nature, size, and intended use.  \n",
    "- Data Modelling and Schema Design  \n",
    "  - Design a logical structure for the data that supports efficient storage, query, and retrieval.  \n",
    "- Normalization and Denormalization  \n",
    "  - Organize the data to reduce redundancy and improve integrity in relational databases through\n",
    "    normalization, or optimize performance and read efficiency through denormalization, especially in\n",
    "    NoSQL databases or data warehouses.\n",
    "- Data Formatting and Encoding  \n",
    "  - Ensure that data is stored in a consistent and appropriate format that matches the storage system’s requirements.  \n",
    "- Implementing Data Security Measures  \n",
    "  - Protect data from unauthorized access and ensure privacy and compliance with data protection regulations (e.g., GDPR, HIPAA).  \n",
    "- Data Indexing and Optimization  \n",
    "  - Enhance the speed and efficiency of data retrieval.  \n",
    "- Backup and Recovery Planning  \n",
    "  - Ensure data durability and the ability to recover from data loss or corruption.  \n",
    "- Data Lifecycle Management  \n",
    "  - Manage the lifecycle of data from creation to deletion, aligning with data retention policies and legal requirements.  \n",
    "- Documentation and Metadata Management  \n",
    "  - Provide context and understanding for the data and how it’s stored, facilitating easier data management, governance, and use.  \n",
    "- Monitoring and Maintenance  \n",
    "  - Ensure the storage system remains efficient, scalable, and reliable as data volume grows and access patterns change.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c2316",
   "metadata": {},
   "source": [
    "Any relation between ice cream consumption and heat stroke?\n",
    "Any relation between ice cream consumption and illness?\n",
    "Any relation between summer weather and happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9117483",
   "metadata": {},
   "source": [
    "\n",
    "which data would you need for the hospital data?  \n",
    "\n",
    "If you need to have this sampled dataset, how do you select the data sample? are there any potential issues needed to be considered?  \n",
    "How to select the data samples:  \n",
    "Select data samples that match the population distribution as closely as possible, in the most important attributes e.g. age, income groups.  \n",
    " \n",
    "Are there any potential issues to be considered?  \n",
    "How are you going to match the samples so closely across all relevant attributes?  \n",
    "For smaller attribute groups, you will have a very small sample size, which is susceptible to outliers.  \n",
    "-> similar country is one, what about age group? income groups? how are you going to stratify for all of them?  \n",
    "\n",
    "does the normalization change data characteristics?   \n",
    "no  \n",
    "-> what if you have data ranging 100.0-100.5? they're practically the same  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92d3f4",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "Pandas dataframes default index to 0,1,2... but you can manually specify their row index and column names  \n",
    "```python\n",
    "pd.Dataframe([[<arr1>],[<arr2>],[<arr3>]], index=[2,3,4], columns=['a','b','c','d'])  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc9ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age     City  Hobbies  Age after 5 years\n",
      "0    Alice   25       NY  Writing                 30\n",
      "1      Bob   30       LA  Writing                 35\n",
      "2  Charlie   35  Chicago  Writing                 40\n",
      "3    David   40  Houston  Writing                 45\n",
      "Name                   Alice\n",
      "Age                       25\n",
      "City                      NY\n",
      "Hobbies              Writing\n",
      "Age after 5 years         30\n",
      "Name: 0, dtype: object\n",
      "['Alice' np.int64(25) 'NY' 'Writing' np.int64(30)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'City': ['NY', 'LA', 'Chicago', 'Houston'],\n",
    "    'Hobbies': \"Writing\"\n",
    "})\n",
    "\n",
    "df['Age after 5 years'] = df['Age'] + 5\n",
    "# The DataFrame looks like this:\n",
    "#     Name   Age     City\n",
    "# 0  Alice    25       NY\n",
    "# 1    Bob    30       LA\n",
    "# 2 Charlie    35  Chicago\n",
    "# 3  David    40   Houston\n",
    "print(df)\n",
    "\n",
    "print(df.iloc[0]) # Property of df: locate by index -> df[0]\n",
    "print(df.iloc[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7239585e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>City</th>\n",
       "      <th>Hobbies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>NY</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>LA</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>Houston</td>\n",
       "      <td>Writing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age     City  Hobbies\n",
       "0   25       NY  Writing\n",
       "1   30       LA  Writing\n",
       "2   35  Chicago  Writing\n",
       "3   40  Houston  Writing"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:,'Age':'Hobbies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207c6a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
