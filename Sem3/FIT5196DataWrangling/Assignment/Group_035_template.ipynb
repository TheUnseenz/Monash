{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXmHqr69qCWG"
      },
      "source": [
        "# FIT5196 Assessment 1 - EDA\n",
        "\n",
        "Due Date: 23:55, Sunday, 14 September 2025\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Group 35:\n",
        "Member 1: Adrian Leong Tat Wei, (27030768), atleo4@student.monash.edu, Contribution\n",
        "\n",
        "Member 2: Jun Yuan, (35833645), jyua0050@student.monash.edu, Contribution\n",
        "\n",
        "...\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmTP0mqeqD71"
      },
      "source": [
        "### Table of Content\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s47bipctul5d",
        "outputId": "bfa4cdb1-c633-4301-b939-a25f6f3850bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in c:\\users\\adria\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38tWLVGrIny"
      },
      "source": [
        "## 1. Load, parse and merge data files\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "luwpeoS5bsYN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "# base = \"./drive/MyDrive/FIT5196/Assignment1/\" # for colab\n",
        "base = \"\" # for local drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWI4Dy7r3L-"
      },
      "source": [
        "### 1.1 Load data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbYmZ-nOrfPW",
        "outputId": "db930422-97b2-4be1-8acb-1c5c9f1dadee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'PostID': '49219111338.0', 'UserID': '124930081@N08', 'secret': '1187640507', 'server': '65535.0', 'title': 'DSC_0652 National Trust Museum (former Port Pirie Railway Station), 73-77 Ellen Street, Port Pirie, South Australia', 'ispublic': '1.0', 'isfriend': '0.0', 'isfamily': '0.0', 'farm': '66.0', 'City': 'Adelaide', 'Country': 'Australia', 'Post_date': '2019-12-14 22:49:28', 'Taken_date': '2019-09-18 13:15:16', 'tags': 'portpirie,museum,railwaystation,southaustralia,australia,architecture,heritage,historic,', 'latitude': '-33.175428', 'longitude': '138.010339', 'description': 'Port Pirie station was the original station in Port Pirie. It opened in 1875 when the Port Pirie-Cockburn line opened to Gladstone. The original building was replaced in 1902.\\n\\nState Heritage ID: 10229', 'min_taken_date': '2019-09-18 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://docs.python.org/3/library/xml.etree.elementtree.html\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parse the XML file\n",
        "tree = ET.parse(base + \"Group035.xml\")\n",
        "root = tree.getroot()\n",
        "\n",
        "# root is <FlickrData>, iterate over each <Record>\n",
        "records = []\n",
        "for record in root.findall(\"Record\"):\n",
        "    record_dict = {child.tag: child.text for child in record}\n",
        "    records.append(record_dict)\n",
        "\n",
        "print(type(records))   # <class 'list'>\n",
        "print(records[0])      # print the first record\n",
        "\n",
        "# Assuming records (from XML) is loaded as a list of dicts\n",
        "df_xml = pd.DataFrame(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz0jVkF4r6fn",
        "outputId": "edda6c36-b27c-4ea3-a76d-15696d860199"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'PostID': 51424269137.0, 'UserID': '73561613@N06', 'secret': '9aa2d9ed0e', 'server': 65535.0, 'title': 'Aerial view at the beach with waves', 'ispublic': 1.0, 'isfriend': 0.0, 'isfamily': 0.0, 'farm': 66.0, 'City': 'Woy Woy', 'Country': 'Australia', 'Post_date': '2021-09-05 01:22:55', 'Taken_date': '2021-08-26 09:29:57', 'tags': 'swell,landscape,winter,nature,water,sky,surf,windy,aerial,waves,newsouthwales,sea,uminabeach,morning,blue,beach,ocean,australia,coast,earlymorning,coastal,nsw,outdoors,waterscape,seascape,centralcoast,southerlyswell,seaside,', 'latitude': -33.527998, 'longitude': 151.315008, 'description': 'Southerly swell producing waves at Umina Beach on the Central Coast, NSW, Australia.', 'min_taken_date': '2021-08-26 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://www.geeksforgeeks.org/python/read-json-file-using-python/\n",
        "import json\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(base + \"Group035.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# json_data is now a list of dictionaries\n",
        "print(type(json_data))  # <class 'list'>\n",
        "print(json_data[0])     # print the first item\n",
        "\n",
        "# Assuming json_data is loaded as a list of dicts\n",
        "df_json = pd.DataFrame(json_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTu6Iaq3r688"
      },
      "source": [
        "### 1.2 Merge dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw3J_luaU1ZV",
        "outputId": "131b7391-cddb-48f9-93d9-a1e8b95c5c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning title field...\n",
            "Cleaning title field...\n",
            "Cleaning tags field...\n",
            "Cleaning tags field...\n",
            "Cleaning description field...\n",
            "Cleaning description field...\n",
            "Cleaning City field...\n",
            "Cleaning City field...\n",
            "Cleaning Country field...\n",
            "Cleaning Country field...\n",
            "Text field cleaning completed successfully\n",
            "Merged dataset contains 70000 records from both JSON and XML files\n",
            "Dataset has 18 columns total\n",
            "\n",
            "Sample of cleaned text fields:\n",
            "Record 1:\n",
            "  Title: aerial view at the beach with waves\n",
            "  Description: southerly swell producing waves at umina beach on the central coast, nsw, austra...\n",
            "  Tags: swell,landscape,winter,nature,water,sky,surf,windy,aerial,waves,newsouthwales,sea,uminabeach,morning,blue,beach,ocean,australia,coast,earlymorning,coastal,nsw,outdoors,waterscape,seascape,centralcoast,southerlyswell,seaside,\n",
            "\n",
            "Record 2:\n",
            "  Title: mack muster 2020\n",
            "  Description: rowlspec titan\n",
            "  Tags: nan\n",
            "\n",
            "Record 3:\n",
            "  Title: nan\n",
            "  Description: nan\n",
            "  Tags: melbourne,victoria,australia,urban,city,cbd,walk,outdoor,winter,tattersallslane,graffiti,streetart,sticker,stickers,slaps,poverty,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Data cleaning and type conversion\n",
        "def clean_text_content(text_input):\n",
        "    \"\"\"\n",
        "    Clean text fields according to assignment specifications:\n",
        "    - Convert to lowercase (except NaN)\n",
        "    - Remove XML/JSON tags using regex\n",
        "    - Remove emojis using emoji library\n",
        "    - Remove non-English characters using regex\n",
        "    - Replace null values with 'NaN'\n",
        "    \"\"\"\n",
        "    # Handle null/empty values first\n",
        "    if pd.isna(text_input) or text_input is None or str(text_input).strip() == '' or text_input == \"null\":\n",
        "        return np.nan\n",
        "\n",
        "    text_str = str(text_input)\n",
        "\n",
        "    # Skip conversion for 'NaN' strings\n",
        "    if text_str == 'NaN':\n",
        "        return np.nan\n",
        "\n",
        "    # Convert to lowercase as required\n",
        "    text_str = text_str.lower()\n",
        "\n",
        "    # Remove XML/JSON tags - matches <...> patterns\n",
        "    text_str = re.sub(r'<[^>]*>', '', text_str)\n",
        "    # Remove HTML entities like &quot; &amp; etc\n",
        "    text_str = re.sub(r'&[a-zA-Z0-9#]+;', '', text_str)\n",
        "\n",
        "    # Remove emoji characters using emoji library\n",
        "    text_str = emoji.replace_emoji(text_str, replace='')\n",
        "\n",
        "    # Keep only valid characters, numbers and basic punctuation\n",
        "    text_str = re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\\-\\'\"/\\\\@#$%&*+=<>~`|{}[\\]^]+', '', text_str)\n",
        "\n",
        "    # Should multiple tabs, spaces, new lines, all be condensed into a single space? \n",
        "    # A: Clean up multiple spaces and new lines, condensing them into a single space\n",
        "    text_str = re.sub(r'\\s+', ' ', text_str)\n",
        "    text_str = text_str.strip()\n",
        "\n",
        "    # B: Condense tabs and spaces, but not new lines\n",
        "    # Replace runs of spaces or tabs, but preserve real line breaks\n",
        "    text_str = re.sub(r'[ \\t]+', ' ', text_str)\n",
        "    text_str = text_str.strip()\n",
        "\n",
        "    # Return NaN if empty after cleaning\n",
        "    if len(text_str) == 0:\n",
        "        return np.nan\n",
        "\n",
        "    return text_str\n",
        "\n",
        "# Handle alphanumeric columns\n",
        "alphanumeric_columns = [\"UserID\", \"secret\"]\n",
        "for col in alphanumeric_columns:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
        "            \n",
        "\n",
        "# Handle numeric columns\n",
        "numeric_columns = [\"PostID\", \"server\", \"ispublic\", \"isfriend\", \"isfamily\", \"farm\", \"latitude\", \"longitude\"]\n",
        "for col in numeric_columns:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
        "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
        "            \n",
        "\n",
        "# Handle datetime columns\n",
        "datetime_columns = [\"Post_date\", \"Taken_date\", \"min_taken_date\"]\n",
        "for col in datetime_columns:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
        "            dataframe[col] = pd.to_datetime(dataframe[col], errors='coerce')\n",
        "\n",
        "\n",
        "text_columns_to_clean = [\"title\", \"tags\", \"description\", \"City\", \"Country\"]\n",
        "for col in text_columns_to_clean:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            print(f\"Cleaning {col} field...\")\n",
        "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
        "\n",
        "print(\"Text field cleaning completed successfully\")\n",
        "\n",
        "# Merge datasets\n",
        "# -------------------------------\n",
        "df_all = pd.concat([df_json, df_xml], ignore_index=True)\n",
        "\n",
        "print(f\"Merged dataset contains {len(df_all)} records from both JSON and XML files\")\n",
        "print(f\"Dataset has {len(df_all.columns)} columns total\")\n",
        "\n",
        "print(\"\\nSample of cleaned text fields:\")\n",
        "for i in range(min(3, len(df_all))):\n",
        "    print(f\"Record {i+1}:\")\n",
        "    if 'title' in df_all.columns:\n",
        "        title_sample = df_all.iloc[i]['title']\n",
        "        print(f\"  Title: {title_sample[:80]}...\" if len(str(title_sample)) > 80 else f\"  Title: {title_sample}\")\n",
        "    if 'description' in df_all.columns:\n",
        "        desc_sample = df_all.iloc[i]['description']\n",
        "        print(f\"  Description: {desc_sample[:80]}...\" if len(str(desc_sample)) > 80 else f\"  Description: {desc_sample}\")\n",
        "    if 'tags' in df_all.columns:\n",
        "        tags_sample = df_all.iloc[i]['tags']\n",
        "        print(f\"  Tags: {tags_sample}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "laxaM29rr6Z4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Post_ID', 'User_ID', 'Secret', 'Server', 'Title', 'Is_Public',\n",
            "       'Is_Friend', 'Is_Family', 'Farm', 'City', 'Country', 'Post_Date',\n",
            "       'Taken_Date', 'Tags', 'Latitude', 'Longitude', 'Description',\n",
            "       'Min_Taken_Date'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import wordninja\n",
        "\n",
        "def rename_column(colname: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert column names into Title_Case with underscores.\n",
        "    Handles camelCase, PascalCase, acronyms, and concatenated words.\n",
        "    \"\"\"\n",
        "    # Step 1: Split camelCase / PascalCase into separate words\n",
        "    # e.g. UserID -> ['User', 'ID'], isPublic -> ['is', 'Public']\n",
        "    camel_split = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', colname)\n",
        "\n",
        "    # Step 2: Split on underscores (already separated words)\n",
        "    tokens = re.split(r'[_\\s]+', camel_split)\n",
        "\n",
        "    final_tokens = []\n",
        "    for token in tokens:\n",
        "        if not token:\n",
        "            continue\n",
        "        # Step 3: Preserve acronyms (all caps, length > 1)\n",
        "        if token.isupper() and len(token) > 1:\n",
        "            final_tokens.append(token)\n",
        "        else:\n",
        "            # Step 4: Word segmentation for lowercase tokens\n",
        "            if token.islower():\n",
        "                split_words = wordninja.split(token)\n",
        "            else:\n",
        "                split_words = [token]\n",
        "            # Step 5: Capitalize first letter of each segment\n",
        "            final_tokens.extend([w.capitalize() for w in split_words])\n",
        "\n",
        "    # Step 6: Join with underscores\n",
        "    return \"_\".join(final_tokens)\n",
        "\n",
        "# Example usage on your dataframe:\n",
        "df_all.rename(columns=lambda c: rename_column(c), inplace=True)\n",
        "\n",
        "print(df_all.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "GYC5K7FphcdV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([18])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all.to_csv('Group035_dataset.csv', index=False, na_rep=\"NaN\", encoding=\"utf-8\", quoting=1)\n",
        "df_all.apply(lambda row: len(row), axis=1).unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfL0RePgrIlA"
      },
      "source": [
        "## 2. EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-YCkaN_rm0y",
        "outputId": "83bb88b0-797a-40a2-f725-9092d712a92b"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'seaborn'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Configure plotting settings\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\"EDA libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0idKn45gsHkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWM7rgPtsL2m"
      },
      "source": [
        "### 2.1 Dataset overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKCA3vXsHq7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3kro6TusHna"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17gszKFvsUFC"
      },
      "source": [
        "### 2.2 Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysronk0MrnRI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asJEEedfrnN4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIZ1G6UosYhI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocaWUc0bsZuv"
      },
      "source": [
        "### 2.3 Bivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBU-huXrsZWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcAySjPJsZTA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7o90s7ysd1q"
      },
      "source": [
        "### 2.4 Multivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmdIgADwsZPx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNascrK6sZMs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1wXI3uasZI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQzK1KihsZF5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lztBdXkrIh9"
      },
      "source": [
        "## 3. Key insights and research questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wN_l-Prwz7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6yOi3pOslpJ"
      },
      "source": [
        "### 3.1 Key findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8WT9OIrrxa2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-bWHdGslD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdnN7MB1rxXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOb7mZgdsplD"
      },
      "source": [
        "### 3.2 Machine Learning research questions and justification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyeYHIUsrxRr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVQFsDpNrxOp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y3BM4OEp-IZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIhwcn9Ksw3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUtPZ9mIsx1n"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jDc3OLjsxxr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEV8wKnsxe6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e42de34",
        "outputId": "24de0cc0-9a37-4034-fd8b-5e6ab1256ab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f250954"
      },
      "source": [
        "After you run the cell above and authorize Google Drive access, your Drive will be mounted at `/content/drive`. You can then access your files using the path `/content/drive/MyDrive/your_folder_name/your_file_name`.\n",
        "\n",
        "Now, I'll update the code cells to load the XML and JSON files from your Google Drive. **Please update the file paths in the code cells below to the correct location of your files in Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebd086b",
        "outputId": "b1acc76e-9bba-40d5-83c6-7e07d854e928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
