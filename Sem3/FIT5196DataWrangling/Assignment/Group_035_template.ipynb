{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXmHqr69qCWG"
      },
      "source": [
        "# FIT5196 Assessment 1 - EDA\n",
        "\n",
        "Due Date: 23:55, Sunday, 14 September 2025\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Group 35:\n",
        "Member 1: Adrian Leong Tat Wei, (27030768), atleo4@student.monash.edu, Contribution\n",
        "\n",
        "Member 2: Jun Yuan, (35833645), jyua0050@student.monash.edu, Contribution\n",
        "\n",
        "...\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmTP0mqeqD71"
      },
      "source": [
        "### Table of Content\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s47bipctul5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa4cdb1-c633-4301-b939-a25f6f3850bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38tWLVGrIny"
      },
      "source": [
        "## 1. Load, parse and merge data files\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "base = \"./drive/MyDrive/FIT5196/Assignment1/\""
      ],
      "metadata": {
        "id": "luwpeoS5bsYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWI4Dy7r3L-"
      },
      "source": [
        "### 1.1 Load data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbYmZ-nOrfPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db930422-97b2-4be1-8acb-1c5c9f1dadee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "{'PostID': '49219111338.0', 'UserID': '124930081@N08', 'secret': '1187640507', 'server': '65535.0', 'title': 'DSC_0652 National Trust Museum (former Port Pirie Railway Station), 73-77 Ellen Street, Port Pirie, South Australia', 'ispublic': '1.0', 'isfriend': '0.0', 'isfamily': '0.0', 'farm': '66.0', 'City': 'Adelaide', 'Country': 'Australia', 'Post_date': '2019-12-14 22:49:28', 'Taken_date': '2019-09-18 13:15:16', 'tags': 'portpirie,museum,railwaystation,southaustralia,australia,architecture,heritage,historic,', 'latitude': '-33.175428', 'longitude': '138.010339', 'description': 'Port Pirie station was the original station in Port Pirie. It opened in 1875 when the Port Pirie-Cockburn line opened to Gladstone. The original building was replaced in 1902.\\n\\nState Heritage ID: 10229', 'min_taken_date': '2019-09-18 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://docs.python.org/3/library/xml.etree.elementtree.html\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parse the XML file\n",
        "tree = ET.parse(base + \"Group035.xml\")\n",
        "root = tree.getroot()\n",
        "\n",
        "# root is <FlickrData>, iterate over each <Record>\n",
        "records = []\n",
        "for record in root.findall(\"Record\"):\n",
        "    record_dict = {child.tag: child.text for child in record}\n",
        "    records.append(record_dict)\n",
        "\n",
        "print(type(records))   # <class 'list'>\n",
        "print(records[0])      # print the first record\n",
        "\n",
        "# Assuming records (from XML) is loaded as a list of dicts\n",
        "df_xml = pd.DataFrame(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz0jVkF4r6fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edda6c36-b27c-4ea3-a76d-15696d860199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "{'PostID': 51424269137.0, 'UserID': '73561613@N06', 'secret': '9aa2d9ed0e', 'server': 65535.0, 'title': 'Aerial view at the beach with waves', 'ispublic': 1.0, 'isfriend': 0.0, 'isfamily': 0.0, 'farm': 66.0, 'City': 'Woy Woy', 'Country': 'Australia', 'Post_date': '2021-09-05 01:22:55', 'Taken_date': '2021-08-26 09:29:57', 'tags': 'swell,landscape,winter,nature,water,sky,surf,windy,aerial,waves,newsouthwales,sea,uminabeach,morning,blue,beach,ocean,australia,coast,earlymorning,coastal,nsw,outdoors,waterscape,seascape,centralcoast,southerlyswell,seaside,', 'latitude': -33.527998, 'longitude': 151.315008, 'description': 'Southerly swell producing waves at Umina Beach on the Central Coast, NSW, Australia.', 'min_taken_date': '2021-08-26 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://www.geeksforgeeks.org/python/read-json-file-using-python/\n",
        "import json\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(base + \"Group035.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# json_data is now a list of dictionaries\n",
        "print(type(json_data))  # <class 'list'>\n",
        "print(json_data[0])     # print the first item\n",
        "\n",
        "# Assuming json_data is loaded as a list of dicts\n",
        "df_json = pd.DataFrame(json_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTu6Iaq3r688"
      },
      "source": [
        "### 1.2 Merge dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqsULk_Rr6cz"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Clean and convert data types\n",
        "# -------------------------------\n",
        "# Specify numeric columns\n",
        "numeric_cols = [\"PostID\", \"UserID\", \"server\", \"ispublic\", \"isfriend\", \"isfamily\", \"farm\", \"latitude\", \"longitude\"]\n",
        "for col in numeric_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')  # converts invalid entries to NaN\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Specify datetime columns\n",
        "date_cols = [\"Post_date\", \"Taken_date\", \"min_taken_date\"]\n",
        "for col in date_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Specify text (alphanumeric) columns\n",
        "text_cols = [\"title\", \"tags\", \"description\", \"City\", \"Country\"]\n",
        "for col in text_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            # TODO: convert to lower case\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Last column not to be cleaned: \"secret\", is alphanumeric, unsure if it should be cleaned\n",
        "# TODO: No XML or JSON tags, using regular expressions\n",
        "# TODO: No non-english characters, using regular expressions\n",
        "\n",
        "# # Fill missing values if desired\n",
        "# fill_values = {\"City\": \"Unknown\", \"Country\": \"Unknown\"}\n",
        "# for col, val in fill_values.items():\n",
        "#     for df in [df_json, df_xml]:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = df[col].fillna(val)\n",
        "\n",
        "# # Add source column (source column must not be in final output, only for debug)\n",
        "# df_json[\"source\"] = \"json\"\n",
        "# df_xml[\"source\"] = \"xml\"\n",
        "\n",
        "# -------------------------------\n",
        "# Merge/Combine DataFrames\n",
        "# -------------------------------\n",
        "\n",
        "# Option 1: Concatenate all posts, keeping all users\n",
        "df_all = pd.concat([df_json, df_xml], ignore_index=True)\n",
        "# df_all.rename(columns={\"PostID\": \"Post_ID\", \"UserID\": \"User_ID\"})\n",
        "\n",
        "# Option 2: Merge by UserID, keeping only users present in both\n",
        "# df_merged = pd.merge(df_json, df_xml, on=\"UserID\", suffixes=(\"_json\", \"_xml\"))\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Inspect final DataFrame\n",
        "# -------------------------------\n",
        "# print(df_all.info())\n",
        "# print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data cleaning and type conversion\n",
        "\n",
        "def clean_text_content(text_input):\n",
        "    \"\"\"\n",
        "    Clean text fields according to assignment specifications:\n",
        "    - Convert to lowercase (except NaN)\n",
        "    - Remove XML/JSON tags using regex\n",
        "    - Remove emojis using emoji library\n",
        "    - Remove non-English characters using regex\n",
        "    - Replace null values with 'NaN'\n",
        "    \"\"\"\n",
        "    # Handle null/empty values first\n",
        "    if pd.isna(text_input) or text_input is None or str(text_input).strip() == '' or text_input == \"null\":\n",
        "        return 'NaN'\n",
        "\n",
        "    text_str = str(text_input)\n",
        "\n",
        "    # Skip conversion for 'NaN' strings\n",
        "    if text_str == 'NaN':\n",
        "        return 'NaN'\n",
        "\n",
        "    # Convert to lowercase as required\n",
        "    text_str = text_str.lower()\n",
        "\n",
        "    # Remove XML/JSON tags - matches <...> patterns\n",
        "    text_str = re.sub(r'<[^>]*>', '', text_str)\n",
        "    # Remove HTML entities like &quot; &amp; etc\n",
        "    text_str = re.sub(r'&[a-zA-Z0-9#]+;', '', text_str)\n",
        "\n",
        "    # Remove emoji characters using emoji library\n",
        "    text_str = emoji.replace_emoji(text_str, replace='')\n",
        "\n",
        "    # Keep only valid characters, numbers and basic punctuation\n",
        "    text_str = re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\\-\\'\"/\\\\@#$%&*+=<>~`|{}[\\]^]+', '', text_str)\n",
        "\n",
        "    # Clean up multiple spaces\n",
        "    text_str = re.sub(r'\\s+', ' ', text_str)\n",
        "    text_str = text_str.strip()\n",
        "\n",
        "    # Return NaN if empty after cleaning\n",
        "    if len(text_str) == 0:\n",
        "        return 'NaN'\n",
        "\n",
        "    return text_str\n",
        "\n",
        "# Handle numeric columns\n",
        "numeric_columns = [\"PostID\", \"UserID\", \"server\", \"ispublic\", \"isfriend\", \"isfamily\", \"farm\", \"latitude\", \"longitude\"]\n",
        "for col in numeric_columns:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
        "\n",
        "# Handle datetime columns\n",
        "datetime_columns = [\"Post_date\", \"Taken_date\", \"min_taken_date\"]\n",
        "for col in datetime_columns:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            dataframe[col] = pd.to_datetime(dataframe[col], errors='coerce')\n",
        "\n",
        "\n",
        "text_columns_to_clean = [\"title\", \"tags\", \"description\", \"City\", \"Country\"]\n",
        "for col in text_columns_to_clean:\n",
        "    for dataframe in [df_json, df_xml]:\n",
        "        if col in dataframe.columns:\n",
        "            print(f\"Cleaning {col} field...\")\n",
        "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
        "\n",
        "print(\"Text field cleaning completed successfully\")\n",
        "\n",
        "# Merge datasets\n",
        "# -------------------------------\n",
        "df_all = pd.concat([df_json, df_xml], ignore_index=True)\n",
        "\n",
        "print(f\"Merged dataset contains {len(df_all)} records from both JSON and XML files\")\n",
        "print(f\"Dataset has {len(df_all.columns)} columns total\")\n",
        "\n",
        "print(\"\\nSample of cleaned text fields:\")\n",
        "for i in range(min(3, len(df_all))):\n",
        "    print(f\"Record {i+1}:\")\n",
        "    if 'title' in df_all.columns:\n",
        "        title_sample = df_all.iloc[i]['title']\n",
        "        print(f\"  Title: {title_sample[:80]}...\" if len(str(title_sample)) > 80 else f\"  Title: {title_sample}\")\n",
        "    if 'description' in df_all.columns:\n",
        "        desc_sample = df_all.iloc[i]['description']\n",
        "        print(f\"  Description: {desc_sample[:80]}...\" if len(str(desc_sample)) > 80 else f\"  Description: {desc_sample}\")\n",
        "    if 'tags' in df_all.columns:\n",
        "        tags_sample = df_all.iloc[i]['tags']\n",
        "        print(f\"  Tags: {tags_sample}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw3J_luaU1ZV",
        "outputId": "131b7391-cddb-48f9-93d9-a1e8b95c5c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning title field...\n",
            "Cleaning title field...\n",
            "Cleaning tags field...\n",
            "Cleaning tags field...\n",
            "Cleaning description field...\n",
            "Cleaning description field...\n",
            "Cleaning City field...\n",
            "Cleaning City field...\n",
            "Cleaning Country field...\n",
            "Cleaning Country field...\n",
            "Text field cleaning completed successfully\n",
            "Merged dataset contains 70000 records from both JSON and XML files\n",
            "Dataset has 18 columns total\n",
            "\n",
            "Sample of cleaned text fields:\n",
            "Record 1:\n",
            "  Title: aerial view at the beach with waves\n",
            "  Description: southerly swell producing waves at umina beach on the central coast, nsw, austra...\n",
            "  Tags: swell,landscape,winter,nature,water,sky,surf,windy,aerial,waves,newsouthwales,sea,uminabeach,morning,blue,beach,ocean,australia,coast,earlymorning,coastal,nsw,outdoors,waterscape,seascape,centralcoast,southerlyswell,seaside,\n",
            "\n",
            "Record 2:\n",
            "  Title: mack muster 2020\n",
            "  Description: rowlspec titan\n",
            "  Tags: NaN\n",
            "\n",
            "Record 3:\n",
            "  Title: NaN\n",
            "  Description: NaN\n",
            "  Tags: melbourne,victoria,australia,urban,city,cbd,walk,outdoor,winter,tattersallslane,graffiti,streetart,sticker,stickers,slaps,poverty,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laxaM29rr6Z4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import wordninja\n",
        "\n",
        "def rename_column(colname: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert column names into Title_Case with underscores.\n",
        "    Handles camelCase, PascalCase, acronyms, and concatenated words.\n",
        "    \"\"\"\n",
        "    # Step 1: Split camelCase / PascalCase into separate words\n",
        "    # e.g. UserID -> ['User', 'ID'], isPublic -> ['is', 'Public']\n",
        "    camel_split = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', colname)\n",
        "\n",
        "    # Step 2: Split on underscores (already separated words)\n",
        "    tokens = re.split(r'[_\\s]+', camel_split)\n",
        "\n",
        "    final_tokens = []\n",
        "    for token in tokens:\n",
        "        if not token:\n",
        "            continue\n",
        "        # Step 3: Preserve acronyms (all caps, length > 1)\n",
        "        if token.isupper() and len(token) > 1:\n",
        "            final_tokens.append(token)\n",
        "        else:\n",
        "            # Step 4: Word segmentation for lowercase tokens\n",
        "            if token.islower():\n",
        "                split_words = wordninja.split(token)\n",
        "            else:\n",
        "                split_words = [token]\n",
        "            # Step 5: Capitalize first letter of each segment\n",
        "            final_tokens.extend([w.capitalize() for w in split_words])\n",
        "\n",
        "    # Step 6: Join with underscores\n",
        "    return \"_\".join(final_tokens)\n",
        "\n",
        "# Example usage on your dataframe:\n",
        "df_all.rename(columns=lambda c: rename_column(c), inplace=True)\n",
        "\n",
        "print(df_all.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.to_csv('Group035_dataset.csv', index=False)"
      ],
      "metadata": {
        "id": "GYC5K7FphcdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfL0RePgrIlA"
      },
      "source": [
        "## 2. EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-YCkaN_rm0y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0idKn45gsHkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWM7rgPtsL2m"
      },
      "source": [
        "### 2.1 Dataset overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKCA3vXsHq7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3kro6TusHna"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17gszKFvsUFC"
      },
      "source": [
        "### 2.2 Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysronk0MrnRI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asJEEedfrnN4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIZ1G6UosYhI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocaWUc0bsZuv"
      },
      "source": [
        "### 2.3 Bivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBU-huXrsZWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcAySjPJsZTA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7o90s7ysd1q"
      },
      "source": [
        "### 2.4 Multivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmdIgADwsZPx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNascrK6sZMs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1wXI3uasZI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQzK1KihsZF5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lztBdXkrIh9"
      },
      "source": [
        "## 3. Key insights and research questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wN_l-Prwz7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6yOi3pOslpJ"
      },
      "source": [
        "### 3.1 Key findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8WT9OIrrxa2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-bWHdGslD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdnN7MB1rxXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOb7mZgdsplD"
      },
      "source": [
        "### 3.2 Machine Learning research questions and justification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyeYHIUsrxRr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVQFsDpNrxOp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y3BM4OEp-IZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIhwcn9Ksw3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUtPZ9mIsx1n"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jDc3OLjsxxr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEV8wKnsxe6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e42de34",
        "outputId": "24de0cc0-9a37-4034-fd8b-5e6ab1256ab3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f250954"
      },
      "source": [
        "After you run the cell above and authorize Google Drive access, your Drive will be mounted at `/content/drive`. You can then access your files using the path `/content/drive/MyDrive/your_folder_name/your_file_name`.\n",
        "\n",
        "Now, I'll update the code cells to load the XML and JSON files from your Google Drive. **Please update the file paths in the code cells below to the correct location of your files in Google Drive.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eebd086b",
        "outputId": "b1acc76e-9bba-40d5-83c6-7e07d854e928"
      },
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'drive', 'sample_data']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}