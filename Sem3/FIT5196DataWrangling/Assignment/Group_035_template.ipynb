{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXmHqr69qCWG"
      },
      "source": [
        "# FIT5196 Assessment 1 - EDA\n",
        "\n",
        "Due Date: 23:55, Sunday, 14 September 2025\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Group 35:\n",
        "Member 1: Adrian Leong Tat Wei, (27030768), atleo4@student.monash.edu, Contribution\n",
        "\n",
        "Member 2: Full Name, (Student ID), Email, Contribution\n",
        "\n",
        "...\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmTP0mqeqD71"
      },
      "source": [
        "### Table of Content\n",
        "\n",
        "1.\n",
        "2.\n",
        "3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s47bipctul5d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v38tWLVGrIny"
      },
      "source": [
        "## 1. Load, parse and merge data files\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luwpeoS5bsYN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjWthEv_rfSZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTWI4Dy7r3L-"
      },
      "source": [
        "### 1.1 Load data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vbYmZ-nOrfPW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'PostID': '49219111338.0', 'UserID': '124930081@N08', 'secret': '1187640507', 'server': '65535.0', 'title': 'DSC_0652 National Trust Museum (former Port Pirie Railway Station), 73-77 Ellen Street, Port Pirie, South Australia', 'ispublic': '1.0', 'isfriend': '0.0', 'isfamily': '0.0', 'farm': '66.0', 'City': 'Adelaide', 'Country': 'Australia', 'Post_date': '2019-12-14 22:49:28', 'Taken_date': '2019-09-18 13:15:16', 'tags': 'portpirie,museum,railwaystation,southaustralia,australia,architecture,heritage,historic,', 'latitude': '-33.175428', 'longitude': '138.010339', 'description': 'Port Pirie station was the original station in Port Pirie. It opened in 1875 when the Port Pirie-Cockburn line opened to Gladstone. The original building was replaced in 1902.\\n\\nState Heritage ID: 10229', 'min_taken_date': '2019-09-18 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://docs.python.org/3/library/xml.etree.elementtree.html\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Parse the XML file\n",
        "tree = ET.parse(\"Group035.xml\")\n",
        "root = tree.getroot()\n",
        "\n",
        "# root is <FlickrData>, iterate over each <Record>\n",
        "records = []\n",
        "for record in root.findall(\"Record\"):\n",
        "    record_dict = {child.tag: child.text for child in record}\n",
        "    records.append(record_dict)\n",
        "\n",
        "print(type(records))   # <class 'list'>\n",
        "print(records[0])      # print the first record\n",
        "\n",
        "# Assuming records (from XML) is loaded as a list of dicts\n",
        "df_xml = pd.DataFrame(records)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kz0jVkF4r6fn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "{'PostID': 51424269137.0, 'UserID': '73561613@N06', 'secret': '9aa2d9ed0e', 'server': 65535.0, 'title': 'Aerial view at the beach with waves', 'ispublic': 1.0, 'isfriend': 0.0, 'isfamily': 0.0, 'farm': 66.0, 'City': 'Woy Woy', 'Country': 'Australia', 'Post_date': '2021-09-05 01:22:55', 'Taken_date': '2021-08-26 09:29:57', 'tags': 'swell,landscape,winter,nature,water,sky,surf,windy,aerial,waves,newsouthwales,sea,uminabeach,morning,blue,beach,ocean,australia,coast,earlymorning,coastal,nsw,outdoors,waterscape,seascape,centralcoast,southerlyswell,seaside,', 'latitude': -33.527998, 'longitude': 151.315008, 'description': 'Southerly swell producing waves at Umina Beach on the Central Coast, NSW, Australia.', 'min_taken_date': '2021-08-26 00:00:00'}\n"
          ]
        }
      ],
      "source": [
        "# https://www.geeksforgeeks.org/python/read-json-file-using-python/\n",
        "import json\n",
        "\n",
        "# Open and load the JSON file\n",
        "with open(\"Group035.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "\n",
        "# json_data is now a list of dictionaries\n",
        "print(type(json_data))  # <class 'list'>\n",
        "print(json_data[0])     # print the first item\n",
        "\n",
        "# Assuming json_data is loaded as a list of dicts\n",
        "df_json = pd.DataFrame(json_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTu6Iaq3r688"
      },
      "source": [
        "### 1.2 Merge dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqsULk_Rr6cz"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Clean and convert data types\n",
        "# -------------------------------\n",
        "# Specify numeric columns\n",
        "numeric_cols = [\"PostID\", \"UserID\", \"server\", \"ispublic\", \"isfriend\", \"isfamily\", \"farm\", \"latitude\", \"longitude\"]\n",
        "for col in numeric_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')  # converts invalid entries to NaN\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Specify datetime columns\n",
        "date_cols = [\"Post_date\", \"Taken_date\", \"min_taken_date\"]\n",
        "for col in date_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Specify text (alphanumeric) columns\n",
        "text_cols = [\"title\", \"tags\", \"description\", \"City\", \"Country\"]\n",
        "for col in text_cols:\n",
        "    for df in [df_json, df_xml]:\n",
        "        if col in df.columns:\n",
        "            # TODO: convert to lower case\n",
        "            if df[col] == \"null\" or df[col] == None or df[col] == \"\":\n",
        "                df[col] = \"NaN\"\n",
        "\n",
        "\n",
        "# Last column not to be cleaned: \"secret\", is alphanumeric, unsure if it should be cleaned\n",
        "# TODO: No XML or JSON tags, using regular expressions\n",
        "# TODO: No non-english characters, using regular expressions\n",
        "\n",
        "# # Fill missing values if desired\n",
        "# fill_values = {\"City\": \"Unknown\", \"Country\": \"Unknown\"}\n",
        "# for col, val in fill_values.items():\n",
        "#     for df in [df_json, df_xml]:\n",
        "#         if col in df.columns:\n",
        "#             df[col] = df[col].fillna(val)\n",
        "\n",
        "# # Add source column (source column must not be in final output, only for debug)\n",
        "# df_json[\"source\"] = \"json\"\n",
        "# df_xml[\"source\"] = \"xml\"\n",
        "\n",
        "# -------------------------------\n",
        "# Merge/Combine DataFrames\n",
        "# -------------------------------\n",
        "\n",
        "# Option 1: Concatenate all posts, keeping all users\n",
        "df_all = pd.concat([df_json, df_xml], ignore_index=True)\n",
        "# df_all.rename(columns={\"PostID\": \"Post_ID\", \"UserID\": \"User_ID\"})\n",
        "\n",
        "# Option 2: Merge by UserID, keeping only users present in both\n",
        "# df_merged = pd.merge(df_json, df_xml, on=\"UserID\", suffixes=(\"_json\", \"_xml\"))\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Inspect final DataFrame\n",
        "# -------------------------------\n",
        "# print(df_all.info())\n",
        "# print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laxaM29rr6Z4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import wordninja\n",
        "\n",
        "def rename_column(colname: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert column names into Title_Case with underscores.\n",
        "    Handles camelCase, PascalCase, acronyms, and concatenated words.\n",
        "    \"\"\"\n",
        "    # Step 1: Split camelCase / PascalCase into separate words\n",
        "    # e.g. UserID -> ['User', 'ID'], isPublic -> ['is', 'Public']\n",
        "    camel_split = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', colname)\n",
        "\n",
        "    # Step 2: Split on underscores (already separated words)\n",
        "    tokens = re.split(r'[_\\s]+', camel_split)\n",
        "\n",
        "    final_tokens = []\n",
        "    for token in tokens:\n",
        "        if not token:\n",
        "            continue\n",
        "        # Step 3: Preserve acronyms (all caps, length > 1)\n",
        "        if token.isupper() and len(token) > 1:\n",
        "            final_tokens.append(token)\n",
        "        else:\n",
        "            # Step 4: Word segmentation for lowercase tokens\n",
        "            if token.islower():\n",
        "                split_words = wordninja.split(token)\n",
        "            else:\n",
        "                split_words = [token]\n",
        "            # Step 5: Capitalize first letter of each segment\n",
        "            final_tokens.extend([w.capitalize() for w in split_words])\n",
        "\n",
        "    # Step 6: Join with underscores\n",
        "    return \"_\".join(final_tokens)\n",
        "\n",
        "# Example usage on your dataframe:\n",
        "df_all.rename(columns=lambda c: rename_column(c), inplace=True)\n",
        "\n",
        "print(df_all.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYC5K7FphcdV"
      },
      "outputs": [],
      "source": [
        "df_all.to_csv('Group035_dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfL0RePgrIlA"
      },
      "source": [
        "## 2. EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-YCkaN_rm0y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0idKn45gsHkI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWM7rgPtsL2m"
      },
      "source": [
        "### 2.1 Dataset overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcKCA3vXsHq7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3kro6TusHna"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17gszKFvsUFC"
      },
      "source": [
        "### 2.2 Univariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysronk0MrnRI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asJEEedfrnN4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIZ1G6UosYhI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocaWUc0bsZuv"
      },
      "source": [
        "### 2.3 Bivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBU-huXrsZWC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcAySjPJsZTA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7o90s7ysd1q"
      },
      "source": [
        "### 2.4 Multivariate analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmdIgADwsZPx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNascrK6sZMs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1wXI3uasZI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQzK1KihsZF5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lztBdXkrIh9"
      },
      "source": [
        "## 3. Key insights and research questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wN_l-Prwz7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6yOi3pOslpJ"
      },
      "source": [
        "### 3.1 Key findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8WT9OIrrxa2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-bWHdGslD3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdnN7MB1rxXj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOb7mZgdsplD"
      },
      "source": [
        "### 3.2 Machine Learning research questions and justification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyeYHIUsrxRr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVQFsDpNrxOp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y3BM4OEp-IZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIhwcn9Ksw3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUtPZ9mIsx1n"
      },
      "source": [
        "## Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jDc3OLjsxxr"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyEV8wKnsxe6"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
