{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXmHqr69qCWG"
   },
   "source": [
    "# FIT5196 Assessment 1 - EDA\n",
    "\n",
    "Due Date: 23:55, Sunday, 15 September 2025\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "#### Group 35:\n",
    "Member 1: Adrian Leong Tat Wei, (27030768), atleo4@student.monash.edu, Contribution\n",
    "\n",
    "Member 2: Jun Yuan, (35833645), jyua0050@student.monash.edu, Contribution\n",
    "\n",
    "Member 3: Low Xuan Nan (35373849), alow0028@student.monash.edu, Contribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmTP0mqeqD71"
   },
   "source": [
    "### Table of Content\n",
    "\n",
    "1. Load, parse and merge data files\n",
    "2. EDA\n",
    "3. Key Key findings, insights and research questions\n",
    "4. Reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8467,
     "status": "ok",
     "timestamp": 1757945030774,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "s47bipctul5d",
    "outputId": "606df288-ba9d-4392-d8d4-a23b0adfff1e"
   },
   "outputs": [],
   "source": [
    "!pip install wordninja\n",
    "!pip install cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v38tWLVGrIny"
   },
   "source": [
    "## 1. Load, parse and merge data files\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3021,
     "status": "ok",
     "timestamp": 1757945033792,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "0e42de34",
    "outputId": "e523cead-71c5-414e-bf80-d0726657a202"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# # import os\n",
    "\n",
    "# # print(os.listdir('.'))\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# base = \"/content/drive/MyDrive/FIT5196/Assignment1/\" # for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1757945033808,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "luwpeoS5bsYN"
   },
   "outputs": [],
   "source": [
    "# begin here if running locally\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# for local drive\n",
    "base = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTWI4Dy7r3L-"
   },
   "source": [
    "### 1.1 Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "error",
     "timestamp": 1757945033833,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "vbYmZ-nOrfPW",
    "outputId": "a2b6960f-e0ed-4f11-c965-6cae13d4f2ad"
   },
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/xml.etree.elementtree.html\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(base + \"Group035.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "# root is <FlickrData>, iterate over each <Record>\n",
    "records = []\n",
    "for record in root.findall(\"Record\"):\n",
    "    record_dict = {child.tag: child.text for child in record}\n",
    "    records.append(record_dict)\n",
    "\n",
    "print(type(records))   # <class 'list'>\n",
    "print(records[0])      # print the first record\n",
    "\n",
    "# Assuming records (from XML) is loaded as a list of dicts\n",
    "df_xml = pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1757945033874,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "VeO0Ujokz3_w"
   },
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/python/read-json-file-using-python/\n",
    "import json\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(base + \"Group035.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# json_data is now a list of dictionaries\n",
    "print(type(json_data))  # <class 'list'>\n",
    "print(json_data[0])     # print the first item\n",
    "\n",
    "# Assuming json_data is loaded as a list of dicts\n",
    "df_json = pd.DataFrame(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTu6Iaq3r688"
   },
   "source": [
    "### 1.2 Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "aborted",
     "timestamp": 1757945033941,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "Gw3J_luaU1ZV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clean_text_content(text_input):\n",
    "    \"\"\"\n",
    "    Clean text fields according to assignment specifications:\n",
    "    - Convert to lowercase (except NaN)\n",
    "    - Remove XML/JSON tags using regex\n",
    "    - Remove non-English characters using regex\n",
    "    - Replace null values with 'NaN'\n",
    "    \"\"\"\n",
    "    if pd.isna(text_input) or text_input is None or str(text_input).strip() == '' or text_input == \"null\":\n",
    "        return np.nan\n",
    "\n",
    "    text_str = str(text_input).strip()\n",
    "    if text_str == \"\" or text_str.lower() in {\"nan\", \"null\", \"none\", \"n/a\", \"na\"}:\n",
    "        return np.nan\n",
    "\n",
    "    text_str = text_str.lower()\n",
    "    text_str = re.sub(r'<[^>]*>', '', text_str)\n",
    "    text_str = re.sub(r'&[a-zA-Z0-9#]+;', '', text_str)\n",
    "    text_str = re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\\-\\'\"/\\\\@#$%&*+=~`|{}[\\]^]+', '', text_str)\n",
    "    text_str = re.sub(r'\\s+', ' ', text_str)\n",
    "    text_str = text_str.strip()\n",
    "\n",
    "    if len(text_str) == 0:\n",
    "        return np.nan\n",
    "    return text_str\n",
    "\n",
    "def clean_tags(tags_input):\n",
    "    \"\"\"\n",
    "    Specialized tag cleaning: tokenize by comma, drop empties\n",
    "    \"\"\"\n",
    "    if pd.isna(tags_input) or tags_input is None or str(tags_input).strip() == '' or tags_input == \"null\":\n",
    "        return np.nan\n",
    "\n",
    "    text_str = str(tags_input)\n",
    "    if text_str.lower() == 'nan':\n",
    "        return np.nan\n",
    "\n",
    "    cleaned_text = clean_text_content(tags_input)\n",
    "    if pd.isna(cleaned_text):\n",
    "        return np.nan\n",
    "\n",
    "    tags = [tag.strip() for tag in cleaned_text.split(',')]\n",
    "    tags = [tag for tag in tags if tag and tag != '']\n",
    "\n",
    "    if not tags:\n",
    "        return np.nan\n",
    "\n",
    "    return ','.join(tags)\n",
    "\n",
    "# Clean alphanumeric columns\n",
    "alphanumeric_columns = [\"UserID\", \"secret\"]\n",
    "for col in alphanumeric_columns:\n",
    "    for dataframe in [df_json, df_xml]:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = dataframe[col].astype(str).str.strip()\n",
    "            #dataframe[col] = dataframe[col].apply(clean_text_content)\n",
    "\n",
    "# Clean numeric columns\n",
    "numeric_columns = [\"PostID\", \"server\", \"ispublic\", \"isfriend\", \"isfamily\", \"farm\", \"latitude\", \"longitude\"]\n",
    "for col in numeric_columns:\n",
    "    for dataframe in [df_json, df_xml]:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = pd.to_numeric(dataframe[col], errors='coerce')\n",
    "\n",
    "# Clean datetime columns\n",
    "datetime_columns = [\"Post_date\", \"Taken_date\", \"min_taken_date\"]\n",
    "for col in datetime_columns:\n",
    "    for dataframe in [df_json, df_xml]:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = pd.to_datetime(dataframe[col], errors='coerce')\n",
    "\n",
    "# Clean text columns (Member B focus)\n",
    "text_columns = [\"title\", \"description\", \"City\", \"Country\"]\n",
    "for col in text_columns:\n",
    "    for dataframe in [df_json, df_xml]:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = dataframe[col].apply(clean_text_content)\n",
    "\n",
    "# Clean tags with specialized processing\n",
    "for dataframe in [df_json, df_xml]:\n",
    "    if \"tags\" in dataframe.columns:\n",
    "        dataframe[\"tags\"] = dataframe[\"tags\"].apply(clean_tags)\n",
    "\n",
    "# Merge datasets\n",
    "df_all = pd.concat([df_json, df_xml], ignore_index=True)\n",
    "\n",
    "print(f\"Original shape: {df_all.shape}\")\n",
    "# Data cleaning: Drop duplicate posts (same PostID)\n",
    "df_all = (\n",
    "    df_all.sort_values(\"min_taken_date\")\n",
    "          .drop_duplicates(subset=[\"PostID\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "print(f\"After dropping duplicates: {df_all.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Dataset merged successfully: {len(df_all)} records, {len(df_all.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "aborted",
     "timestamp": 1757945033958,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "laxaM29rr6Z4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import wordninja\n",
    "\n",
    "def rename_column(colname: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert column names into Title_Case with underscores.\n",
    "    Handles camelCase, PascalCase, acronyms, and concatenated words.\n",
    "    \"\"\"\n",
    "    # Step 1: Split camelCase / PascalCase into separate words\n",
    "    # e.g. UserID -> ['User', 'ID'], isPublic -> ['is', 'Public']\n",
    "    camel_split = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', colname)\n",
    "\n",
    "    # Step 2: Split on underscores (already separated words)\n",
    "    tokens = re.split(r'[_\\s]+', camel_split)\n",
    "\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token:\n",
    "            continue\n",
    "        # Step 3: Preserve acronyms (all caps, length > 1)\n",
    "        if token.isupper() and len(token) > 1:\n",
    "            final_tokens.append(token)\n",
    "        else:\n",
    "            # Step 4: Word segmentation for lowercase tokens\n",
    "            if token.islower():\n",
    "                split_words = wordninja.split(token)\n",
    "            else:\n",
    "                split_words = [token]\n",
    "            # Step 5: Capitalize first letter of each segment\n",
    "            final_tokens.extend([w.capitalize() for w in split_words])\n",
    "\n",
    "    # Step 6: Join with underscores\n",
    "    return \"_\".join(final_tokens)\n",
    "\n",
    "# Example usage on your dataframe:\n",
    "df_all.rename(columns=lambda c: rename_column(c), inplace=True)\n",
    "\n",
    "print(df_all.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 49,
     "status": "aborted",
     "timestamp": 1757945033960,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "GYC5K7FphcdV"
   },
   "outputs": [],
   "source": [
    "df_all.to_csv('Group035_dataset.csv', index=False, na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfL0RePgrIlA"
   },
   "source": [
    "## 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "aborted",
     "timestamp": 1757945033962,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "A-YCkaN_rm0y"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import calendar\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import geodesic\n",
    "from shapely.geometry import LineString, Point\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure plotting settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"EDA libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWM7rgPtsL2m"
   },
   "source": [
    "### 2.1 Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1757945033965,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "jcKCA3vXsHq7"
   },
   "outputs": [],
   "source": [
    "print(\"Dataset Structure and Dimensions\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"Total number of records: {len(df_all)}\")\n",
    "print(f\"Number of attributes: {len(df_all.columns)}\")\n",
    "print(f\"Dataset shape: {df_all.shape}\")\n",
    "\n",
    "# Column information\n",
    "summary = (\n",
    "    pd.DataFrame({\n",
    "        \"Non-Null Count\": df_all.count(),\n",
    "        \"Distinct Count\": df_all.nunique()\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"Column\"})\n",
    "    .set_index(pd.Index(range(1, len(df_all.columns)+1)))\n",
    ")\n",
    "\n",
    "print(summary.to_string(index=True, index_names=False))\n",
    "# print(f\"\\nColumn names:\")\n",
    "# for i, col in enumerate(df_all.columns, 1):\n",
    "#     print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = df_all.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "print(f\"\\nTotal memory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Data types summary\n",
    "print(f\"\\nData types summary:\")\n",
    "dtype_counts = df_all.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "print(\"\\nFirst 5 records:\")\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17gszKFvsUFC"
   },
   "source": [
    "### 2.2 Univariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1757945033967,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "Ny5Q5IlSEWf6"
   },
   "outputs": [],
   "source": [
    "# Metadata analysis\n",
    "\n",
    "print(f\"Range of values of Is_Family: [{min(df_all['Is_Family'])}, {max(df_all['Is_Family'])}]\")\n",
    "print(f\"Range of values of Is_Friend: [{min(df_all['Is_Friend'])}, {max(df_all['Is_Friend'])}]\")\n",
    "print(f\"Range of values of Is_Public: [{min(df_all['Is_Public'])}, {max(df_all['Is_Public'])}]\")\n",
    "\n",
    "def bucketize(n):\n",
    "    if n == 1:\n",
    "        return \"1\"\n",
    "    elif 2 <= n <= 5:\n",
    "        return \"2-5\"\n",
    "    elif 6 <= n <= 10:\n",
    "        return \"6-10\"\n",
    "    elif 11 <= n <= 50:\n",
    "        return \"11-50\"\n",
    "    elif 51 <= n <= 100:\n",
    "        return \"51-100\"\n",
    "    elif 101 <= n <= 1000:\n",
    "        return \"101-1000\"\n",
    "    else:\n",
    "        return \"1001+\"\n",
    "# Order by magnitude\n",
    "bucket_order = [\"1\", \"2-5\", \"6-10\", \"11-50\", \"51-100\", \"101-1000\", \"1001+\"]\n",
    "\n",
    "# 1. Posts per user\n",
    "posts_per_user = df_all.groupby(\"User_ID\")[\"Post_ID\"].count()\n",
    "posts_per_user_bucketed = posts_per_user.map(bucketize).value_counts()\n",
    "posts_per_user_bucketed = posts_per_user_bucketed.reindex(bucket_order).dropna()\n",
    "\n",
    "# 2. Posts per server\n",
    "posts_per_server = df_all.groupby(\"Server\")[\"Post_ID\"].count()\n",
    "posts_per_server_bucketed = posts_per_server.map(bucketize).value_counts()\n",
    "posts_per_server_bucketed = posts_per_server_bucketed.reindex(bucket_order).dropna()\n",
    "\n",
    "# 3. Posts per farm (non-bucketed)\n",
    "posts_per_farm = df_all.groupby(\"Farm\")[\"Post_ID\"].count()\n",
    "\n",
    "# Plot in subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "def plot_series(ax, series, title, xlabel, ylabel, logy=True):\n",
    "    series.plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "        ax.set_ylim(1, 100000)\n",
    "\n",
    "# 1. Posts per user\n",
    "plot_series(axes[0], posts_per_user_bucketed, \"Distribution of Users by Posts Made\", \"Posts per User (bucketed)\", \"Number of Users\")\n",
    "\n",
    "# 2. Posts per server\n",
    "plot_series(axes[1], posts_per_server_bucketed, \"Distribution of Servers by Posts Received\", \"Posts per Server (bucketed)\", \"Number of Servers\")\n",
    "\n",
    "# 4. Posts per farm (non-bucketed, alphabetical order OK)\n",
    "plot_series(axes[2], posts_per_farm, \"Posts per Farm\", \"Farm\", \"Number of Posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 54,
     "status": "aborted",
     "timestamp": 1757945033970,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "ysronk0MrnRI"
   },
   "outputs": [],
   "source": [
    "# Text length distributions\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "title_lengths = df_all['Title'].dropna().str.len()\n",
    "plt.hist(title_lengths, bins=30, alpha=0.7)\n",
    "plt.title('Title Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "desc_lengths = df_all['Description'].dropna().str.len()\n",
    "plt.hist(desc_lengths, bins=30, alpha=0.7)\n",
    "plt.title('Description Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "tag_counts = df_all['Tags'].dropna().apply(lambda x: len(x.split(',')))\n",
    "plt.hist(tag_counts, bins=20, alpha=0.7)\n",
    "plt.title('Number of Tags per Post')\n",
    "plt.xlabel('Tag Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "aborted",
     "timestamp": 1757945033971,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "asJEEedfrnN4"
   },
   "outputs": [],
   "source": [
    "# Tag analysis\n",
    "all_tags = []\n",
    "for tags_str in df_all['Tags'].dropna():\n",
    "    all_tags.extend(tags_str.split(','))\n",
    "\n",
    "tag_freq = Counter(all_tags).most_common(20)\n",
    "\n",
    "# Plot top tags\n",
    "plt.figure(figsize=(10, 6))\n",
    "tags_list, counts_list = zip(*tag_freq)\n",
    "plt.bar(tags_list, counts_list)\n",
    "plt.title('Top 20 Most Frequent Tags')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total unique tags: {len(set(all_tags))}\")\n",
    "print(f\"Posts with tags: {df_all['Tags'].notna().sum()} / {len(df_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1757945033973,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "UIZ1G6UosYhI"
   },
   "outputs": [],
   "source": [
    "# description length distributions\n",
    "desc_lengths = df_all['Description'].dropna().str.len()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(desc_lengths, bins=40, alpha=0.7, color='lightblue')\n",
    "plt.title('Description Length Distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(desc_lengths.mean(), color='red', linestyle='--', label=f'Mean: {desc_lengths.mean():.0f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"description statistics: mean={desc_lengths.mean():.0f}, max={desc_lengths.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1757945033974,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "qTiZcd8DMyyd"
   },
   "outputs": [],
   "source": [
    "# tag co-occurrence analysis\n",
    "# get top tags first\n",
    "all_tags = []\n",
    "for tags in df_all['Tags'].dropna():\n",
    "    all_tags.extend(tags.split(','))\n",
    "\n",
    "top_tags = [tag for tag, count in Counter(all_tags).most_common(10)]\n",
    "\n",
    "# build co-occurrence matrix\n",
    "import numpy as np\n",
    "cooc = np.zeros((10, 10))\n",
    "\n",
    "for tags in df_all['Tags'].dropna():\n",
    "    tag_list = tags.split(',')\n",
    "    for i, tag1 in enumerate(top_tags):\n",
    "        for j, tag2 in enumerate(top_tags):\n",
    "            if tag1 in tag_list and tag2 in tag_list and i != j:\n",
    "                cooc[i][j] += 1\n",
    "\n",
    "# plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cooc, xticklabels=top_tags, yticklabels=top_tags,\n",
    "            annot=True, fmt='.0f', cmap='Blues')\n",
    "plt.title('Tag Co-occurrence (Top 10 Tags)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"most common tag combinations:\")\n",
    "for i in range(len(top_tags)):\n",
    "    for j in range(i+1, len(top_tags)):\n",
    "        if cooc[i][j] > 50:  # only show frequent combinations\n",
    "            print(f\"{top_tags[i]} + {top_tags[j]}: {int(cooc[i][j])} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1757945033975,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "Ge0VSSNeKjo4"
   },
   "outputs": [],
   "source": [
    "# Geospatial EDA\n",
    "\n",
    "# Distinct and Missing Country values\n",
    "print(\"Number of distinct Country:\", df_all[\"Country\"].nunique())\n",
    "print(\"Distinct Country:\", df_all[\"Country\"].dropna().unique())\n",
    "print(\"Missing values for Country:\", df_all[\"Country\"].isna().sum())\n",
    "\n",
    "# Distinct and Missing City values\n",
    "print(\"Number of distinct City:\", df_all[\"City\"].nunique())\n",
    "print(\"Distinct City:\", df_all[\"City\"].dropna().unique())\n",
    "print(\"Missing values for City:\", df_all[\"City\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "aborted",
     "timestamp": 1757945033976,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "onpRouWxbh4A"
   },
   "outputs": [],
   "source": [
    "# Country names\n",
    "\n",
    "# Regex helper\n",
    "RE_SPLIT_MULTI = re.compile(r\"[\\/,&;|]|(?:\\band\\b)\", re.I)\n",
    "\n",
    "# List of the valid countries\n",
    "valid_countries_list = [\"Australia\", \"Ireland\", \"Italy\", \"United States\",\n",
    "                    \"New Zealand\", \"United Kingdom\", \"Switzerland\",\n",
    "                   \"Portugal\", \"Denmark\", \"Czech Republic\", \"Belgium\",\n",
    "                   \"Canada\", \"Netherlands\", \"Japan\", \"Singapore\", \"Finland\",\n",
    "                   \"Thailand\", \"Taiwan\", \"Russia\", \"China\", \"Hong Kong\",\n",
    "                   \"Argentina\", \"France\", \"United Arab Emirates\", \"Sweden\",\n",
    "                   \"Sri Lanka\", \"Spain\", \"Germany\", \"Austria\", \"Belarus\",\n",
    "                   \"Papua New Guinea\", \"South Korea\", \"Hungary\", \"Vietnam\",\n",
    "                   \"Bulgaria\", \"Poland\", \"Estonia\", \"Malaysia\", \"Luxembourg\",\n",
    "                   \"Saudi Arabia\", \"Chile\", \"Liechtenstein\", \"Brazil\",\n",
    "                   \"Greece\", \"Egypt\", \"Philippines\", \"Norway\", \"Kazakhstan\"]\n",
    "\n",
    "COUNTRY_RE = re.compile(r\"^(?:\" + \"|\".join(map(re.escape, valid_countries_list)) + r\")$\", re.I)\n",
    "\n",
    "def country_check(country_val):\n",
    "  \"\"\"\n",
    "  Check if a country name is valid based on valid_countries_list\n",
    "\n",
    "  Returns:\n",
    "    (NaN, invalid) -> if no value or not a valid country\n",
    "    (country, valid) -> if valid_name = 1 and is valid\n",
    "    (NaN, multi) -> if valid_name > 1\n",
    "  \"\"\"\n",
    "  s = clean_text_content(country_val)\n",
    "  if pd.isna(s):\n",
    "    return np.nan, \"invalid\" # No value\n",
    "  parts = [p.strip() for p in RE_SPLIT_MULTI.split(s) if p.strip()] # Split on multi separators defined in RE_SPLIT_MULTI\n",
    "  valid_name = [p for p in parts if COUNTRY_RE.match(p)] # Keep as valid_names when country name(s) matched\n",
    "  if len(valid_name) == 1:\n",
    "    return valid_name[0], \"valid\" # Exactly one country name\n",
    "  elif len(valid_name) > 1:\n",
    "    return np.nan, \"multi\" # More than one country name\n",
    "  else:\n",
    "    return np.nan, \"invalid\" # Country names unmatched\n",
    "\n",
    "# Apply the validator\n",
    "tmp = df_all[\"Country\"].apply(country_check)\n",
    "\n",
    "# Create a df to store the values\n",
    "df_country = pd.DataFrame({\n",
    "    \"Country\": df_all[\"Country\"],\n",
    "    \"Valid_country\": tmp.apply(lambda x: x[0]),\n",
    "    \"Country_status\": tmp.apply(lambda x: x[1])\n",
    "})\n",
    "\n",
    "# Data quality and accuracy\n",
    "# Plot country_status bar chart\n",
    "df_country_status = df_country[\"Country_status\"].value_counts().reset_index()\n",
    "df_country_status.columns = [\"Country_status\", \"Count\"]\n",
    "\n",
    "sns.barplot(data = df_country_status, x = \"Country_status\", y = \"Count\", palette = \"Set2\")\n",
    "plt.title(\"The Number of Records for each Country Name Status\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.xlabel(\"Status of Country Name (Valid/Invalid/Multi)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "total = df_country_status[\"Count\"].sum()\n",
    "\n",
    "# add label\n",
    "for i, v in enumerate(df_country_status[\"Count\"].values):\n",
    "  pct = v / total * 100\n",
    "  label = f\"{v} ({pct:.3f}%)\"\n",
    "  plt.text(i, v * 1.05, label, ha = \"center\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1757945033978,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "RoxAuT5aHlcr"
   },
   "outputs": [],
   "source": [
    "# Skewness of the dataset\n",
    "# Plot country bar chart\n",
    "df_country_name = df_country[\"Valid_country\"].value_counts().reset_index().head(10)\n",
    "df_country_name.columns = [\"Country\", \"Count\"]\n",
    "\n",
    "sns.barplot(data = df_country_name, x = \"Country\", y = \"Count\", palette = \"viridis\")\n",
    "plt.title(\"Top 10 Countries by Number of Records\")\n",
    "plt.xlabel(\"Country Name\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.xticks(rotation = 45, ha = \"right\")\n",
    "\n",
    "total = df_country_status[\"Count\"].sum()\n",
    "\n",
    "# add label\n",
    "for i, v in enumerate(df_country_name[\"Count\"].values):\n",
    "  pct = v / total * 100\n",
    "  label = f\"{v} ({pct:.1f}%)\"\n",
    "  plt.text(i, v * 1.05, label, ha = \"center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "aborted",
     "timestamp": 1757945034010,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "njBGta4ANx4T"
   },
   "outputs": [],
   "source": [
    "# Data quality and accuracy\n",
    "# Longitude and Latitude\n",
    "\n",
    "# Check min and max values for longitude\n",
    "print(min(df_all[\"Longitude\"]))\n",
    "print(max(df_all[\"Longitude\"]))\n",
    "print(min(df_all[\"Latitude\"]))\n",
    "print(max(df_all[\"Latitude\"]))\n",
    "\n",
    "# Check if any longitude or latitude values fall outside the valid range\n",
    "invalid_lon = df_all[(df_all[\"Longitude\"] < -180) | (df_all[\"Longitude\"] > 180)]\n",
    "invalid_lat = df_all[(df_all[\"Latitude\"] < -90) | (df_all[\"Latitude\"] > 90)]\n",
    "\n",
    "print(\"Invalid longitude values:\", len(invalid_lon))\n",
    "print(\"Invalid latitude values:\", len(invalid_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1757945034012,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "b1FOuazqIg6K"
   },
   "outputs": [],
   "source": [
    "# Temporal EDA\n",
    "\n",
    "# Missing Taken_Date value\n",
    "print(\"Missing values for Taken_Date:\", df_all[\"Taken_Date\"].isna().sum())\n",
    "\n",
    "# Missing Post_Date value\n",
    "print(\"Missing values for Post_Date:\", df_all[\"Post_Date\"].isna().sum())\n",
    "\n",
    "# Missing Min_Taken_Date value\n",
    "print(\"Missing values for Min_Taken_Date:\", df_all[\"Min_Taken_Date\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11923,
     "status": "aborted",
     "timestamp": 1757945034014,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "uW3CjZ1ci6Ga"
   },
   "outputs": [],
   "source": [
    "# Count of Taken_Date\n",
    "taken_per_year = df_all[\"Taken_Date\"].dt.year.dropna().value_counts().sort_index()\n",
    "taken_year = taken_per_year.index.astype(int) # ensure year is in integer form to avoid decimals\n",
    "taken_counts = taken_per_year.values\n",
    "\n",
    "plt.plot(taken_year, taken_counts, marker ='o', linestyle ='-')\n",
    "\n",
    "plt.title(\"Photos Taken per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Photos Taken\")\n",
    "\n",
    "# Add label\n",
    "for i, v in zip(taken_year, taken_counts):\n",
    "  plt.text(i, v * 1.01, str(v), ha = \"center\", va = \"bottom\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034016,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "-8V3jIk3mcoT"
   },
   "outputs": [],
   "source": [
    "# Count of Post_Date\n",
    "post_per_year = df_all[\"Post_Date\"].dt.year.dropna().value_counts().sort_index()\n",
    "post_year = post_per_year.index.astype(int) # ensure year is in integer form to avoid decimals\n",
    "post_counts = post_per_year.values\n",
    "\n",
    "plt.plot(post_year, post_counts, marker ='o', linestyle ='-')\n",
    "\n",
    "plt.title(\"Photos Posted per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Photos Posted\")\n",
    "plt.xticks(post_year)\n",
    "plt.yscale(\"log\")\n",
    "plt.gca().yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "# Add label\n",
    "for i, v in zip(post_year, post_counts):\n",
    "  plt.text(i, v * 1.1, str(v), ha = \"center\", va = \"bottom\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11923,
     "status": "aborted",
     "timestamp": 1757945034017,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "S-zRf-Pzq_pW"
   },
   "outputs": [],
   "source": [
    "# Count of Min_Taken_Date\n",
    "min_taken_per_year = df_all[\"Min_Taken_Date\"].dt.year.dropna().value_counts().sort_index()\n",
    "min_year = min_taken_per_year.index.astype(int) # ensure year is in integer form to avoid decimals\n",
    "min_counts = min_taken_per_year.values\n",
    "\n",
    "plt.plot(min_year, min_counts, marker ='o', linestyle ='-')\n",
    "\n",
    "plt.title(\"Min Photos Taken per Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Min Number of Photos Taken\")\n",
    "plt.gca().yaxis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "# Add label\n",
    "for i, v in zip(min_year, min_counts):\n",
    "  plt.text(i, v * 1.01, str(v), ha = \"center\", va = \"bottom\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocaWUc0bsZuv"
   },
   "source": [
    "### 2.3 Bivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034019,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "eBU-huXrsZWC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "\n",
    "def bucketize(n):\n",
    "    if n == 1:\n",
    "        return \"1\"\n",
    "    elif 2 <= n <= 5:\n",
    "        return \"2-5\"\n",
    "    elif 6 <= n <= 10:\n",
    "        return \"6-10\"\n",
    "    elif 11 <= n <= 50:\n",
    "        return \"11-50\"\n",
    "    elif 51 <= n <= 100:\n",
    "        return \"51-100\"\n",
    "    elif 101 <= n <= 1000:\n",
    "        return \"101-1000\"\n",
    "    else:\n",
    "        return \"1001+\"\n",
    "# Order by magnitude\n",
    "bucket_order = [\"1\", \"2-5\", \"6-10\", \"11-50\", \"51-100\", \"101-1000\", \"1001+\"]\n",
    "\n",
    "# 4. Users per server\n",
    "users_per_server = df_all.groupby(\"Server\")[\"User_ID\"].nunique()\n",
    "users_per_server_bucketed = users_per_server.map(bucketize).value_counts()\n",
    "users_per_server_bucketed = users_per_server_bucketed.reindex(bucket_order).dropna()\n",
    "\n",
    "# 5. Users per farm (non-bucketed)\n",
    "users_per_farm = df_all.groupby(\"Farm\")[\"User_ID\"].nunique()\n",
    "\n",
    "# Plot in subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes = axes.flatten()\n",
    "\n",
    "def plot_series(ax, series, title, xlabel, ylabel, logy=True):\n",
    "    series.plot(kind=\"bar\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if logy:\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.yaxis.set_major_formatter(ScalarFormatter())\n",
    "        ax.set_ylim(1, 100000)\n",
    "\n",
    "# 4. Users per server\n",
    "plot_series(axes[0], users_per_server_bucketed, \"Distribution of Servers by User Count\", \"Users per Server (bucketed)\", \"Number of Servers\")\n",
    "\n",
    "# 5. Users per farm (non-bucketed)\n",
    "plot_series(axes[1], users_per_farm, \"Users per Farm\", \"Farm\", \"Number of Users\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11923,
     "status": "aborted",
     "timestamp": 1757945034020,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "Ze1JPyYYgnxU"
   },
   "outputs": [],
   "source": [
    "# Geospatial EDA\n",
    "\n",
    "# World-Map\n",
    "plt.scatter(df_all[\"Longitude\"], df_all[\"Latitude\"],\n",
    "            alpha = 0.5, s = 10)\n",
    "\n",
    "plt.title(\"Global Map with Longitude and Latitude Values\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# World coordinate ranges\n",
    "plt.xlim(-180, 180)\n",
    "plt.ylim(-90, 90)\n",
    "\n",
    "plt.grid(True, linestyle = \"--\", alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034022,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "j3PwY-v6p3p-"
   },
   "outputs": [],
   "source": [
    "# Monthly count of Taken_Date\n",
    "taken_2023 = df_all.loc[df_all[\"Taken_Date\"].dt.year == 2023, \"Taken_Date\"]\n",
    "taken_per_month = taken_2023.dt.month.value_counts().sort_index()\n",
    "\n",
    "# Monthly count of Post_Date\n",
    "post_2023 = df_all.loc[df_all[\"Post_Date\"].dt.year == 2023, \"Post_Date\"]\n",
    "post_per_month = post_2023.dt.month.value_counts().sort_index()\n",
    "\n",
    "# Months\n",
    "months = [calendar.month_name[i] for i in taken_per_month.index]\n",
    "\n",
    "plt.plot(months, taken_per_month, marker ='^', linestyle ='--', label = \"Photos Taken\")\n",
    "plt.plot(months, post_per_month, marker ='o', linestyle ='--', label = \"Photos Posted\")\n",
    "\n",
    "plt.title(\"Monthly Trend of Photos Taken and Posted in 2023\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.xticks(rotation = 45, ha = \"right\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7o90s7ysd1q"
   },
   "source": [
    "### 2.4 Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034023,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "xNascrK6sZMs"
   },
   "outputs": [],
   "source": [
    "# Temporal and Geospatial EDA\n",
    "\n",
    "# Photo Density Heatmap\n",
    "photo_heatmap_2023 = df_all[df_all[\"Post_Date\"].dt.year == 2023]\n",
    "\n",
    "# Set up map projection\n",
    "fig, ax = plt.subplots(figsize=(12,8), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "# World features\n",
    "ax.add_feature(cfeature.LAND, facecolor = 'lightgrey')\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth = 0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth = 0.5)\n",
    "\n",
    "post_heatmap = sns.kdeplot(\n",
    "    x = photo_heatmap_2023[\"Longitude\"], y = photo_heatmap_2023[\"Latitude\"],\n",
    "    fill = True, cmap = \"Reds\", levels = 100, alpha = 0.7, ax = ax\n",
    ")\n",
    "\n",
    "# Add legend\n",
    "contour = ax.collections[-1]\n",
    "cbar = plt.colorbar(contour, ax = ax, orientation = \"vertical\", shrink = 0.6, pad = 0.02)\n",
    "cbar.set_label(\"Density\", rotation = -90, labelpad = 20)\n",
    "\n",
    "plt.title(\"Photo Density Heatmap in Australia (2023)\")\n",
    "\n",
    "# Major cities coordinates\n",
    "major_cities = {\n",
    "    \"Sydney\": (-33.8688, 151.2093),\n",
    "    \"Melbourne\": (-37.8136, 144.9631),\n",
    "    \"Brisbane\": (-27.4698, 153.0251),\n",
    "    \"Perth\": (-31.9505, 115.8605),\n",
    "    \"Adelaide\": (-34.9285, 138.6007)\n",
    "}\n",
    "\n",
    "# Add labels\n",
    "for city, (lat, lon) in major_cities.items():\n",
    "    ax.text(lon, lat, city, transform = ccrs.PlateCarree(),\n",
    "            fontsize = 10, ha ='center', va = 'bottom', color = 'black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034024,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "TlVpiFNq1UuG"
   },
   "outputs": [],
   "source": [
    "# User profile/Location/Time Multivariate EDA\n",
    "\n",
    "# PARAMETERS\n",
    "EPS_KM = 25                       # spatial threshold in km\n",
    "CONSISTENT_POSTS = 2              # >= posts to consider a 'consistent' cluster (home)\n",
    "CONSISTENT_SPAN_DAYS = 30         # >= days for consistent cluster\n",
    "HOLIDAY_MAX_SPAN_DAYS = 30        # <= days for short-term holiday\n",
    "MIN_POSTS_FOR_USER = 3            # users with fewer posts will be ignored\n",
    "MIN_BUFFER_DEG = 0.5              # minimum buffer (deg) for zooming (≈ ~50 km lat)\n",
    "\n",
    "# --- helper: cluster user posts using haversine DBSCAN (min_samples=1 allows singletons)\n",
    "def cluster_user_posts(user_df, eps_km=EPS_KM):\n",
    "    coords = user_df[['Latitude', 'Longitude']].values\n",
    "    if len(coords) == 0:\n",
    "        user_df = user_df.copy()\n",
    "        user_df['Cluster'] = np.array([], dtype=int)\n",
    "        return user_df\n",
    "    coords_rad = np.radians(coords)\n",
    "    db = DBSCAN(eps=eps_km / 6371.0, min_samples=1, metric='haversine').fit(coords_rad)\n",
    "    user_df = user_df.copy()\n",
    "    user_df['Cluster'] = db.labels_.astype(int)\n",
    "    return user_df\n",
    "\n",
    "# --- core: detect movements for a single user\n",
    "def detect_movements(user_df,\n",
    "                     eps_km=EPS_KM,\n",
    "                     consistent_posts=CONSISTENT_POSTS,\n",
    "                     consistent_span_days=CONSISTENT_SPAN_DAYS,\n",
    "                     holiday_max_span_days=HOLIDAY_MAX_SPAN_DAYS,\n",
    "                     min_posts_for_user=MIN_POSTS_FOR_USER):\n",
    "    \"\"\"\n",
    "    Returns a list of movement dictionaries for the user (maybe empty).\n",
    "    Each movement dict contains: category ('holiday'|'migration'), user_id, from, to, count, start, end\n",
    "    \"\"\"\n",
    "    movements = []\n",
    "\n",
    "    # Skip users with too few posts overall\n",
    "    if len(user_df) < min_posts_for_user:\n",
    "        return movements\n",
    "\n",
    "    # Ensure sorted by date and dates exist\n",
    "    user_df = user_df.sort_values('Taken_Date').reset_index(drop=True)\n",
    "\n",
    "    # Spatial clustering (allow singletons so holidays with 1 post are valid clusters)\n",
    "    clustered = cluster_user_posts(user_df, eps_km=eps_km)\n",
    "\n",
    "    # Build clusters as a dict (exclude any -1 just in case)\n",
    "    clusters = {lab: grp for lab, grp in clustered.groupby('Cluster') if lab != -1}\n",
    "\n",
    "    # Your requested safe-check (keeps backward-compatible behavior)\n",
    "    if not clusters or not isinstance(clusters, dict):\n",
    "        return movements\n",
    "\n",
    "    # Summarize all clusters (count, date range, centroid)\n",
    "    all_clusters = []\n",
    "    for label, g in clusters.items():\n",
    "        # drop NaT in Taken_Date just in case\n",
    "        valid_dates = g['Taken_Date'].dropna()\n",
    "        if len(valid_dates) == 0:\n",
    "            # cannot establish date ranges for this cluster — still include as zero-span\n",
    "            start = end = pd.NaT\n",
    "            span = 0\n",
    "        else:\n",
    "            start = valid_dates.min()\n",
    "            end = valid_dates.max()\n",
    "            span = (end - start).days\n",
    "\n",
    "        all_clusters.append({\n",
    "            'label': int(label),\n",
    "            'center': (float(g['Latitude'].mean()), float(g['Longitude'].mean())),\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'span_days': int(span),\n",
    "            'count': len(g)\n",
    "        })\n",
    "\n",
    "    # Split clusters by type:\n",
    "    consistent_clusters = [c for c in all_clusters if c['count'] >= consistent_posts and c['span_days'] >= consistent_span_days]\n",
    "    # short_clusters allow singletons or small-span clusters (holidays)\n",
    "    short_clusters = [c for c in all_clusters if c['span_days'] <= holiday_max_span_days]\n",
    "\n",
    "    # Need at least one consistent cluster to call a \"home\" or to reason about holiday-from-home\n",
    "    if not consistent_clusters:\n",
    "        return movements\n",
    "\n",
    "    # Choose main cluster (home) = largest by count, tie-breaker by span_days\n",
    "    main_cluster = max(consistent_clusters, key=lambda x: (x['count'], x['span_days']))\n",
    "    home_center = main_cluster['center']\n",
    "\n",
    "    # ------------ Category 2 (migration) detection ------------\n",
    "    # Look for chronological switch between consistent clusters: earlier cluster(s) -> later cluster(s)\n",
    "    if len(consistent_clusters) >= 2:\n",
    "        consistent_sorted = sorted(consistent_clusters, key=lambda x: x['start'] if pd.notna(x['start']) else pd.Timestamp.min)\n",
    "        first = consistent_sorted[0]\n",
    "        last = consistent_sorted[-1]\n",
    "        # require the earlier cluster ends before the later cluster starts (clear switch)\n",
    "        if pd.notna(first['end']) and pd.notna(last['start']) and (first['end'] < last['start']):\n",
    "            dist_km = geodesic(first['center'], last['center']).km\n",
    "            if dist_km > eps_km:\n",
    "                movements.append({\n",
    "                    'category': 'migration',\n",
    "                    'user_id': user_df['User_ID'].iloc[0],\n",
    "                    'from': first['center'],\n",
    "                    'to': last['center'],\n",
    "                    'count': int(first['count'] + last['count']),\n",
    "                    'start': first['start'],\n",
    "                    'end': last['end']\n",
    "                })\n",
    "                # If migration found, we prioritize it and return (do not also label holiday)\n",
    "                return movements\n",
    "\n",
    "    # ------------ Category 1 (holiday / short trip) detection ------------\n",
    "    # Any short-cluster sufficiently far from home -> holiday\n",
    "    for sc in short_clusters:\n",
    "        # Ignore if this short cluster is essentially the same as the main consistent cluster (very near)\n",
    "        dist_km = geodesic(home_center, sc['center']).km\n",
    "        if dist_km > eps_km:\n",
    "            movements.append({\n",
    "                'category': 'holiday',\n",
    "                'user_id': user_df['User_ID'].iloc[0],\n",
    "                'from': home_center,\n",
    "                'to': sc['center'],\n",
    "                'count': int(sc['count']),\n",
    "                'start': sc['start'],\n",
    "                'end': sc['end']\n",
    "            })\n",
    "\n",
    "    return movements\n",
    "\n",
    "\n",
    "# === RUN over all users and collect results ===\n",
    "def analyze_all_users(df_all):\n",
    "    movements = []\n",
    "    skipped_too_few_posts = 0\n",
    "    skipped_no_consistent = 0\n",
    "    processed_users = 0\n",
    "\n",
    "    # ensure dates are proper\n",
    "    df = df_all.copy()\n",
    "    df['Taken_Date'] = pd.to_datetime(df['Taken_Date'], errors='coerce')\n",
    "\n",
    "    for uid, user_df in df.groupby('User_ID'):\n",
    "        if len(user_df) < MIN_POSTS_FOR_USER:\n",
    "            skipped_too_few_posts += 1\n",
    "            continue\n",
    "\n",
    "        processed_users += 1\n",
    "        user_moves = detect_movements(user_df)\n",
    "        if not user_moves:\n",
    "            skipped_no_consistent += 1\n",
    "            continue\n",
    "        movements.extend(user_moves)\n",
    "\n",
    "    # summary\n",
    "    print(f\"Users processed (>= {MIN_POSTS_FOR_USER} posts): {processed_users}\")\n",
    "    print(f\"Skipped (too few posts): {skipped_too_few_posts}\")\n",
    "    print(f\"Skipped (no consistent clusters): {skipped_no_consistent}\")\n",
    "    print(f\"Total movements found: {len(movements)}\")\n",
    "\n",
    "    return movements\n",
    "\n",
    "# === Visualize (static map) ===\n",
    "\n",
    "def plot_user_movements_static(movements):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_global()\n",
    "\n",
    "    # Base map\n",
    "    ax.add_feature(cfeature.LAND, facecolor=\"lightgray\")\n",
    "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "    ax.add_feature(cfeature.OCEAN, facecolor=\"aliceblue\")\n",
    "\n",
    "    # Define gradient color ranges: origin → destination\n",
    "    gradients = {\n",
    "        \"holiday\": (np.array([0.0, 0.0, 1.0]), np.array([0.0, 0.8, 0.0])),   # blue → green\n",
    "        \"migration\": (np.array([1.0, 1.0, 0.0]), np.array([1.0, 0.0, 0.0]))  # yellow → red\n",
    "    }\n",
    "\n",
    "    all_lats, all_lons = [], []\n",
    "\n",
    "    for m in movements:\n",
    "        start_loc = m.get(\"from\")   # (lat, lon)\n",
    "        end_loc   = m.get(\"to\")     # (lat, lon)\n",
    "        cat       = m.get(\"category\")\n",
    "\n",
    "        if not start_loc or not end_loc or cat not in gradients:\n",
    "            continue\n",
    "\n",
    "        start_lon, start_lat = start_loc[1], start_loc[0]\n",
    "        end_lon, end_lat     = end_loc[1], end_loc[0]\n",
    "\n",
    "        all_lats.extend([start_lat, end_lat])\n",
    "        all_lons.extend([start_lon, end_lon])\n",
    "\n",
    "        # Line thickness (log-scale)\n",
    "        width = np.log1p(m.get(\"count\", 1))/5\n",
    "        if cat == \"migration\":\n",
    "            width *= 5.0\n",
    "\n",
    "        # Break into N segments\n",
    "        N = 20\n",
    "        lons = np.linspace(start_lon, end_lon, N)\n",
    "        lats = np.linspace(start_lat, end_lat, N)\n",
    "\n",
    "        # Start and end colors\n",
    "        c_start, c_end = gradients[cat]\n",
    "\n",
    "        for i in range(N - 1):\n",
    "            frac = i / (N - 1)\n",
    "            color = (1 - frac) * c_start + frac * c_end\n",
    "            ax.plot(\n",
    "                [lons[i], lons[i+1]],\n",
    "                [lats[i], lats[i+1]],\n",
    "                color=color,\n",
    "                linewidth=width,\n",
    "                transform=ccrs.PlateCarree(),\n",
    "                alpha=0.15\n",
    "            )\n",
    "\n",
    "    # Zoom to bounding box\n",
    "    if all_lons and all_lats:\n",
    "        margin = 2.0\n",
    "        ax.set_extent([\n",
    "            min(all_lons) - margin, max(all_lons) + margin,\n",
    "            min(all_lats) - margin, max(all_lats) + margin\n",
    "        ], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=\"blue\", label=\"Holiday Origin\"),\n",
    "        mpatches.Patch(color=\"green\", label=\"Holiday Destination\"),\n",
    "        mpatches.Patch(color=\"yellow\", label=\"Migration Origin\"),\n",
    "        mpatches.Patch(color=\"red\", label=\"Migration Destination\")\n",
    "    ]\n",
    "\n",
    "    ax.legend(handles=legend_handles, loc=\"lower left\", title=\"Movement Legend\")\n",
    "    plt.title(\"User Movements with Directionality\", fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "movements = analyze_all_users(df_all)\n",
    "plot_user_movements_static(movements)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lztBdXkrIh9"
   },
   "source": [
    "## 3. Key findings, insights and research questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6yOi3pOslpJ"
   },
   "source": [
    "### 3.1 Key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHay6RrfEWf-"
   },
   "source": [
    "**Metadata Key Findings**  \n",
    "The count of distinct values in access fields Is_Family, Is_Friend, Is_Public show that the collected data is unanimously Is_Public.\n",
    "This makes sense, because it simply means that the dataset was collected from public data,\n",
    "so naturally, data which had Is_Family and Is_Friend tags simply could not be collected.\n",
    "```python\n",
    "print(f\"Range of values of Is_Family: [{min(df_all['Is_Family'])}, {max(df_all['Is_Family'])}]\") # [0, 0]\n",
    "print(f\"Range of values of Is_Friend: [{min(df_all['Is_Friend'])}, {max(df_all['Is_Friend'])}]\") # [0, 0]\n",
    "print(f\"Range of values of Is_Public: [{min(df_all['Is_Public'])}, {max(df_all['Is_Public'])}]\") # [1, 1]\n",
    "```\n",
    "Some of the Post_ID were not distinct (5352 out of the original 70k). They have been cleaned as they were found to be duplicate data.\n",
    "\n",
    "**User post distribution:**  \n",
    "The distribution of the number of posts users make follows a log distribution. The vast majority of users make very few posts, but the users that do post a lot, post an incredible amount.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGsUy2lc7gCA"
   },
   "source": [
    "**Textual Key Findings**\n",
    "1. **Data completeness:** 95.0% posts have titles, 44.5% have descriptions, 65.8% have tags\n",
    "2. **Text characteristics:** Average 27 chars per title, 175 chars per description, 11 tags per post  \n",
    "3. **Popular topics:** Australia, landscape, melbourne, nature, nsw are most frequent tags\n",
    "4. **Tag patterns:** Strong co-occurrence between location and nature tags observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11924,
     "status": "aborted",
     "timestamp": 1757945034026,
     "user": {
      "displayName": "Amanda Low",
      "userId": "01199426854625260317"
     },
     "user_tz": -480
    },
    "id": "4H-bWHdGslD3"
   },
   "outputs": [],
   "source": [
    "# analyze tag patterns for ML categories\n",
    "nature_tags = ['landscape', 'nature', 'outdoor', 'beach', 'water', 'sky']\n",
    "urban_tags = ['city', 'urban', 'melbourne', 'architecture', 'street']\n",
    "australia_tags = ['australia', 'nsw', 'victoria', 'queensland']\n",
    "\n",
    "nature_posts = sum(1 for tags_str in df_all['Tags'].dropna()\n",
    "                   if any(tag in nature_tags for tag in tags_str.split(',')))\n",
    "urban_posts = sum(1 for tags_str in df_all['Tags'].dropna()\n",
    "                  if any(tag in urban_tags for tag in tags_str.split(',')))\n",
    "australia_posts = sum(1 for tags_str in df_all['Tags'].dropna()\n",
    "                      if any(tag in australia_tags for tag in tags_str.split(',')))\n",
    "\n",
    "print(f\"nature posts: {nature_posts:,}\")\n",
    "print(f\"urban posts: {urban_posts:,}\")\n",
    "print(f\"australia posts: {australia_posts:,}\")\n",
    "print(f\"posts with title+tags: {df_all[['Title','Tags']].dropna().shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKYDlyHf8LuV"
   },
   "source": [
    "**Geospatial Key Findings**\n",
    "\n",
    "1. Only 50.665% of the values in Country column contain valid country names, while 49.312% with invalid country names and 0.023% are multivalued.\n",
    "2. Australia accounts for the largest share of records in Country column, representing 48.1% of the dataset.\n",
    "3. Scatter map of longitude and latitude shows that the dataset only contains coordinates from Australia.\n",
    "\n",
    "**Temporal Key Findings**\n",
    "\n",
    "1. The data collection for temporal data is inconsistent as photo posting records exists as early as 2007, while photo-taking records only appear from 2017 onwards.\n",
    "2. The number of photos taken, posted, and the minimum photos taken all peaked in 2019, with 14,628, 13,659, and 14,630 photos respectively.\n",
    "3. Photos taken plummeted to only 6 photos in 2024. Photos posted remained less than 10 photos annually during 2007 to 2017, then surged in 2018 and 2019 with over 12,000 posts annually.\n",
    "4. The Min_Taken_Date distribution mirrors the Taken_Date, with a peak in 2019 and a drop in 2020-2022, followed by recovery in 2023.\n",
    "5. Photos taken in 2023 peaked during May (~ 1090) and showed another smaller rise in September (~ 980), while the lowest point was in June (~ 700).\n",
    "6. Photos posted in 2023 peaked during May (~ 1030) but gradually declined in the second half of the year with a slight increase in October(~ 800 - 930 per month).\n",
    "\n",
    "**Geospatial and Temporal Key Findings**\n",
    "1. The highest concentration for photo activity in 2023 is clustered around major cities such as Sydney, Melbourne, and Brisbane, with secondary hotspot are visible in Adelaide and Perth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCuIqkitZw3q"
   },
   "source": [
    "**Geospatial Key Insights**\n",
    "\n",
    "1. The mismatch between Country column with Longitude and Latitude column highlights the need to rely on coordinates over country field.\n",
    "\n",
    "**Temporal Key Insights**\n",
    "1. The mismatch of year range in temporal records suggested either incomplete metadata or selective collection. Analyses involving long-term trends should therefore focus on the overlapping window from 2018 to 2023.\n",
    "2. The sharp rise in 2018 for photo posting activity, suggested either change in platform usage or data collection. This aligns with the peak of photo-taking activity, showing consistency between taken and posted patterns.\n",
    "3. The sharp rise in photo-taking activity from February to May corresponds with Australia's Autumn season where many major cultural festivals are held (Australia, 2022). In contrast, the steady activity observed from June to September aligns with Winter season, when landscape photography is popular due to clearer skies and tranquil landmarks (AdamC, 2024).\n",
    "4. The photo-posting taking trend shows that there are some delayed or backlog posting during peak photo-taking period in April and September, suggesting that the delayed posting behaviour may occur during trips, festivals, or seasonal events.\n",
    "\n",
    "**Geospatial and Temporal Key Insights**\n",
    "\n",
    "1. Photo-taking activity may strongly associated with population density (Australian Bureau of Statistics, 2025) and tourism activities, reflecting user behaviour centered around major cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6y3Zy0A-UVFv"
   },
   "source": [
    "**User, Location and Time Insights**  \n",
    "User movement can be identified by identifying user post location and dates. We find that a majority of movement for holiday is from major cities into the countryside, while a majority of long-term migration is from the countryside into major cities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOb7mZgdsplD"
   },
   "source": [
    "### 3.2 Machine Learning research questions and justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EevlnmSD7mDH"
   },
   "source": [
    "**Question 1:** Can we predict post categories from title and description text?\n",
    "\n",
    "From looking at the tags, I noticed that different types of posts have different tag patterns. Nature photos usually have tags like 'landscape', 'outdoor', 'beach', while city photos have 'city', 'melbourne', 'architecture'. Since most posts have titles (95%) and the tag patterns are quite different, we could probably train a model to predict the category by analyzing the words in titles and descriptions. The TF-IDF method could work well here to find important words for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Est6Ck5Y-fHl"
   },
   "source": [
    "**Question 2:** Can we group similar posts together using their tags?\n",
    "\n",
    "Since each post has about 11 tags on average, there's lots of information to compare posts. When I looked at the data, posts about similar topics tend to share many tags - like landscape photos often have 'nature', 'outdoor', 'australia' together. We could measure how similar two posts are by counting how many tags they share (Jaccard similarity) and then use clustering algorithms like K-means to group them. The co-occurrence analysis already shows that some tags naturally go together, which makes clustering possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h2BQgy0EWgL"
   },
   "source": [
    "**Question 3:** Who are the people posting extraordinary amounts of posts?  \n",
    "\n",
    "When looking at the distribution of posts made by users, I notice that many users don't post a lot, but some users post an incredible volume of posts. Making an incredible volume of posts is a behaviour of interest for a multitude of reasons (For example, a study on students (Li et al., 2024) found that brain rot content, often associated with social media usage, significantly affects student academic anxiety, academic engagement, and mindfulness for the worse), so we want to analyze what type of people they are. Location of these users is an obvious trait to look for, but we can also look at the post categories they post, the language they use, to identify their behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pN87bQjEEWgM"
   },
   "source": [
    "**Question 4:** How can we better distribute server load?  \n",
    "\n",
    "The number of users and posts are not very directly correlated with the number of servers and/or farms, some servers and/or farms serve many more people than others. This could be indicative of poor load balancing, which is an infrastructure problem that virtually every company wants to always improve on (if it is feasible to do so). By identifying the noteworthy servers and/or farms, and comparing their locations in proximity to their users, we can identify weak links in the infrastructure where some places may be overburdened, while other places may have too much resources for their value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tl0miQt7Ua5"
   },
   "source": [
    "**Question 5:** Can we predict photo taking and posting activity based on season?\n",
    "\n",
    "The monthly trend showed fluctuations during seasons, highlighting photo taking and posting activities peak in May and dropping significantly in June. This reflects that seasonality influences user behaviour in both photo taking and posting. Therefore, Months input extracted from Post_Date and Taken_Date are the key predictors to predict the seasonal photo taking and posting activities. For instance, regression models such as Random Forest Regressor could be applied to predict the number of photos being taken or posted throughout the season, and using classification models to categorise months into high or low activity levels. The prediction may benefit platforms in resource planning to forecast server loads and storage needs by predicting when uploads will spike to avoid performance issues during seasonal surges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9x3dxgA8BcE"
   },
   "source": [
    "**Question 6:** Can we identify major hotspots of user activity in Australia during a certain year?\n",
    "\n",
    "From the photo density heatmap for 2023, it showed high concentration of photo activity around Australia's major cities such as Sydney, Melbourne, and Brisbane, while rural area exhibited minimal activity. By using geospatial data such as Longitude and Latitude, pairing with Post_Date or Taken_Date allow us to identify the major hotspots of user activity in Australia over the years or a certain year with machine learning techniques such as K-means, XGBoost, etc. This can benefit tourism to target hotspots for event promotion and tourism campaigns in high-activity cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX0cGqja8B4p"
   },
   "source": [
    "**Question 7:** Can we identify user movement through social media data?  \n",
    "\n",
    "Based on the multivariate analysis using user, location and time data, we can identify where users have been, and by extension, where people have moved around to, and whether they are there for a short holiday or have moved there for long-term stay. This is tremendously helpful not just to tourism marketing and planning, but also for long-term government and urban planning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUtPZ9mIsx1n"
   },
   "source": [
    "## 4. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jDc3OLjsxxr"
   },
   "source": [
    "1. AdamC. (2024, April 19). Is winter a good time to visit Australia? | ULTIMATE. Ultimate Adventure Travel. https://www.ultimate.travel/our-blog/is-winter-a-good-time-to-visit-australia/\n",
    "2. Australia, T. (2022, March 16). Australia’s seasons - Tourism Australia. Www.australia.com. https://www.australia.com/en-my/facts-and-planning/when-to-go/australias-seasons.html\n",
    "3. Australian Bureau of Statistics. (2025). Capital cities continue strong growth. Australian Bureau of Statistics. https://www.abs.gov.au/media-centre/media-releases/capital-cities-continue-strong-growth\n",
    "4. IBM. (n.d.). Exploratory Data Analysis. Ibm.com. https://www.ibm.com/think/topics/exploratory-data-analysis\n",
    "5. Li, G., Geng, Y., & Wu, T. (2024). Effects of short-form video app addiction on academic anxiety and academic engagement: The mediating role of mindfulness. Frontiers in Psychology, 15. https://doi.org/10.3389/fpsyg.2024.1428813"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
