{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "Optimization and deep learning have different goals  \n",
    "- Optimization wants to minimize error (training loss)  \n",
    "- Deep learning wants to find the best generalization (test accuracy)  \n",
    "- The optimal minima for training set is often not the same as the optimal minima for true generalization (test set)  \n",
    "\n",
    "## Problems\n",
    "Local minima  \n",
    "- Is a minimum point with derivatives on both sides saying so, but is not global minimum  \n",
    "Saddle points  \n",
    "- Is neither a minimum or max, but all gradients = 0.\n",
    "Vanishing gradients  \n",
    "- Is close to 0 gradient, so optimizers just get stuck without moving  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbdd42",
   "metadata": {},
   "source": [
    "## Convexity\n",
    "Convexity = any two points can be connected by a line without going out of bounds of the area  \n",
    "Models are much easier to test in convex functions, and if an algorithm performs pooly in convex settings it is unlikely to perform good outside convex settings.  \n",
    "Local minima of convex functions are also global minima  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
