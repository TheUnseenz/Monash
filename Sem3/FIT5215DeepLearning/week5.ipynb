{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Optimizers and Practical Skills\n",
    "## Deep Learning Toolbox\n",
    "### Data processing\n",
    "Data augmentation  \n",
    "- Flip, rotate, random crop, colour shift, noise addition, information loss, contrast change  \n",
    "- Batch normalization  \n",
    "\n",
    "Training neural network parameters  \n",
    "- Epoch  \n",
    "- Mini-batch gradient descent  \n",
    "- Loss function  \n",
    "    - Cross-entropy loss\n",
    "\n",
    "Finding optimal weights  \n",
    "- Backpropagation weight update  \n",
    "\n",
    "Parameter tuning - Weights initialization  \n",
    "- Xavier initialization  \n",
    "    - Instead of random initialization, initialize to take into account characteristics unique to the architecture  \n",
    "- Transfer learning\n",
    "    - Can freeze all layers and train only on classifier/last layers and classifier or retrain all depending on how much training we have  \n",
    "\n",
    "Optimizing convergence  \n",
    "- Learning rate  \n",
    "- Adaptive learning rates  \n",
    "\n",
    "Regularization  \n",
    "- Dropout  \n",
    "- Weight regularization  \n",
    "    - Lasso: L1 regularization, shrinks coefficients to 0  \n",
    "    - Ridge: L2 regularization, makes coefficients smaller  \n",
    "    - Elastic Net: L1+L2, trade off being variable selection and small coefficients  \n",
    "- Early stopping  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b09b5",
   "metadata": {},
   "source": [
    "# 🧠 Neural Network Debugging Checklist (Quick Notes)\n",
    "\n",
    "## 0. First Response\n",
    "- ✅ Use simple baseline model (e.g., VGG for images).  \n",
    "- ✅ Standard loss, no custom functions.  \n",
    "- ✅ Disable regularization & augmentation.  \n",
    "- ✅ Check preprocessing (esp. for finetuning).  \n",
    "- ✅ Verify input data visually.  \n",
    "- ✅ Overfit on a tiny dataset (2–20 samples).  \n",
    "- ✅ Add complexity back gradually.  \n",
    "\n",
    "---\n",
    "\n",
    "## I. Dataset Issues\n",
    "- 📸 Check input/labels (e.g., swapped dims, wrong batch, all zeroes).  \n",
    "- 🎲 Feed random input → if same error, data not used properly.  \n",
    "- 🛠️ Validate data loader → inspect first layer’s input.  \n",
    "- 🔗 Ensure correct label mapping & shuffling.  \n",
    "- ❓ Check if input–output relationship is meaningful.  \n",
    "- 🔊 Inspect dataset noise & mislabels.  \n",
    "- 🔀 Shuffle dataset properly.  \n",
    "- ⚖️ Handle class imbalance (loss balancing, resampling).  \n",
    "- 📈 Enough training examples? (~1k images/class for scratch training).  \n",
    "- 🗂️ Ensure batches aren’t single-label.  \n",
    "- 📦 Reduce batch size if too large.  \n",
    "- 🏷️ Use standard datasets first (MNIST, CIFAR-10) to validate pipeline.  \n",
    "\n",
    "---\n",
    "\n",
    "## II. Data Normalization / Augmentation\n",
    "- 📏 Standardize features (zero mean, unit variance).  \n",
    "- 🔄 Avoid excessive augmentation → underfitting risk.  \n",
    "- 🖼️ Match pretrained model preprocessing ([0,1], [-1,1], [0,255]).  \n",
    "- 📊 Train/val/test preprocessing split correctly (train-only stats).  \n",
    "\n",
    "---\n",
    "\n",
    "## III. Implementation Issues\n",
    "- 🧩 Solve simpler subproblem first.  \n",
    "- 🎯 Check loss “at chance” (e.g., 10 classes → CE loss ≈ 2.302).  \n",
    "- ⚠️ Verify loss function (bugs in custom loss?).  \n",
    "- 🛑 Ensure correct inputs to loss (NLLLoss vs CrossEntropyLoss).  \n",
    "- ⚖️ Balance multi-loss weights.  \n",
    "- 📊 Track multiple metrics (not just loss).  \n",
    "- 🧪 Unit test custom layers.  \n",
    "- 🔒 Check for unintentionally frozen layers.  \n",
    "- 🏗️ Increase network size if too weak.  \n",
    "- 🔢 Use unusual dims (primes) to detect shape errors.  \n",
    "- 🧮 Gradient checking (if manual backprop).  \n",
    "\n",
    "---\n",
    "\n",
    "## IV. Training Issues\n",
    "- 🔍 Overfit tiny subset (1–2 samples).  \n",
    "- 🎲 Try different weight inits (Xavier, He).  \n",
    "- 🔧 Tune hyperparams (grid/random search).  \n",
    "- 🚫 Reduce reg. if underfitting (dropout, weight decay, BN).  \n",
    "- ⏳ Allow more training time if loss steadily ↓.  \n",
    "- 🔀 Switch Train ↔ Test mode correctly (BN, dropout).  \n",
    "- 👁️ Visualize training (weights, activations, updates, TensorBoard).  \n",
    "- 📉 Check activations (std ~ 0.5–2.0) for vanishing/exploding.  \n",
    "- ⚡ Try different optimizer (Adam, SGD+momentum).  \n",
    "- 🎚️ Adjust learning rate (×0.1 or ×10).  \n",
    "- 🚫 Debug NaNs (reduce LR, check div/0, log(≤0), trace layer by layer).  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
