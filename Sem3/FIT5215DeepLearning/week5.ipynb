{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Optimizers and Practical Skills\n",
    "## Deep Learning Toolbox\n",
    "### Data processing\n",
    "Data augmentation  \n",
    "- Flip, rotate, random crop, colour shift, noise addition, information loss, contrast change  \n",
    "- Batch normalization  \n",
    "\n",
    "Training neural network parameters  \n",
    "- Epoch  \n",
    "- Mini-batch gradient descent  \n",
    "- Loss function  \n",
    "    - Cross-entropy loss\n",
    "\n",
    "Finding optimal weights  \n",
    "- Backpropagation weight update  \n",
    "\n",
    "Parameter tuning - Weights initialization  \n",
    "- Xavier initialization  \n",
    "    - Instead of random initialization, initialize to take into account characteristics unique to the architecture  \n",
    "- Transfer learning\n",
    "    - Can freeze all layers and train only on classifier/last layers and classifier or retrain all depending on how much training we have  \n",
    "\n",
    "Optimizing convergence  \n",
    "- Learning rate  \n",
    "- Adaptive learning rates  \n",
    "\n",
    "Regularization  \n",
    "- Dropout  \n",
    "- Weight regularization  \n",
    "    - Lasso: L1 regularization, shrinks coefficients to 0  \n",
    "    - Ridge: L2 regularization, makes coefficients smaller  \n",
    "    - Elastic Net: L1+L2, trade off being variable selection and small coefficients  \n",
    "- Early stopping  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b09b5",
   "metadata": {},
   "source": [
    "# ğŸ§  Neural Network Debugging Checklist (Quick Notes)\n",
    "\n",
    "## 0. First Response\n",
    "- âœ… Use simple baseline model (e.g., VGG for images).  \n",
    "- âœ… Standard loss, no custom functions.  \n",
    "- âœ… Disable regularization & augmentation.  \n",
    "- âœ… Check preprocessing (esp. for finetuning).  \n",
    "- âœ… Verify input data visually.  \n",
    "- âœ… Overfit on a tiny dataset (2â€“20 samples).  \n",
    "- âœ… Add complexity back gradually.  \n",
    "\n",
    "---\n",
    "\n",
    "## I. Dataset Issues\n",
    "- ğŸ“¸ Check input/labels (e.g., swapped dims, wrong batch, all zeroes).  \n",
    "- ğŸ² Feed random input â†’ if same error, data not used properly.  \n",
    "- ğŸ› ï¸ Validate data loader â†’ inspect first layerâ€™s input.  \n",
    "- ğŸ”— Ensure correct label mapping & shuffling.  \n",
    "- â“ Check if inputâ€“output relationship is meaningful.  \n",
    "- ğŸ”Š Inspect dataset noise & mislabels.  \n",
    "- ğŸ”€ Shuffle dataset properly.  \n",
    "- âš–ï¸ Handle class imbalance (loss balancing, resampling).  \n",
    "- ğŸ“ˆ Enough training examples? (~1k images/class for scratch training).  \n",
    "- ğŸ—‚ï¸ Ensure batches arenâ€™t single-label.  \n",
    "- ğŸ“¦ Reduce batch size if too large.  \n",
    "- ğŸ·ï¸ Use standard datasets first (MNIST, CIFAR-10) to validate pipeline.  \n",
    "\n",
    "---\n",
    "\n",
    "## II. Data Normalization / Augmentation\n",
    "- ğŸ“ Standardize features (zero mean, unit variance).  \n",
    "- ğŸ”„ Avoid excessive augmentation â†’ underfitting risk.  \n",
    "- ğŸ–¼ï¸ Match pretrained model preprocessing ([0,1], [-1,1], [0,255]).  \n",
    "- ğŸ“Š Train/val/test preprocessing split correctly (train-only stats).  \n",
    "\n",
    "---\n",
    "\n",
    "## III. Implementation Issues\n",
    "- ğŸ§© Solve simpler subproblem first.  \n",
    "- ğŸ¯ Check loss â€œat chanceâ€ (e.g., 10 classes â†’ CE loss â‰ˆ 2.302).  \n",
    "- âš ï¸ Verify loss function (bugs in custom loss?).  \n",
    "- ğŸ›‘ Ensure correct inputs to loss (NLLLoss vs CrossEntropyLoss).  \n",
    "- âš–ï¸ Balance multi-loss weights.  \n",
    "- ğŸ“Š Track multiple metrics (not just loss).  \n",
    "- ğŸ§ª Unit test custom layers.  \n",
    "- ğŸ”’ Check for unintentionally frozen layers.  \n",
    "- ğŸ—ï¸ Increase network size if too weak.  \n",
    "- ğŸ”¢ Use unusual dims (primes) to detect shape errors.  \n",
    "- ğŸ§® Gradient checking (if manual backprop).  \n",
    "\n",
    "---\n",
    "\n",
    "## IV. Training Issues\n",
    "- ğŸ” Overfit tiny subset (1â€“2 samples).  \n",
    "- ğŸ² Try different weight inits (Xavier, He).  \n",
    "- ğŸ”§ Tune hyperparams (grid/random search).  \n",
    "- ğŸš« Reduce reg. if underfitting (dropout, weight decay, BN).  \n",
    "- â³ Allow more training time if loss steadily â†“.  \n",
    "- ğŸ”€ Switch Train â†” Test mode correctly (BN, dropout).  \n",
    "- ğŸ‘ï¸ Visualize training (weights, activations, updates, TensorBoard).  \n",
    "- ğŸ“‰ Check activations (std ~ 0.5â€“2.0) for vanishing/exploding.  \n",
    "- âš¡ Try different optimizer (Adam, SGD+momentum).  \n",
    "- ğŸšï¸ Adjust learning rate (Ã—0.1 or Ã—10).  \n",
    "- ğŸš« Debug NaNs (reduce LR, check div/0, log(â‰¤0), trace layer by layer).  \n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
