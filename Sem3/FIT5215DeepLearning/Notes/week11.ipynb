{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d5eb8c",
   "metadata": {},
   "source": [
    "## Quiz Questions Explained\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1: The Main Task of a Generative Model\n",
    "\n",
    "* **The Question:** This asks for the primary goal of a generative model. 🎨\n",
    "* **Correct Answer Explained:**\n",
    "    * **C. Learn to generate meaningful and good examples that imitate realistic examples in a given dataset from noises**. This is the core purpose of a generative model. It learns the underlying probability distribution of a dataset (e.g., of celebrity faces) and can then sample from that learned distribution to create new, realistic data points that look like they belong to the original set. The process typically starts from a simple random noise vector, as shown in the diagram.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: The Components of GANs\n",
    "\n",
    "* **The Question:** This question asks about the fundamental components of a Generative Adversarial Network (GAN).\n",
    "* **Correct Answers Explained:**\n",
    "    * **C. The noise is fed to the generator to generate fake examples**. The **Generator (G)** is the part of the GAN that creates new data. It takes a random noise vector `z` as input and transforms it into a synthetic data sample (e.g., an image) that resembles the real data.\n",
    "    * **B. GANs use a discriminator to be aware of the difference or divergence between the distribution of generated examples and the distribution of data**. The **Discriminator (D)** acts as a guide for the generator. By learning to tell real and fake data apart, its feedback implicitly measures the \"distance\" or \"divergence\" between the real data distribution and the one the generator is currently producing.\n",
    "    * **D. The discriminator tries to distinguish the real and fake data examples**. This is the explicit task of the discriminator. It's a binary classifier that takes a data sample as input and outputs a probability of that sample being real.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: The Tasks of G and D in GANs\n",
    "\n",
    "* **The Question:** This question delves deeper into the specific objectives of the generator and the discriminator during training.\n",
    "* **Correct Answers Explained:**\n",
    "    * **A. Discriminator tries to discriminate the real and fake data**. This is the discriminator's primary function—to act as a classifier.\n",
    "    * **B. Discriminator tries to set high values for real data and low values for fake data**. More specifically, the discriminator is trained to output a probability close to 1 for real samples and close to 0 for fake samples.\n",
    "    * **D. Generator tries to fool discriminator by generating examples that mimic real data**. The generator's goal is to produce samples that are so realistic that the discriminator classifies them as real (i.e., outputs a probability close to 1). This is how it \"fools\" the discriminator.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4: The Analogy of GANs\n",
    "\n",
    "* **The Question:** This question uses a common analogy to explain the adversarial relationship between the generator and the discriminator. 💰\n",
    "* **Correct Answers Explained:**\n",
    "    * **C. Generator is the machine that produces fake money**. The generator is the **counterfeiter**, trying to create fake currency that is indistinguishable from the real thing.\n",
    "    * **D. Discriminator is the police who attempts to detect fake money**. The discriminator is the **detective or police**, trying to become an expert at spotting the fakes. As the police get better, the counterfeiter has to improve their craft, and vice-versa, in a continuous cycle of improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: The GAN Training Objective\n",
    "\n",
    "* **The Question:** This question asks you to identify the correct mathematical formula for the GAN's min-max objective function.\n",
    "* **Correct Answer Explained:**\n",
    "    * **A. $\\min_G \\max_D J(G,D) = \\mathbb{E}_{x \\sim p_d(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))]$**. This formula represents the two-player game:\n",
    "        * **$\\max_D$ (The Discriminator's Goal):** The discriminator wants to maximize this function. It does this by making $D(x)$ close to 1 for real data (maximizing the first term) and making $D(G(z))$ close to 0 for fake data (also maximizing the second term, since $\\log(1-0)=0$).\n",
    "        * **$\\min_G$ (The Generator's Goal):** The generator wants to minimize this function. It can only affect the second term. It tries to make $D(G(z))$ close to 1 (fooling the discriminator), which makes $\\log(1-D(G(z)))$ go towards $-\\infty$, thus minimizing the overall value.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6: Alternating Updates in GAN Training\n",
    "\n",
    "* **The Question:** This question asks how the single min-max objective is broken down into separate update steps for the generator and discriminator.\n",
    "* **Correct Answer Explained:**\n",
    "    * **C. $\\max_D J(G,D)$ and $\\min_G \\mathbb{E}_{z}[\\log(1-D(G(z)))]$**. The training alternates between two steps:\n",
    "        1.  **Update Discriminator:** We fix the generator and update the discriminator by performing gradient *ascent* on its objective: $\\max_D \\mathbb{E}_{x \\sim p_d(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))]$.\n",
    "        2.  **Update Generator:** We fix the discriminator and update the generator by performing gradient *descent* on its objective: $\\min_G \\mathbb{E}_{z}[\\log(1-D(G(z)))]$. (Note: In practice, we often use $\\max_G \\mathbb{E}_{z}[\\log(D(G(z)))]$ as it provides stronger gradients early in training, as seen in option A).\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7: When the Generator Gets Good\n",
    "\n",
    "* **The Question:** This asks what happens to the discriminator when the generator becomes very effective and its output distribution ($p_g$) is similar to the real data distribution ($p_d$).\n",
    "* **Correct Answers Explained:**\n",
    "    * **B. The task of discriminator becomes harder**. If the generated images are almost indistinguishable from real ones, the discriminator will have a very difficult time telling them apart.\n",
    "    * **C. The accuracy of discriminator approaches 50% (random guest)**. When the discriminator can no longer find any features to distinguish real from fake, its performance will drop to that of random guessing, which is 50% accuracy for a balanced binary classification task.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 8: The Discriminator's Output\n",
    "\n",
    "* **The Question:** This asks about the interpretation of the discriminator's output value, $D(x)$.\n",
    "* **Correct Answers Explained:**\n",
    "    * **A. We need to apply sigmoid at the output layer of discriminator**. Since the discriminator is performing a binary classification task, its final layer uses a sigmoid activation to output a value between 0 and 1, representing a probability.\n",
    "    * **C. $D(x)$ is the probability $x$ to be a real example**. By convention, the discriminator's output is interpreted as the probability that the input sample $x$ is from the real data distribution.\n",
    "    * **D. $1 - D(x)$ is the probability $x$ to be a fake/generated example**. Consequently, $1 - D(x)$ represents the probability that the input sample is fake.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 9: The Optimal Discriminator\n",
    "\n",
    "* **The Question:** This question asks for the mathematical formula for a theoretically optimal discriminator, $D^*(x)$, given the probability density functions of the real ($p_d$) and generated ($p_g$) data.\n",
    "* **Correct Answer Explained:**\n",
    "    * **C. $D^*(x) = \\frac{p_d(x)}{p_g(x) + p_d(x)}$**. This formula can be derived by finding the maximum of the discriminator's objective function. It states that the optimal discriminator's output at any point $x$ is the ratio of the real data density to the sum of the real and fake data densities at that point.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 10: Nash Equilibrium in GANs\n",
    "\n",
    "* **The Question:** This question asks what conditions are met when the GAN training process reaches its ideal convergence point, known as the Nash equilibrium.\n",
    "* **Correct Answers Explained:**\n",
    "    * **C. $p_d(x) = p_g^*(x), \\forall x$**. The theoretical goal of GAN training is for the generator's distribution ($p_g$) to become identical to the real data distribution ($p_d$). At this point, the generator has perfectly learned to mimic the data.\n",
    "    * **B. $D^*(x) = 0.5, \\forall x$**. If $p_d(x) = p_g(x)$, and we plug this into the optimal discriminator formula from the previous question, we get $D^*(x) = \\frac{p_d(x)}{p_d(x) + p_d(x)} = \\frac{p_d(x)}{2p_d(x)} = \\frac{1}{2}$. This means the discriminator is completely confused and can only guess randomly (50% probability).\n",
    "\n",
    "---\n",
    "\n",
    "### Question 11: Mode Collapse\n",
    "\n",
    "* **The Question:** This asks for the definition of \"mode collapse,\" a common failure mode in GAN training.\n",
    "* **Correct Answer Explained:**\n",
    "    * **D. The generated data can cover only a few modes in real data and miss many other modes**. A \"mode\" is a specific variation or category within the data (e.g., in a dataset of dog breeds, \"poodle\" is one mode, \"beagle\" is another). Mode collapse occurs when the generator finds it easy to fool the discriminator by producing only a very limited variety of outputs (e.g., only generating poodles), failing to capture the full diversity of the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 12: The Source of Mode Collapse\n",
    "\n",
    "* **The Question:** This question asks for the underlying reason why mode collapse happens.\n",
    "* **Correct Answer Explained:**\n",
    "    * **D. When updating the generator, there is not any constraints for it to generate data corresponding to all modes**. The generator's objective is simply to create samples that the discriminator thinks are real. If it finds one type of output (one mode) that is particularly effective at fooling the discriminator, the training objective provides no direct incentive for it to explore other modes. It will just keep exploiting the one thing that works.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 13: Common Issues in Training GANs\n",
    "\n",
    "* **The Question:** This question asks for a summary of the most common challenges faced when training GANs.\n",
    "* **Correct Answer Explained:**\n",
    "    * **A. Mode collapse, unrealistic generated images for complex datasets, unstable training**. These are the three classic problems:\n",
    "        * **Mode Collapse:** As discussed, the generator lacks diversity.\n",
    "        * **Unrealistic Images:** Especially with complex datasets, it can be hard for the generator to learn all the necessary details, resulting in artifacts or blurry images.\n",
    "        * **Unstable Training:** The min-max game is notoriously difficult to balance. Sometimes the discriminator becomes too strong and the generator can't learn, or vice versa, leading to oscillations and a failure to converge.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 14: Transposed Convolution Calculation\n",
    "\n",
    "* **The Question:** This asks you to calculate the output size of a `ConvTranspose2d` layer in PyTorch.\n",
    "* **Correct Answer Explained:**\n",
    "    * **A. (16,128, 19, 19)**. The formula for the output size is: $H_{out} = (H_{in} - 1) \\times S - 2P + K + OP$.\n",
    "        * Input Size ($H_{in}$): The code shows the input is 16x128x7x7. So, $H_{in}=7$.\n",
    "        * Stride (S): `stride=3`.\n",
    "        * Padding (P): `padding=2`.\n",
    "        * Kernel Size (K): `kernel_size=5`.\n",
    "        * Output Padding (OP): `output_padding=0`.\n",
    "        * Calculation: $H_{out} = (7 - 1) \\times 3 - 2 \\times 2 + 5 + 0 = 6 \\times 3 - 4 + 5 = 18 - 4 + 5 = 19$.\n",
    "        * The final shape is `(channels, height, width)` -> `(128, 19, 19)`.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 15: Diffusion Models - Forward and Backward Processes\n",
    "\n",
    "* **The Question:** This is a matching exercise to connect the two main processes of a diffusion model with their descriptions. 🌪️\n",
    "* **Correct Matching Explained:**\n",
    "    * **A. Forward process of DMs -> 2. Do shrinking and adding noises to clean images to obtain their noisy versions**. The **forward process** is a fixed procedure where you start with a clean image and incrementally add a small amount of Gaussian noise over many steps until the image becomes pure noise.\n",
    "    * **B. Backward process of DMs -> 1. Go backward from noises and do denoising at each step**. The **backward (or reverse) process** is the generative part. It starts with a pure noise image and uses a trained neural network to gradually remove the noise step-by-step, eventually reconstructing a clean image.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 16: Forward Process Equations in DMs\n",
    "\n",
    "* **The Question:** This asks for the correct mathematical formulas describing the forward diffusion process.\n",
    "* **Correct Answers Explained:**\n",
    "    * **A. $x_t = \\sqrt{1 - \\beta_t} x_{t-1} + \\sqrt{\\beta_t} \\epsilon$ where $\\epsilon \\sim N(0, I)$**. This formula describes a single step of the forward process. The new noisy image $x_t$ is a combination of the previous image $x_{t-1}$ (scaled down slightly) and some new noise $\\epsilon$ (scaled up).\n",
    "    * **C. $x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$ where $\\epsilon \\sim N(0, I)$ and $\\bar{\\alpha}_t = \\prod_{i=1}^t (1 - \\beta_i)$**. This is the \"closed-form\" solution. It allows you to jump directly to any timestep $t$ from the original image $x_0$ without having to compute all the intermediate steps. It's a combination of the scaled original image and scaled noise.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 17: The U-Net in Diffusion Models\n",
    "\n",
    "* **The Question:** This asks about the role and training objective of the U-Net neural network used in diffusion models.\n",
    "* **Correct Answers Explained:**\n",
    "    * **C. This network aims to predict the Gaussian noise $\\epsilon$ we added to a clean image $x_0$ from its corresponding noisy version $x_t$**. The core task of the U-Net is to look at a noisy image $x_t$ at timestep $t$ and predict the exact noise component $\\epsilon$ that was used to create it from the original image $x_0$.\n",
    "    * **A. We train this network by minimizing $||\\epsilon_\\theta(x_t, t) - \\epsilon||_2^2$**. The training objective is a simple mean squared error loss. We want the network's prediction of the noise, $\\epsilon_\\theta(x_t, t)$, to be as close as possible to the true noise $\\epsilon$ that was actually added.\n",
    "    * **D. This network is crucial to denoise noises to gain clear/clean images**. By predicting the noise, the network effectively learns how to *remove* it. During the reverse process, this predicted noise is subtracted from the noisy image to take a step towards a cleaner image.\n",
    "\n",
    "## Revision Notes: Key Takeaways\n",
    "\n",
    "### 1. Generative Models: The Art of Creation 🎨\n",
    "\n",
    "* **Goal:** To learn the underlying distribution of a dataset and generate new, realistic samples from it.\n",
    "* **Input/Output:** They typically take a random noise vector `z` as input and output a complex data sample like an image.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Generative Adversarial Networks (GANs)\n",
    "\n",
    "* **Concept:** A two-player, zero-sum game between two neural networks:\n",
    "    * **Generator (G):** The counterfeiter. Tries to create realistic fake data from random noise to fool the discriminator.\n",
    "    * **Discriminator (D):** The police. Tries to distinguish between real data and the generator's fake data.\n",
    "* **Training Objective:** A min-max game described by the formula:\n",
    "    $\\min_G \\max_D \\mathbb{E}_{x \\sim p_d}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$.\n",
    "* **Nash Equilibrium:** The ideal convergence point where the generator's distribution perfectly matches the real data distribution ($p_g = p_d$), and the discriminator is completely fooled ($D(x)=0.5$).\n",
    "* **Challenges:**\n",
    "    * **Mode Collapse:** The generator produces a very limited variety of outputs.\n",
    "    * **Training Instability:** The adversarial training is hard to balance and may not converge.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Diffusion Models: A Different Approach\n",
    "\n",
    "* **Concept:** A class of generative models inspired by thermodynamics that produce high-quality samples by gradually reversing a noising process.\n",
    "* **Two Main Processes:**\n",
    "    1.  **Forward Process (Fixed):** Start with a clean image and incrementally add Gaussian noise over T steps until it becomes pure noise. This process is mathematically defined and requires no training.\n",
    "    2.  **Reverse Process (Learned):** Start with pure noise and use a trained neural network (typically a **U-Net**) to gradually denoise it over T steps, reconstructing a clean image.\n",
    "* **The U-Net's Job:** The core of the model is a U-Net, $\\epsilon_\\theta(x_t, t)$, that is trained to predict the noise ($\\epsilon$) that was added to an image, given the noisy image ($x_t$) and the current timestep ($t$).\n",
    "* **Training:** The objective is to minimize the Mean Squared Error between the predicted noise and the actual noise: $||\\epsilon_\\theta(x_t, t) - \\epsilon||_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "one-hot encoding\n",
    "continuous bag of words vs skipgram\n",
    "skip-gram predicts context words from target word\n",
    "CBOW predicts target word from context words\n",
    "\n",
    "multi head self attention\n",
    "positional encoding in transformers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
