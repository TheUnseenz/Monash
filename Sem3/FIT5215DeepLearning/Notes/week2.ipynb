{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a46b71",
   "metadata": {},
   "source": [
    "Recap:  \n",
    "Supervised learning requires labeled data, unsupervised learning does not  \n",
    "One hot encoding is used for categorical features  \n",
    "Cross entropy loss   \n",
    "    = -$\\sum$ $p_i$ log $q_i$  \n",
    "    = measure of surprise e.g. shaved beard  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c59a67",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "Relu  \n",
    "- function gate (on or off)  \n",
    "- popular because it's good enough and very simple (well-behaved derivatives, better-behaved optimization)\n",
    "Sigmoid  \n",
    "- squashing function (squish output to [0, 1] range)  \n",
    "Tanh  \n",
    "- also squashing function (squish to [-1, 1] range)  \n",
    "\n",
    "# Training\n",
    "Forward propagation sequentially calculates and stores intermediate variables within the computational graph defined by the neural network. It proceeds from the input to the output layer. Backpropagation sequentially calculates and stores the gradients of intermediate variables and parameters within the neural network in the reversed order. When training deep learning models, forward propagation and backpropagation are interdependent, and training requires significantly more memory than prediction (because the forward prop values are reused in backprop, so they need to be stored). The size of such intermediate values is roughly proportional to the number of network layers and the batch size. Thus, training deeper networks using larger batch sizes more easily leads to out-of-memory errors.\n",
    "\n",
    "# Vanishing and Exploding Gradients in Machine Learning\n",
    "---\n",
    "## Vanishing Gradients\n",
    "- **Definition:** During backpropagation, gradients (used to update weights) shrink as they move backward through layers.  \n",
    "- **Cause:**  \n",
    "  - Activation functions like **sigmoid** or **tanh** have derivatives ≤ 1.  \n",
    "  - Multiplying many small numbers across layers → gradients become **exponentially smaller**.  \n",
    "- **Effect:** Earlier layers learn very slowly or stop learning altogether.  \n",
    "- **Problem:** Makes it hard to learn **long-range dependencies**, especially in deep and recurrent networks.  \n",
    "\n",
    "---\n",
    "## Exploding Gradients\n",
    "- **Definition:** Gradients grow exponentially as they propagate backward.  \n",
    "- **Cause:**  \n",
    "  - Large weights or derivatives > 1.  \n",
    "  - Multiplying across layers → gradients become **very large**.  \n",
    "- **Effect:** Training becomes unstable (loss oscillates or becomes NaN).  \n",
    "\n",
    "---\n",
    "## Analogy\n",
    "- **Vanishing case:** Like a message whispered quietly down a line of people — by the end, it’s inaudible.  \n",
    "- **Exploding case:** Like each person shouting louder and louder — by the end, it’s chaotic noise.  \n",
    "\n",
    "---\n",
    "## Mitigation Techniques\n",
    "\n",
    "### For Vanishing Gradients:\n",
    "- Use **ReLU** or variants instead of sigmoid/tanh.  \n",
    "- Apply **Batch Normalization**.  \n",
    "- Add **Residual/Skip connections** (ResNets).  \n",
    "- Use proper **weight initialization** (e.g., Xavier, He).  \n",
    "\n",
    "### For Exploding Gradients:\n",
    "- Apply **Gradient Clipping** (cap max gradient values).  \n",
    "- Careful **weight initialization**.  \n",
    "- Use **normalization layers**.  \n",
    "- Use adaptive optimizers like **Adam** or **RMSprop**.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
