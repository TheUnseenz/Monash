{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# FIT5215: Deep Learning Exam Revision Notes\n",
    "\n",
    "## 1. Course Overview: The Big Picture\n",
    "\n",
    "This unit covers the following key areas of Deep Learning:\n",
    "* **Fundamentals (Weeks 1, 2, 4, 5):** ML basics, Feed-forward Neural Networks (FFNs), backpropagation, and optimization.\n",
    "* **Deep Computer Vision (Weeks 3, 6, 10):** Convolutional Neural Networks (CNNs), architectures (ResNet), and Vision Transformers (ViT).\n",
    "* **Sequential / Time-Series (Weeks 7, 9):** Recurrent Neural Networks (RNNs), LSTMs, GRUs, Seq2Seq, and Transformers.\n",
    "* **Representation Learning (Week 8):** Word2Vec (Skip-Gram, CBOW).\n",
    "* **Deep Generative Models (Week 11):** Generative Adversarial Networks (GANs) and Diffusion Models.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feed-Forward Neural Networks (FFNs)\n",
    "\n",
    "### 2.1. Architecture & Forward Propagation\n",
    "\n",
    "* **Definition:** A base model for deep learning composed of an input layer, hidden layers, and an output layer.\n",
    "* **Parameters:** Weight matrices ($W^k$) and bias vectors ($b^k$) for each layer $k$.\n",
    "* **Forward Propagation (Classification):**\n",
    "    1.  **Input:** $h^0(x) = x$\n",
    "    2.  **Hidden Layers ($k=1$ to $L-1$):**\n",
    "        * **Linear Operation:** $\\bar{h}^k(x) = h^{k-1}(x)W^k + b^k$\n",
    "        * **Activation:** $h^k(x) = \\sigma(\\bar{h}^k(x))$ (introduces non-linearity)\n",
    "    3.  **Output Layer (Logits):** $h^L(x) = h^{L-1}(x)W^L + b^L$\n",
    "    4.  **Prediction:** $p(x) = \\text{softmax}(h^L(x))$\n",
    "* **Forward Propagation (Regression):**\n",
    "    * Same as classification, but the output layer is linear.\n",
    "    * **Prediction:** $\\hat{y} = h^L(x)$\n",
    "\n",
    "### 2.2. Activation Functions\n",
    "\n",
    "The key to giving NNs non-linear capabilities. Without them, a deep network is just a linear model.\n",
    "\n",
    "| Function | Formula | Output Range | Gradient | Notes |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Sigmoid** | $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ | (0, 1) | $\\sigma(z)(1 - \\sigma(z))$ | S-shaped. Prone to **vanishing gradients** as gradient is near 0 for large positive/negative inputs (saturation). |\n",
    "| **tanh** | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (-1, 1) | $1 - \\sigma^2(z)$ | S-shaped and zero-centered, which can help convergence. Still saturates. |\n",
    "| **ReLU** | $\\text{ReLU}(z) = \\max(0, z)$ | [0, $\\infty$) | $\\begin{cases} 1 & \\text{if } z \\ge 0 \\\\ 0 & \\text{otherwise} \\end{cases}$ | Most common default. Fast to compute. Solves vanishing gradient for $z>0$. Can \"die\" if $z<0$ (no gradient). |\n",
    "\n",
    "### 2.3. Loss Function & Training\n",
    "\n",
    "* **Loss (Classification):** Cross-Entropy (CE) Loss / Negative Log-Likelihood.\n",
    "    * Measures the difference between predicted probabilities $p(x)$ and the true label $y$.\n",
    "    * $CE(x, y) = -\\log p_y(x)$\n",
    "* **Training:** The goal is to find parameters $\\theta = \\{(W^k, b^k)\\}$ that minimize the total loss over the training set $D$.\n",
    "    * $L(D; \\theta) = \\frac{1}{N} \\sum_{i=1}^N CE(1_{y_i}, p(x_i))$\n",
    "* **Process:** This minimization is done using optimizers (like SGD) which require calculating the gradient of the loss w.r.t. all parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Optimization\n",
    "\n",
    "### 3.1. Calculus & Backpropagation\n",
    "\n",
    "* **Goal:** Find $\\nabla_\\theta J(\\theta)$, the gradient of the loss w.r.t. parameters.\n",
    "* **Chain Rule:** The core of backpropagation. It allows us to compute gradients layer by layer, starting from the loss and moving backward.\n",
    "    * If $z = g(y)$ and $y = f(x)$, then $\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x}$.\n",
    "* **Jacobian Matrix:** A matrix of all first-order partial derivatives of a vector-valued function. The chain rule for vectors/matrices uses the Jacobian.\n",
    "* **Key Gradient (Softmax + CE Loss):** The gradient of the CE loss w.r.t. the logits $h$ (output *before* softmax) is remarkably simple:\n",
    "    * $\\frac{\\partial l}{\\partial h} = p - 1_y$ (where $p$ is the softmax probability vector and $1_y$ is the one-hot true label).\n",
    "* **Backpropagation:** An algorithm that uses the chain rule to efficiently compute gradients. It propagates the error gradient backward from the output layer to the input layer.\n",
    "    * **Forward Pass:** Compute layer outputs and the final loss.\n",
    "    * **Backward Pass:** Compute gradients for each $W^k$ and $b^k$ using the chain rule.\n",
    "\n",
    "### 3.2. Optimization Algorithms\n",
    "\n",
    "* **Problem:** The loss surface of a deep network is highly complex, non-convex, and filled with exponentially more **saddle points** than local minima.\n",
    "* **Gradient Descent (GD):**\n",
    "    * Update rule: $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)$\n",
    "    * Computes the gradient $\\nabla_\\theta J(\\theta_t)$ using the **entire** training set $N$.\n",
    "    * Impractical for large datasets ($O(N)$ cost per step).\n",
    "* **Stochastic Gradient Descent (SGD):**\n",
    "    * Estimates the gradient using a **mini-batch** $b$ of data (e.g., $b=32$).\n",
    "    * $\\nabla_\\theta \\tilde{L}(\\theta_t) = \\frac{1}{b} \\sum_{k=1}^b \\nabla_\\theta l(x_{i_k}, y_{i_k}; \\theta_t)$\n",
    "    * This is an unbiased estimate of the true gradient and is much faster ($O(b)$ cost).\n",
    "    * Update rule: $\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta \\tilde{L}(\\theta_t)$\n",
    "* **SGD with Momentum:**\n",
    "    * Adds a \"velocity\" vector $v$ that accumulates past gradients.\n",
    "    * $v = \\alpha v + (1-\\alpha)g$ (This is a common, simplified form; slides may show $v = \\alpha g_{t-1} + ...$)\n",
    "    * Update: $\\theta = \\theta - \\eta v$\n",
    "    * Helps to speed up and stabilize convergence, dampening oscillations.\n",
    "* **AdaGrad (Adaptive Gradient):**\n",
    "    * Adaptively scales the learning rate for each parameter.\n",
    "    * Accumulates the sum of squared gradients $\\gamma$ for each parameter.\n",
    "    * Update: $\\theta = \\theta - \\frac{\\eta}{\\sqrt{\\epsilon + \\gamma}} \\odot g$\n",
    "    * Gives smaller updates to parameters with large gradients (prevents overshooting) and larger updates to parameters with small gradients.\n",
    "    * Weakness: The learning rate always decreases, which can cause training to stall.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Convolutional Neural Networks (CNNs)\n",
    "\n",
    "### 4.1. CNN Architecture\n",
    "\n",
    "A typical CNN consists of a **Feature Extractor** followed by a **Classifier**.\n",
    "1.  **Feature Extractor:** `(CONV -> ReLU -> CONV -> ReLU -> POOL)`*...\n",
    "2.  **Classifier:** `(FLATTEN -> FC -> ReLU -> FC -> SOFTMAX)`\n",
    "\n",
    "### 4.2. Convolutional Layer\n",
    "\n",
    "* **Operation:** Slides a small filter/kernel (e.g., 3x3) over the input image/feature map. At each position, it computes a dot product (convolution) between the kernel and the input patch.\n",
    "* **Key Components:**\n",
    "    * **Kernel/Filter ($f_h, f_w$):** The small matrix of learnable weights. Detects specific features (edges, corners, textures).\n",
    "    * **Strides ($s_h, s_w$):** The step size the kernel moves. A stride > 1 downsamples the output.\n",
    "    * **Padding ($p$):** Adds a border (usually of zeros) to the input. \"Zero padding\" is common. `p=1` for a 3x3 kernel is often used to preserve the input dimensions (known as 'same' padding).\n",
    "* **Output Size:** The height ($H_o$) and width ($W_o$) of the output feature map are:\n",
    "    * $W_o = \\lfloor \\frac{W_i + 2p - f_w}{s_w} \\rfloor + 1$\n",
    "    * $H_o = \\lfloor \\frac{H_i + 2p - f_h}{s_h} \\rfloor + 1$\n",
    "* **Feature Volume:** An input of shape `(C_in, H_i, W_i)` convolved with `C_out` filters (each of shape `(C_in, f_h, f_w)`) produces an output feature map of shape `(C_out, H_o, W_o)`.\n",
    "\n",
    "### 4.3. Pooling Layer\n",
    "\n",
    "* **Operation:** A non-linear downsampling layer. Operates independently on each feature map.\n",
    "* **Types:**\n",
    "    * **Max Pooling:** Takes the maximum value from a window (e.g., 2x2). Most common.\n",
    "    * **Average Pooling:** Takes the average value from the window.\n",
    "* **Purpose:**\n",
    "    1.  Reduces the spatial dimensions ($H \\times W$) of the feature maps.\n",
    "    2.  Reduces the number of parameters and computation.\n",
    "    3.  Provides basic translation invariance.\n",
    "* **Hinton's Critique:** Pooling is a \"big mistake\" because it throws away precise spatial information, failing to capture relationships between parts (e.g., a \"face\" is just a bag of features: one nose, two eyes).\n",
    "\n",
    "### 4.4. Global Pooling\n",
    "\n",
    "* **Operation:** A pooling layer where the kernel size is equal to the entire feature map size.\n",
    "* **Types:** Global Average Pooling (GAP) or Global Max Pooling (GMP).\n",
    "* **Purpose:**\n",
    "    * Reduces a feature map of `(C, H, W)` to a vector of `(C, 1, 1)`.\n",
    "    * Often used to replace the `Flatten` and `FC` layers at the end of a CNN, drastically reducing parameters and preventing overfitting.\n",
    "\n",
    "### 4.5. ResNet (Residual Networks)\n",
    "\n",
    "* **Problem:** Very deep \"plain\" networks suffer from *degradation* (accuracy gets worse) due to vanishing gradients.\n",
    "* **Solution:** The **Residual Block** with a **skip connection**.\n",
    "* **Architecture:** The block learns a *residual* function $g(x)$ instead of the full mapping $f(x)$.\n",
    "    * **Plain block:** $h(x) = \\text{ReLU}(W_2 \\cdot \\text{ReLU}(W_1 x))$\n",
    "    * **Residual block:** $h(x) = \\text{ReLU}(\\text{MainPath}(x) + x)$\n",
    "    * The model learns $g(x) = \\text{MainPath}(x)$, so the full output is $f(x) = g(x) + x$.\n",
    "* **Benefit:** The gradient can flow directly through the identity skip connection ($\\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial x} + 1$), preventing the gradient from vanishing.\n",
    "* **1x1 Convolutions:** Used in skip connections when the input and output dimensions (channels or spatial size) do not match.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Practical Skills & Regularization\n",
    "\n",
    "### 5.1. Common Training Problems\n",
    "\n",
    "* **Gradient Vanishing:** Gradients become exponentially small in deep networks, especially with saturating activations (sigmoid, tanh). Lower layers learn very slowly or not at all.\n",
    "    * **Fix:** Use ReLU, use ResNets, use good initialization (He).\n",
    "* **Gradient Exploding:** Gradients become exponentially large, leading to unstable training (NaNs). Common in RNNs.\n",
    "    * **Fix:** Gradient Clipping (clamping gradients to a max value), use ResNets, good initialization.\n",
    "* **Internal Covariate Shift (ICS):** The distribution of each layer's inputs changes during training, as the parameters of previous layers change. This forces the layer to constantly re-adapt.\n",
    "    * **Fix:** Batch Normalization.\n",
    "* **Overfitting:** The model \"memorizes\" the training data (low training error) but fails to generalize to new data (high test error).\n",
    "    * **Fix:** Regularization (L1, L2, Dropout, Data Augmentation), Early Stopping.\n",
    "\n",
    "### 5.2. Key Techniques\n",
    "\n",
    "* **Weight Initialization:** Crucial to break symmetry and maintain good gradient flow.\n",
    "    * **Xavier/Glorot Init:** Good for `sigmoid` and `tanh`. Variance scales with $\\frac{1}{n_{in} + n_{out}}$.\n",
    "    * **He Init:** Designed for `ReLU`. Variance scales with $\\frac{1}{n_{in}}$.\n",
    "* **Batch Normalization (BN):**\n",
    "    * Normalizes the output of a layer (before activation) across the mini-batch to have zero mean and unit variance.\n",
    "    * Learns two parameters, $\\gamma$ (scale) and $\\beta$ (shift), to restore representative power.\n",
    "    * **Benefits:** Solves ICS, allows higher learning rates, smooths the loss landscape, acts as a regularizer.\n",
    "* **L1/L2 Regularization:**\n",
    "    * Adds a penalty to the loss function based on the magnitude of the weights $\\theta$.\n",
    "    * $J(\\theta) = \\text{Loss} + \\Omega(\\theta)$\n",
    "    * **L2 (Weight Decay):** $\\Omega(\\theta) = \\lambda \\sum ||W^k||^2_F$. Prefers small, diffuse weights.\n",
    "    * **L1:** $\\Omega(\\theta) = \\lambda \\sum |W^k|$. Promotes sparsity (many weights become zero).\n",
    "* **Dropout:**\n",
    "    * Regularization technique. During training, randomly \"drops\" (sets to zero) a fraction $p$ of neurons in a layer.\n",
    "    * Forces the network to learn redundant representations; prevents \"co-adaptation.\"\n",
    "    * At test time, all neurons are used (dropout is turned off).\n",
    "* **Early Stopping:**\n",
    "    * Monitor the validation loss. Stop training when the validation loss starts to increase, even if training loss is still decreasing.\n",
    "* **Data Augmentation:**\n",
    "    * Create more training data by applying realistic transformations (e.g., for images: flip, rotate, crop, color shift).\n",
    "    * Acts as a powerful regularizer, making the model invariant to these transformations.\n",
    "* **Label Smoothing:**\n",
    "    * Regularization. Changes hard one-hot labels (e.g., `[0, 1, 0]`) to soft labels (e.g., `[0.05, 0.9, 0.05]`).\n",
    "    * Prevents the model from becoming overconfident.\n",
    "* **Mixup / CutMix:**\n",
    "    * Data augmentation. Blends two images ($x_1, x_2$) and their labels ($y_1, y_2$).\n",
    "    * $\\hat{x} = \\lambda x_1 + (1-\\lambda) x_2$\n",
    "    * $\\hat{y} = \\lambda y_1 + (1-\\lambda) y_2$\n",
    "    * CutMix cuts a patch from one image and pastes it onto another.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Adversarial Attacks\n",
    "\n",
    "* **Adversarial Example:** An input $x_{adv}$ created by adding a small, human-imperceptible perturbation $\\delta$ to a clean image $x$, such that $x_{adv}$ fools the model.\n",
    "* **How it Works:** Attacks move the input $x$ in the direction that **maximizes** the loss. This is the *opposite* of training.\n",
    "* **Untargeted Attack:** Goal is to make the model predict *any* wrong class.\n",
    "    * Find $x_{adv}$ that maximizes $l(f(x'), y_{\\text{true}})$.\n",
    "    * **Fast Gradient Sign Method (FGSM):** A one-step attack.\n",
    "        * $x_{adv} = x + \\epsilon \\cdot \\text{sign}(\\nabla_x l(f(x), y))$\n",
    "* **Targeted Attack:** Goal is to make the model predict a *specific* target class $y_{\\text{target}}$.\n",
    "    * Find $x_{adv}$ that *minimizes* $l(f(x'), y_{\\text{target}})$.\n",
    "    * $x_{adv} = x - \\epsilon \\cdot \\text{sign}(\\nabla_x l(f(x), y_{\\text{target}}))$\n",
    "* **Projected Gradient Descent (PGD):** A stronger, iterative version of FGSM. Takes multiple small steps, projecting the result back into the $\\epsilon$-ball (e.g., $||x' - x||_\\infty \\le \\epsilon$) at each step.\n",
    "* **Defense: Adversarial Training:**\n",
    "    * The most effective known defense.\n",
    "    * Find adversarial examples (e.g., using PGD).\n",
    "    * Train the model on a mix of clean and adversarial examples. This makes the loss surface smoother around data points.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "### 7.1. Basic RNN\n",
    "\n",
    "* **Architecture:** A network with a \"loop.\" The hidden state $h_t$ at time $t$ is a function of the previous hidden state $h_{t-1}$ and the current input $x_t$.\n",
    "* **Equations:**\n",
    "    * $h_t = \\tanh(h_{t-1}W_h + x_t U_x + b_h)$\n",
    "    * $\\hat{y}_t = \\text{softmax}(h_t V_y + b_y)$\n",
    "* **Key Idea:** Parameters ($W_h, U_x, V_y$) are **shared** across all time steps.\n",
    "* **Problem:** Vanishing/Exploding Gradients. The gradient signal has to flow through many matrix multiplications ($W_h^T$) back in time. If $W_h$ is \"small,\" gradients vanish; if \"large,\" they explode. This makes it impossible to capture **long-term dependencies**.\n",
    "\n",
    "### 7.2. LSTM (Long Short-Term Memory)\n",
    "\n",
    "* **Solution:** Solves the long-term dependency problem with a **gating mechanism** and a separate **cell state** $c_t$ (long-term memory).\n",
    "* **Core Idea:** The network learns *when* to forget, store, and output information.\n",
    "* **Gates (all are sigmoid functions):**\n",
    "    1.  **Forget Gate ($f_t$):** Decides what to throw away from $c_{t-1}$.\n",
    "        * $f_t = \\sigma(x_t U_f + h_{t-1} W_f + b_f)$\n",
    "    2.  **Input Gate ($i_t$):** Decides what new information to store in $c_t$.\n",
    "        * $i_t = \\sigma(x_t U_i + h_{t-1} W_i + b_i)$\n",
    "        * Candidate values: $g_t = \\tanh(x_t U_g + h_{t-1} W_g + b_g)$\n",
    "    3.  **Cell State Update:**\n",
    "        * $c_t = f_t \\odot c_{t-1} + i_t \\odot g_t$ (Forget old, add new)\n",
    "    4.  **Output Gate ($o_t$):** Decides what to output as the hidden state $h_t$.\n",
    "        * $o_t = \\sigma(x_t U_o + h_{t-1} W_o + b_o)$\n",
    "        * $h_t = o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "### 7.3. GRU (Gated Recurrent Unit)\n",
    "\n",
    "* A simplified version of LSTM with fewer parameters.\n",
    "* Merges $c_t$ and $h_t$ into a single hidden state $h_t$.\n",
    "* Has two gates:\n",
    "    1.  **Update Gate ($z_t$):** Controls how much of $h_{t-1}$ to keep (like $f_t$ and $i_t$ combined).\n",
    "    2.  **Reset Gate ($r_t$):** Controls how much of $h_{t-1}$ to use when computing the candidate state.\n",
    "\n",
    "### 7.4. RNN Architectures\n",
    "\n",
    "* **Many-to-One:** Input sequence $\\rightarrow$ single output (e.g., Sentiment Analysis).\n",
    "* **One-to-Many:** Single input $\\rightarrow$ output sequence (e.g., Image Captioning).\n",
    "* **Many-to-Many (Seq2Seq):** Input sequence $\\rightarrow$ output sequence (e.g., Machine Translation).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Word2Vec\n",
    "\n",
    "* **Goal:** Learn vector representations (embeddings) for words that capture semantic meaning (e.g., $v_{\\text{King}} - v_{\\text{Man}} + v_{\\text{Woman}} \\approx v_{\\text{Queen}}$).\n",
    "* **Pretext Task:** Turns an unsupervised problem (learning from text) into a supervised one.\n",
    "* **Models:**\n",
    "    1.  **CBOW (Continuous Bag-of-Words):**\n",
    "        * **Task:** Predict a target word given its surrounding context words.\n",
    "        * (e.g., `[The, quick, fox, jumps]` $\\rightarrow$ `brown`)\n",
    "    2.  **Skip-Gram:**\n",
    "        * **Task:** Predict context words given a single target word.\n",
    "        * (e.g., `brown` $\\rightarrow$ `[The, quick, fox, jumps]`)\n",
    "* **Negative Sampling:** An optimization to avoid the expensive `softmax` over the entire vocabulary $N$.\n",
    "    * Instead of an $N$-class classification, it becomes a binary classification:\n",
    "    * Is this pair `(target, context)` a real pair (1) or a \"negative\" (randomly sampled) pair (0)?\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Advanced Sequential Models (Seq2Seq & Attention)\n",
    "\n",
    "### 9.1. Seq2Seq (Encoder-Decoder)\n",
    "\n",
    "* **Architecture:** Two RNNs (often LSTMs/GRUs).\n",
    "    1.  **Encoder:** Reads the entire input sequence (e.g., \"I love deep learning\") and compresses it into a single **context vector** $c$ (the final hidden state $h_T$).\n",
    "    2.  **Decoder:** Takes $c$ as its initial hidden state and generates the output sequence token by token (e.g., \"<BOS>\", \"J'adore\", \"le\", \"deep\", \"learning\", \"<EOS>\").\n",
    "* **Problem:** The fixed-size context vector $c$ is a **bottleneck**. It's difficult to store all information from a long sentence in one vector.\n",
    "\n",
    "### 9.2. Attention Mechanism\n",
    "\n",
    "* **Solution:** Solves the bottleneck by allowing the Decoder to look back at *all* Encoder hidden states ($h_1, ..., h_T$) at *each* step of decoding.\n",
    "* **Process (at Decoder step $j$):**\n",
    "    1.  **Query:** Get the current decoder hidden state $q_{j-1}$.\n",
    "    2.  **Keys:** Use all encoder hidden states $h_1, ..., h_T$ as keys.\n",
    "    3.  **Scores:** Compute an **alignment score** $e_j = \\text{score}(q_{j-1}, h_i)$ for each $h_i$. (e.g., $\\text{score} = q^T h$ (dot), $q^T W_a h$ (general)).\n",
    "    4.  **Weights:** Convert scores to probabilities: $\\alpha_j = \\text{softmax}(e_j)$. These $\\alpha_j$ weights show \"how much attention\" to pay to each input word.\n",
    "    5.  **Context:** Create a dynamic context vector $c_j$ as a weighted sum: $c_j = \\sum_i \\alpha_{ji} h_i$.\n",
    "    6.  **Predict:** Use $c_j$ and $q_{j-1}$ to predict the next word $y_j$.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Transformers\n",
    "\n",
    "* **Motto:** \"Attention is All You Need.\" Throws away all recurrence and convolutions, relying only on attention.\n",
    "* **Benefits:**\n",
    "    * **Parallelizable:** Computes all steps at once, unlike RNNs. Massively faster on GPUs.\n",
    "    * **Long-Range:** Path length between any two tokens is $O(1)$, perfectly capturing long-range dependencies.\n",
    "\n",
    "### 10.1. Self-Attention (Scaled Dot-Product)\n",
    "\n",
    "* The core of the Transformer. It's attention *within* a single sequence (e.g., inputs attending to other inputs).\n",
    "* **Process:**\n",
    "    1.  For each input token $x_i$, create three vectors (via learnable matrices $W_Q, W_K, W_V$):\n",
    "        * **Query ($q_i$):** \"What I am looking for.\"\n",
    "        * **Key ($k_i$):** \"What I am.\"\n",
    "        * **Value ($v_i$):** \"What I will provide.\"\n",
    "    2.  Compute attention scores by taking the dot product of every query with every key: $q_i \\cdot k_j$.\n",
    "    3.  **Matrix Form:**\n",
    "        * $Q = X W_Q$, $K = X W_K$, $V = X W_V$\n",
    "        * $\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{Q K^T}{\\sqrt{d_k}}) V$\n",
    "        * The $\\sqrt{d_k}$ scaling prevents dot products from becoming too large and killing the softmax gradient.\n",
    "\n",
    "### 10.2. Transformer Architecture\n",
    "\n",
    "* **Multi-Head Attention:** Runs self-attention $h$ times (e.g., $h=8$) in parallel with different $W_Q, W_K, W_V$ matrices. Concatenates the results.\n",
    "* **Encoder Block:**\n",
    "    1.  Multi-Head Self-Attention\n",
    "    2.  Add & Norm (Residual Connection + Layer Norm)\n",
    "    3.  Point-wise FFN\n",
    "    4.  Add & Norm\n",
    "* **Decoder Block:** Same as Encoder, but adds a *second* Multi-Head Attention block that performs **Cross-Attention** (Queries $Q$ from Decoder, Keys $K$ and Values $V$ from Encoder output).\n",
    "* **Positional Encoding:** Since there is no recurrence, sin/cos functions are added to the input embeddings to inject position information.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Vision Transformer (ViT)\n",
    "\n",
    "* **Goal:** Apply the Transformer to images.\n",
    "* **Problem:** Self-attention on $224 \\times 224$ pixels is computationally infeasible.\n",
    "* **Solution: \"An image is worth 16x16 words\"**\n",
    "    1.  **Patches:** Split the image (e.g., $224 \\times 224$) into a grid of non-overlapping 16x16 patches.\n",
    "    2.  **Flatten & Project:** Flatten each patch ($16 \\times 16 \\times 3$) into a vector and use a linear layer to project it to the model dimension $d_{\\text{model}}$.\n",
    "    3.  **[CLS] Token:** Add a learnable \"class token\" to the beginning of the sequence.\n",
    "    4.  **Positional Encoding:** Add positional encodings to the patch embeddings.\n",
    "    5.  **Transformer:** Feed this sequence of patch \"tokens\" into a standard Transformer Encoder.\n",
    "    6.  **Classify:** Use the final output corresponding to the `[CLS]` token to feed into an MLP head for classification.\n",
    "* **vs. CNNs:** ViTs lack the *inductive bias* (locality, translation invariance) of CNNs. Therefore, they require *much more* data (e.g., ImageNet-21k, JFT-300M) to learn these properties from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Model Fine-Tuning (PEFT)\n",
    "\n",
    "* **Problem:** Fine-tuning an entire large model (like ViT or BERT) on a new task is computationally expensive.\n",
    "* **Solution: Parameter-Efficient Fine-Tuning (PEFT):** Freeze the pre-trained model and only train a small number of *new* parameters.\n",
    "* **Methods:**\n",
    "    * **Adapters:** Add small, learnable FFN \"bottleneck\" modules *inside* each Transformer block.\n",
    "    * **LoRA (Low-Rank Adaptation):** Modifies a weight matrix $W$ by adding a low-rank update: $W \\rightarrow W + B \\cdot A$, where $B$ and $A$ are small, new matrices. Only $B$ and $A$ are trained.\n",
    "    * **Prompt Tuning:** Prepends learnable \"prompt\" vectors to the input sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## 13. Generative Adversarial Networks (GANs)\n",
    "\n",
    "* **Goal:** A generative model that learns to create realistic data (e.g., images).\n",
    "* **Architecture (Two-Player Game):**\n",
    "    1.  **Generator ($G$):** The \"Counterfeiter.\" Tries to create fake data ($G(z)$) from random noise $z$ that looks real.\n",
    "    2.  **Discriminator ($D$):** The \"Police.\" Tries to distinguish between real data $x$ and fake data $G(z)$.\n",
    "* **Minimax Objective:** $G$ and $D$ play a game. $D$ tries to *maximize* this function, while $G$ tries to *minimize* it.\n",
    "    * $\\min_G \\max_D J(G, D) = \\mathbb{E}_{x \\sim p_d}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$\n",
    "* **Training:**\n",
    "    1.  **Train $D$:** Freeze $G$. Show $D$ a batch of real images (labels=1) and a batch of fake images (labels=0). Update $D$ via gradient *ascent*.\n",
    "    2.  **Train $G$:** Freeze $D$. Generate a batch of fake images. Pass them to $D$ and try to fool it (labels=1). Update $G$ via gradient *descent*.\n",
    "* **Nash Equilibrium:** The ideal convergence point where $G$ produces perfect fakes ($p_g = p_d$) and $D$ is completely confused ($D(x) = 0.5$ for all $x$).\n",
    "* **Problems:**\n",
    "    * **Mode Collapse:** $G$ finds one \"good\" fake that fools $D$ and only produces that one (or a few) images, failing to capture the diversity of the data.\n",
    "    * **Hard Convergence:** The minimax training is unstable and hard to balance.\n",
    "* **Evaluation:**\n",
    "    * **Inception Score (IS):** Measures quality. Good generated images should be:\n",
    "        1.  **High-Quality:** A classifier (like InceptionNet) is confident about what it is (low entropy $p(y|x)$).\n",
    "        2.  **Diverse:** The model produces a wide variety of classes (high entropy $p(y)$)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
