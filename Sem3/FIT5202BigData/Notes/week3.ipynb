{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1e845c",
   "metadata": {},
   "source": [
    "# FIT5202 – Big Data  \n",
    "## Week 3a & 3b – Parallel Join & Parallel Outer Join\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Notation & Variable Glossary (used throughout formulas)\n",
    "\n",
    "- **N** — number of processors.\n",
    "- **P** — page size (bytes) for disk and network transfers.\n",
    "- **S** — size of table **S** in bytes.\n",
    "- **|S|** — number of records (tuples) in **S**.\n",
    "- **Si** — size (bytes) of fragment of **S** stored at processor *i* (often `S/N` under equal partitioning).\n",
    "- **|Si|** — number of records of **S** at processor *i* (often `|S|/N` under equal partitioning).\n",
    "- **Ri**, **|Ri|** — analogous to **Si**, **|Si|** but for relation **R** at processor *i*.\n",
    "- **IO** — time to read or write one page between disk and main memory.\n",
    "- **tr** — CPU time to read one record from a memory page into a tuple buffer.\n",
    "- **tw** — CPU time to write one record to a buffer (e.g., output/result buffer).\n",
    "- **th** — CPU time to hash one record during hash-join build/probe.\n",
    "- **tj** — CPU time to perform the actual join comparison/probe per record.\n",
    "- **mp** — per-page **message protocol** handling time (software stack/driver/memcopy/ack) over the network.\n",
    "- **ml** — per-page **message latency** time component (network wire + setup) on send.\n",
    "- **H** — number of records (or capacity) that the in-memory hash table can hold at once.\n",
    "- **σj** — **join selectivity ratio** (fraction of R×S that actually joins, used as a per-record factor).\n",
    "- **πR**, **πS** — **projectivity ratios** for R and S (output bytes / input record bytes after projection).\n",
    "- **M** — main-memory capacity (bytes) allocated for holding incoming data during distribution.\n",
    "\n",
    "> Tip: Think **size in bytes** (S, Si, Ri) for **I/O and network**, and **number of records** (|S|, |Si|, |Ri|) for **CPU work**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Quick Revision (from slides)\n",
    "\n",
    "**Q1.** If a query runs on a multi-core machine, it is parallel query processing. How about if multiple *different* queries run at the same time on a multi-core machine?  \n",
    "**Answer:** **Inter-query parallelism** — different queries run concurrently on different cores.\n",
    "\n",
    "**Q2.** With **hash** data partitioning, a **discrete range** search should use how many processors?  \n",
    "**Answer:** **Selected processors only** — only those owning the relevant hash buckets.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Join Operations (Inner vs Outer)\n",
    "\n",
    "- **Inner Join:** returns only matching rows.\n",
    "- **Outer Join:** returns matches **plus** unmatched rows from one or both sides (left/right/full).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Serial Join Algorithms (baseline building blocks)\n",
    "\n",
    "1) **Nested-Loop Join:** for each record of **R**, scan all records of **S**.  \n",
    "2) **Sort-Merge Join:** sort both on join key(s), then merge.  \n",
    "3) **Hash-Join:** hash-partition both on the join key, then only compare within matching buckets.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Parallel Join — Two-Stage Pattern\n",
    "\n",
    "1) **Data Partitioning** (move/slice data so matches co-locate).  \n",
    "2) **Local Join** (apply a serial algorithm per processor).\n",
    "\n",
    "### 4.1 Partitioning Styles\n",
    "\n",
    "**A. Divide & Broadcast**  \n",
    "- Divide one table into N disjoint slices; **broadcast** the other to all processors.  \n",
    "- Choose the **smaller** table to broadcast.  \n",
    "- Pro: balanced local work. Con: high network traffic for the broadcast.\n",
    "\n",
    "**B. Disjoint Partitioning (Range/Hash)**  \n",
    "- Partition **both** tables so matching keys land on the same processor.  \n",
    "- Pro: avoids global broadcast. Con: can suffer **data skew** (imbalanced work).\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Cost Models for Divide & Broadcast (with plain-English meaning)\n",
    "\n",
    "### Phase 1 — Data Loading (each processor reads its local fragment)\n",
    "\n",
    "**Scan cost**  \n",
    "$$\n",
    "\\text{Scan} \\;=\\; \\frac{S_i}{P}\\times IO\n",
    "$$  \n",
    "*Meaning:* Disk reads happen page-by-page; time is (bytes/pages) × time per page.  \n",
    "*(“**Sᵢ in bytes** divided by **page size** times **I/O cost per page**.”)*\n",
    "\n",
    "**Select cost**  \n",
    "$$\n",
    "\\text{Select} \\;=\\; |S_i|\\times (t_r + t_w)\n",
    "$$  \n",
    "*Meaning:* CPU time to fetch each record from the page and stage it for processing/output.  \n",
    "*(“**records** × (**CPU read** + **CPU write**) per record.”)*\n",
    "\n",
    "**Slide quiz (numbers):** If \\(|S|=600\\) records, 100 bytes each, \\(N=3\\):  \n",
    "\\(|S_i|=200\\) records and \\(S_i=20{,}000\\) bytes.  \n",
    "*Explanation:* \\(600/3=200\\) records; \\(200\\times100=20{,}000\\) bytes.\n",
    "\n",
    "**Slide prompt:** “If \\(|S|=30{,}000\\) records over 3 processors, when can we proceed — after reading 30,000 or 10,000?”  \n",
    "**Answer:** After each processor finishes its **local 10,000** records (the slowest processor becomes the barrier). **[Inference]** This reflects per-node local read before the broadcast phase begins.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2 — Broadcast (each processor sends its Sᵢ to all others)\n",
    "\n",
    "**Transfer (send) cost per sender**  \n",
    "$$\n",
    "\\text{Transfer} \\;=\\; \\frac{S_i}{P}\\times (N-1)\\times (m_p + m_l)\n",
    "$$  \n",
    "*Meaning:* Sender must transmit its pages to each of the other \\(N-1\\) processors; each page pays protocol + latency.  \n",
    "*(“**pages sent** × **other nodes** × (**protocol**+**latency**) per page.”)*\n",
    "\n",
    "**Receive cost per receiver**  \n",
    "$$\n",
    "\\text{Receive} \\;=\\; \\Bigl(\\frac{S}{P}-\\frac{S_i}{P}\\Bigr)\\times m_p\n",
    "$$  \n",
    "*Meaning:* A processor receives **all other fragments** of \\(S\\), and pays the per-page protocol overhead.  \n",
    "**Why \\((S/P - S_i/P)\\)?** You already have your own fragment \\(S_i\\); you only receive the **other** pages. **[Inference]**  \n",
    "**Why \\(m_p\\) only?** The model attributes **latency** \\(m_l\\) to the **sender’s** cost to avoid double-counting; receivers pay protocol handling per page but not the sender’s wire/setup latency again. **[Inference]**\n",
    "\n",
    "**Slide quiz:** “The cost to read data from **disk** is called… ?” → **Scan cost** (not Select; Select is CPU-side).\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3 — Store received fragments locally\n",
    "\n",
    "**Store cost**  \n",
    "$$\n",
    "\\text{Store} \\;=\\; \\Bigl(\\frac{S}{P}-\\frac{S_i}{P}\\Bigr)\\times IO\n",
    "$$  \n",
    "*Meaning:* Write the newly received pages of \\(S\\) to local disk; you don’t rewrite your own \\(S_i\\).  \n",
    "**Why \\((S/P - S_i/P)\\)?** Only the **non-local** pages must be stored. **[Inference]**\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Local Join (Hash-Join shown; same pattern for other joins)\n",
    "\n",
    "### Phase 1 — Load Rᵢ and S\n",
    "\n",
    "**Scan**  \n",
    "$$\n",
    "\\text{Scan} \\;=\\; \\Bigl(\\frac{R_i}{P}+\\frac{S}{P}\\Bigr)\\times IO\n",
    "$$\n",
    "\n",
    "**Select**  \n",
    "$$\n",
    "\\text{Select} \\;=\\; (|R_i|+|S|)\\times (t_r+t_w)\n",
    "$$\n",
    "\n",
    "*Meaning (both):* Disk brings in pages for \\(R_i\\) and \\(S\\); CPU stages each record for hashing/probing.\n",
    "\n",
    "### Phase 2 — Build/Probe + Join\n",
    "\n",
    "**Join CPU cost**  \n",
    "$$\n",
    "\\text{Join} \\;=\\; |R_i|\\times (t_r+t_h)\\;+\\;|S|\\times (t_r+t_h+t_j)\n",
    "$$  \n",
    "*Meaning:* Build phase (read+hash) for \\(R_i\\); probe phase (read+hash+join comparison) for \\(S\\).\n",
    "\n",
    "**Overflow (bucket spill) I/O cost**  \n",
    "$$\n",
    "\\text{Overflow} \\;=\\; \\Bigl(1-\\min\\!\\bigl(\\tfrac{H}{|R_i|},1\\bigr)\\Bigr)\\times \\frac{R_i}{P}\\times 2\\times IO\n",
    "$$  \n",
    "*Meaning:* If the in-memory hash can’t hold all build tuples, extra I/O occurs: **write** overflow buckets then **read** them back (hence ×2).\n",
    "\n",
    "### Phase 3 — Write results\n",
    "\n",
    "**Generate results (CPU)**  \n",
    "$$\n",
    "\\text{Gen} \\;=\\; |R_i|\\times \\sigma_j \\times |S|\\times t_w\n",
    "$$  \n",
    "*Meaning:* Amount of output work scales with join selectivity \\(\\sigma_j\\).\n",
    "\n",
    "**Store results (disk)**  \n",
    "$$\n",
    "\\text{Result I/O} \\;=\\; \\frac{\\pi_R\\times |R_i|\\times \\sigma_j \\times \\pi_S\\times |S|}{P}\\times IO\n",
    "$$  \n",
    "*Meaning:* Projected output bytes divided by page size, times I/O per page.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Memory & Load-Balancing Optimizations\n",
    "\n",
    "**Cut redundant I/O using memory \\(M\\)**  \n",
    "If incoming distributed data can be kept in RAM through the local join, you avoid one write+read cycle:  \n",
    "$$\n",
    "\\text{Saved I/O per node} \\approx \\frac{M}{P}\\times IO\n",
    "$$  \n",
    "*Meaning:* Keep up to \\(M\\) bytes resident; you skip scanning that many bytes from disk later. **[Inference]**\n",
    "\n",
    "**Skew handling**  \n",
    "- Create **more** fragments than processors and **repack** to balance work.  \n",
    "- With **Divide & Broadcast**, load is even; with **Disjoint** partitioning, watch for skew.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Parallel **Outer** Join Algorithms\n",
    "\n",
    "### 8.1 ROJA — Redistribution Outer Join Algorithm\n",
    "**Steps:** (1) redistribute both R and S on the join key; (2) local **outer** join.  \n",
    "**Pros:** simple, only two steps. **Cons:** network cost + potential skew due to redistribution.\n",
    "\n",
    "### 8.2 DOJA — Duplication Outer Join Algorithm\n",
    "**Steps:** (1) replicate small table; (2) local **inner** join; (3) hash-redistribute inner-join result on attribute X; (4) local **outer** join.  \n",
    "**Cons:** expensive if the “small” table isn’t actually small enough.\n",
    "\n",
    "### 8.3 DER — Duplication & Efficient Redistribution\n",
    "**Steps:** (1) broadcast **left** table; (2) local **inner** join; (3) determine **ROW IDs** of unmatched left rows; (4) redistribute **only ROW IDs**; (5) replicate ROW IDs as needed; (6) final **inner** join to attach NULLs correctly.  \n",
    "**Pros:** ships **IDs** instead of full rows (lighter than DOJA). **Cons:** still pays replication if the left is large.\n",
    "\n",
    "### 8.4 OJSO — Outer Join Skew Optimization\n",
    "- Do **not** redistribute dangling (unmatched) records from the previous outer join stage.  \n",
    "- Process them **locally** when possible to avoid hot spots and redundant traffic.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Slide Questions (inserted at their relevant spots)\n",
    "\n",
    "- **Identify the LEFT OUTER JOIN (diagram question).**  \n",
    "  **Answer:** The result that preserves **all left rows** and shows **NULLs** for non-matching right attributes (e.g., `(2,3,NULL,NULL)` in the example) is the **left outer join**. **[Inference]**\n",
    "\n",
    "- **Why do we need to *redistribute* R and S first (multi-join example R ⟕ S ⟕ T)?**  \n",
    "  **Answer:** To **co-locate matching keys** on the same processor; initial placement is not guaranteed to be co-partitioned on the join attributes, so joining without redistribution would miss matches or force remote lookups. **[Inference]**\n",
    "\n",
    "- **Why might a shared-memory system still avoid hash-join after broadcast?**  \n",
    "  **Answer:** If each processor lacks **enough working memory** to hold the hash table (build side), the local algorithm may have to switch away from pure in-memory hash-join or incur heavy spilling. **[Inference]**\n",
    "\n",
    "- **Broadcast Receive formula prompts:**  \n",
    "  – **Why \\((S/P - S_i/P)\\)?** You only **receive others’ pages**, not your own local pages. **[Inference]**  \n",
    "  – **Why \\(m_p\\) only?** To avoid **double-counting latency**; the sender’s transfer cost already accounts for per-page latency \\(m_l\\), while the receiver pays protocol handling \\(m_p\\). **[Inference]**\n",
    "\n",
    "- **Store cost prompt:**  \n",
    "  – **Why \\((S/P - S_i/P)\\)?** You **store** only the **received** (non-local) pages, not your own. **[Inference]**\n",
    "\n",
    "- **“Disk cost” naming quiz:**  \n",
    "  – Reading from disk is **Scan cost** (I/O). CPU extraction from pages is **Select cost** (no disk).  \n",
    "\n",
    "---\n",
    "\n",
    "## 10) Summary\n",
    "\n",
    "- Parallel joins = **Partition** then **Local join**.  \n",
    "- **Divide & Broadcast** vs **Disjoint** partitioning drives both **network** and **I/O** patterns.  \n",
    "- Cost model split:  \n",
    "  - **Bytes/pages** → I/O & network terms (use \\(S, S_i, R_i\\)).  \n",
    "  - **Records** → CPU terms (use \\(|S|, |S_i|, |R_i|\\)).  \n",
    "- Outer joins: **ROJA, DOJA, DER**; handle skew with **OJSO**.  \n",
    "- Optimize by **reducing I/O**, **avoiding unnecessary redistribution**, and **balancing load**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
