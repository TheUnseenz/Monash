{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# FIT5202: Data Processing for Big Data (Revision Notes)\n",
    "\n",
    "This unit was divided into three main pillars: Volume, Complexity, and Velocity.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Volume: Processing Large-Scale Data (Sessions 1-4)\n",
    "\n",
    "This section focuses on the strategies and challenges of processing massive, stored datasets using parallel systems.\n",
    "\n",
    "### üìà Key Performance Metrics\n",
    "\n",
    "* **Speed Up**: Measures how much faster a task runs on a multiprocessor system. The goal is to run a given task in less time by adding more processors.\n",
    "    * **Formula**: $Speed~Up = \\frac{Elapsed~Time~(Uniprocessor)}{Elapsed~Time~(Multiprocessors)}$ \n",
    "    * **Linear Speed Up**: Performance scales perfectly with resources (e.g., 4 processors = 4x faster).\n",
    "    * **Sub-Linear Speed Up**: Performance gain is *less* than the resources added. This is the most common outcome.\n",
    "    * **Super-Linear Speed Up**: Performance gain is *more* than the resources added (rare).\n",
    "\n",
    "* **Scale Up**: Measures the ability to handle a larger task in the *same amount of time* by proportionally increasing resources.\n",
    "    * **Formula**: $Scale~Up = \\frac{Elapsed~Time~(Small~System,~Small~Data)}{Elapsed~Time~(Large~System,~Large~Data)}$ \n",
    "    * **Linear Scale Up**: Achieved if Scale Up = 1. This means you can double the data, double the resources, and the time stays the same.\n",
    "\n",
    "### üöß Obstacles to Parallelism\n",
    "\n",
    "Achieving perfect linear speed up is difficult due to several overheads:\n",
    "\n",
    "* **Start-up & Consolidation**: The cost of initiating all parallel processes and the cost of collecting the final results from all processors.\n",
    "* **Interference & Communication**: Processors compete for shared resources (interference)  or must wait for other processors to be ready (communication).\n",
    "\n",
    "#### Example: Calculating Sub-Linear Speed Up\n",
    "A job takes 1 hour (60 min) on 1 processor. The job has a 10% serial part and a 90% parallel part. We use 4 processors, which have a 20% overhead (e.g., waiting time).\n",
    "\n",
    "1.  **Serial Time**: 60 min * 10% = **6 min** (This cannot be parallelized).\n",
    "2.  **Parallel Time (Ideal)**: 60 min * 90% = 54 min. With 4 processors: 54 / 4 = **13.5 min**.\n",
    "3.  **Parallel Time (Actual)**: Add 20% overhead: 13.5 min * 1.20 = **16.2 min**.\n",
    "4.  **Total Time**: 6 min (Serial) + 16.2 min (Parallel) = **22.2 min**.\n",
    "5.  **Speed Up**: 60 min / 22.2 min = **2.7**.\n",
    "6.  **Result**: Since 2.7 is less than the 4 processors used, this is **Sub-Linear Speed Up**.\n",
    "\n",
    "### üå™Ô∏è Data Skew\n",
    "\n",
    "Skew is the uneven distribution of data  or processing time  across processors. It's a major obstacle, as the total job time is limited by the single most-overloaded processor.\n",
    "\n",
    "* This is often modeled by the **Zipf distribution**.\n",
    "* A skew degree of $\\theta=0$ is a perfectly uniform distribution (no skew), while $\\theta=1$ is highly skewed.\n",
    "\n",
    "### üîç Parallel Search\n",
    "\n",
    "Parallel search involves two steps: partitioning the data and then searching it.\n",
    "\n",
    "**1. Data Partitioning Methods** \n",
    "* **Round-Robin**: Distributes records evenly. Pro: Good load balancing. Con: No semantic grouping (a query might need all processors).\n",
    "* **Hash Partitioning**: Groups data using a hash function. Pro: Good for exact match queries (only 1 processor needed). Con: Can cause data skew.\n",
    "* **Range Partitioning**: Groups data based on a range of values. Pro: Efficient for range queries (only selected processors needed). Con: Can easily cause data skew.\n",
    "\n",
    "**2. Parallel Search Algorithm Components** \n",
    "* **Processor Activation**: How many processors to use. This depends on the partitioning and query type (e.g., an exact-match query on hash-partitioned data only needs 1 processor).\n",
    "* **Local Search Method**: The algorithm used on each processor. Use **Binary Search** for ordered data, **Linear Search** for unordered data.\n",
    "* **Key Comparison**: When to stop searching. Stop on a found match only if the query is an **Exact Match** and the attribute values are **Unique**.\n",
    "\n",
    "### ‚õìÔ∏è Parallel Join\n",
    "\n",
    "Parallel joins also consist of a data partitioning phase and a local join phase.\n",
    "\n",
    "* **Partitioning (Divide & Broadcast)**: The larger table is divided and split among processors. The *smaller* table is broadcast (replicated) to *all* processors.\n",
    "* **Local Join**: Each processor joins its partition of the large table with its full copy of the small table. The most common local join is the **Hash Join** (build a hash table with one table, probe it with the other).\n",
    "\n",
    "**Parallel Outer Joins** \n",
    "* **ROJA (Redistribution)**: Reshuffles both tables based on the join attribute, then performs a local outer join.\n",
    "* **DOJA (Duplication)**: Duplicates the small table, performs a local *inner* join, then redistributes the results to perform the outer join logic.\n",
    "* **DER (Duplication & Efficient Redistribution)**: Duplicates the left table, performs a local inner join, but then *only* redistributes the ROW IDs of unmatched tuples (more efficient).\n",
    "\n",
    "### üìä Parallel Sort & GroupBy\n",
    "\n",
    "**Parallel External Sort**\n",
    "Used when data is too large to fit in memory.\n",
    "\n",
    "* **Serial External Sort-Merge**:\n",
    "    1.  **Pass 0 (Sort)**: Read data in chunks that fit in memory (buffers), sort these chunks, and write them back to disk as sorted \"subfiles\".\n",
    "    2.  **Pass 1+ (Merge)**: Perform a \"k-way merge\" on the subfiles. The number 'k' is (buffers - 1). Repeat until only one sorted file remains.\n",
    "* **Parallel Partitioned Sort**: The most effective method.\n",
    "    1.  **Partitioning**: Perform a \"Range Redistribution\" to send all data within a certain range to a specific processor.\n",
    "    2.  **Local Sort**: Each processor sorts its local data.\n",
    "    3.  **Result**: The data is now globally sorted. **No final merge is needed**. The main problem is that the partitioning step can cause skew.\n",
    "\n",
    "**Parallel GroupBy**\n",
    "* **Two-Phase Method**: 1. Perform local aggregation on each processor. 2. Redistribute the local results. 3. Perform a final global aggregation.\n",
    "* **Redistribution Method**: 1. Redistribute the *raw* records based on the GroupBy attribute. 2. Perform local aggregation. This is simpler but can be skewed.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Complexity: Machine Learning (Sessions 5-8)\n",
    "\n",
    "This section focuses on applying machine learning algorithms to big data.\n",
    "\n",
    "### ü§ñ ML Pipeline\n",
    "\n",
    "A typical ML process follows these steps: **Training Data** $\\rightarrow$ **Featurization** $\\rightarrow$ **Training** $\\rightarrow$ **Model** $\\rightarrow$ **Model Evaluation**.\n",
    "\n",
    "**Featurization** is the process of converting raw data into numerical features:\n",
    "* **Extraction**: Creating features (e.g., TF-IDF, Word2Vec).\n",
    "* **Transformation**: Modifying features (e.g., Tokenization, Stop Words Removal, One Hot Encoding).\n",
    "* **Selection**: Choosing a subset of features (e.g., Vector Slicer).\n",
    "\n",
    "### üè∑Ô∏è Supervised Learning\n",
    "\n",
    "The data has associated **labels** , and the goal is to predict the label for new data.\n",
    "* **Classification**: Predicts a category (e.g., \"dog\" or \"not dog\").\n",
    "* **Regression**: Predicts a continuous value.\n",
    "\n",
    "#### Decision Trees (ID3)\n",
    "This algorithm builds a tree by repeatedly splitting the data.\n",
    "1.  At each node, calculate the **Information Gain (IG)** for every attribute.\n",
    "2.  **IG** is the change in **Entropy** (measure of uncertainty) from splitting on that attribute.\n",
    "3.  The attribute with the **highest IG** is chosen as the splitting node.\n",
    "4.  This process repeats until all data in a leaf node belongs to the same class.\n",
    "\n",
    "#### Parallel Decision Trees\n",
    "* **Data Parallelism (Intra-Node)**: All processors work on the *same node*. Each processor computes the IG for a *subset of attributes* (vertical partitioning).\n",
    "* **Result Parallelism (Inter-Node)**: Different processors work on *different nodes* at the same level of the tree concurrently.\n",
    "\n",
    "### üß© Unsupervised Learning\n",
    "\n",
    "The data has **no labels**. The goal is to find hidden structure.\n",
    "* **Clustering**: Groups data into clusters (e.g., K-Means).\n",
    "* **Association**: Finds relationships (e.g., \"people who buy X also buy Y\").\n",
    "\n",
    "#### K-Means Clustering\n",
    "An iterative algorithm to group data into *k* clusters.\n",
    "1.  **Initialize**: Randomly choose *k* initial cluster centroids.\n",
    "2.  **Assignment Step**: Assign each data point to its closest centroid.\n",
    "3.  **Update Step**: Recalculate each centroid to be the mean of all points assigned to it.\n",
    "4.  **Repeat**: Continue steps 2 and 3 until the cluster memberships stop changing.\n",
    "\n",
    "#### Parallel K-Means\n",
    "* **Data Parallelism**: Each processor clusters its own partition of data. The final clusters are then united.\n",
    "* **Result Parallelism**: Each processor is responsible for *one* of the *k* target clusters. This requires data movement between processors as points change cluster membership.\n",
    "\n",
    "### ü§ù Collaborative Filtering (CF)\n",
    "\n",
    "A common method for building recommendation systems.\n",
    "\n",
    "* **User-Based CF**: Recommends items based on users with similar tastes.\n",
    "    1.  Calculate the **similarity** (e.g., Cosine Similarity) between the target user and all other users.\n",
    "    2.  Predict the target user's rating for an item based on a **weighted average** of the ratings from the *most similar* users.\n",
    "    3.  Recommend the items with the highest predicted ratings.\n",
    "\n",
    "* **Model-Based CF (ALS)**: Uses **Matrix Factorization** to learn \"latent factors\" (hidden preferences) for users and items.\n",
    "    * It factors the large **Rating Matrix (R)** into a smaller **User Matrix (U)** and **Item Matrix (V)**.\n",
    "    * **Alternating Least Squares (ALS)**: An algorithm that finds U and V by minimizing the error between the *original* ratings and the *predicted* ratings (from U * V). It \"alternates\" by fixing U to solve for V, then fixing V to solve for U, repeating until stable.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Velocity: Processing Fast Data (Sessions 9-11)\n",
    "\n",
    "This section focuses on handling real-time, continuous, and unbounded data streams.\n",
    "\n",
    "### üåä Stream Processing Basics\n",
    "\n",
    "* **Data Stream**: A real-time, continuous, ordered, and unbounded sequence of items.\n",
    "* **Challenge**: Data is infinite, so we can't store it all. We must process it in one pass using **sliding windows**.\n",
    "* **Window Types**:\n",
    "    * **Time-Based**: A fixed time duration (e.g., \"all data from the last 5 seconds\").\n",
    "    * **Tuple-Based**: A fixed number of items (e.g., \"the last 100 tuples\").\n",
    "* **Window Movement**:\n",
    "    * **Overlapping (Sliding)**: The window slides by an increment *less than* its size.\n",
    "    * **Non-Overlapping (Tumbling)**: The window slides by an increment *equal to* its size.\n",
    "* **Event Time vs. Processing Time**:\n",
    "    * **Event Time**: The timestamp when the data was generated at the source.\n",
    "    * **Processing Time**: The timestamp when the data arrived at the processing server.\n",
    "    * In the real world, **Event Time is always earlier** than Processing Time due to network delays.\n",
    "\n",
    "### ‚ö° Stream Joins\n",
    "\n",
    "Joining two or more unbounded streams is difficult due to timing.\n",
    "\n",
    "* **Symmetric Hash Join**: The solution to out-of-order arrivals.\n",
    "    * A simple hash join fails if a tuple `r` arrives before its matching tuple `s`.\n",
    "    * A symmetric join maintains **two hash tables**, one for each stream (R and S).\n",
    "    * When `r` arrives, it **probes** Table S for matches, then is **inserted** into Table R for future `s` tuples to find.\n",
    "* **M-Join**: An extension of the symmetric hash join for **more than two streams**. An arriving tuple probes the hash tables of *all other* streams before being inserted into its *own* table.\n",
    "* **Handshake Join**: A conceptual join where two streams \"handshake\" as they pass. The main problem is that tuples can \"miss\" each other. Solutions involve adding empty \"slots\"  or performing multiple handshakes before moving.\n",
    "\n",
    "### üî¨ Granularity Reduction\n",
    "\n",
    "This is the process of aggregating data to a lower level of detail (e.g., from raw data \"level-0\" to an aggregated \"level-1\").\n",
    "\n",
    "* **Moving Average vs. Granularity Reduction**:\n",
    "    * **No Reduction (Rolling Mean)**: Using an *overlapped window* where the slide is 1 record (e.g., a 6-month window sliding 1 month at a time). The number of data points remains the same; the data is just smoothed. This is **Case A: Overlapped Windows - No granularity reduction**.\n",
    "    * **With Reduction**: Using a *non-overlapped (tumbling) window* or an *overlapped window with a slide > 1*. This results in fewer data points.\n",
    "\n",
    "* **Mixed Levels of Granularity**: Combining different granularities, often to allow \"drill-down\" analysis.\n",
    "    * **Temporal-based**: Based on time (e.g., 1-hour granularity at night, 10-minute during the day).\n",
    "    * **Spatial-based**: Based on location (e.g., average by state, but drill-down to see individual cities).\n",
    "\n",
    "### üì° Sensor Arrays\n",
    "\n",
    "A group of distributed sensors working together.\n",
    "\n",
    "* **Category 1: Measuring the SAME Thing** (e.g., 3 weather stations for one city).\n",
    "    * **Method 1: Reduce then Merge**: First, find the 1-hour average for each station. Second, average those averages.\n",
    "    * **Method 2: Merge then Reduce**: First, average the raw data from all 3 stations. Second, find the 1-hour average of that merged stream.\n",
    "\n",
    "* **Category 2: Measuring DIFFERENT Things** (e.g., indoor sensors for Air Quality, Temperature, and Humidity).\n",
    "    * These streams **must be normalized** to a common scale (e.g., a \"Room Quality Score\" from 1-5) before they can be merged.\n",
    "    * **Method 1: Reduce, Normalize, then Merge**.\n",
    "    * **Method 2: Normalize, Merge, then Reduce**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
