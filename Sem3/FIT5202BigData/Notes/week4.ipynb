{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# FIT5202 — Week 4a & 4b Notes (Parallel Sort & GroupBy)\n",
    "\n",
    "---\n",
    "\n",
    "## Week 4a — Parallel Sort\n",
    "\n",
    "### Internal vs External Sorting\n",
    "- **Internal sort**: data fits in RAM. Examples: Bubble Sort, Quick Sort.\n",
    "- **External sort**: data too large for RAM → use **sort-merge**:\n",
    "  1. Form runs (Pass 0) - Break the file up into unsorted subfiles, then sort the subfiles.\n",
    "  2. Repeatedly merge runs until sorted.\n",
    "\n",
    "---\n",
    "\n",
    "### Serial External Sort (examples from slides)\n",
    "\n",
    "- **Initial runs (Example 1)**  \n",
    "  $$\n",
    "  \\#\\text{runs} = 108/5 = 22\n",
    "  $$\n",
    "  With 108 pages and 5 buffer pages, we form 22 sorted runs.\n",
    "\n",
    "- **Merging buffers**  \n",
    "  $$\n",
    "  \\text{input buffers} = B-1,\\quad \\text{output buffers} = 1\n",
    "  $$\n",
    "  With $B=5$ buffers we can merge 4 runs at a time.\n",
    "\n",
    "- **Pass count (Example 1)**  \n",
    "  $$\n",
    "  \\text{passes} = 4\n",
    "  $$\n",
    "  → Pass 0 (run generation) + 3 merge passes.\n",
    "\n",
    "- **Buffer size effect (Example 2)**  \n",
    "  $$\n",
    "  4\\times 8 + 6 = 38\\ \\text{pages}\n",
    "  $$\n",
    "  Shows how run merging arithmetic works when $R=150$ pages and $B=8$ buffers.\n",
    "\n",
    "**Variable notes**  \n",
    "- $B$: number of buffer pages in memory.  \n",
    "- “page”: disk page (fixed-size I/O unit).  \n",
    "- “run”: sorted subfile created in Pass 0.  \n",
    "- “pass”: one full sweep/merge phase of the runs.\n",
    "\n",
    "---\n",
    "\n",
    "### Parallel External Sort — Algorithms\n",
    "\n",
    "- **Parallel Merge-All Sort**: each node sorts locally, then one node merges *everything*.\n",
    "  - *Issue*: bottleneck at the coordinator, high network contention.\n",
    "\n",
    "- **Parallel Binary-Merge Sort**: pairwise merges in a pipeline across processors.\n",
    "  - *Issue*: longer pipeline, more passes.\n",
    "\n",
    "- **k-way vs Binary merging**:\n",
    "  - **k-way**: compare $k$ runs at once (requires many open files/buffers).\n",
    "  - **Binary**: compare 2 at a time (longer merge sequence).\n",
    "\n",
    "- **Other methods listed**: Redistribution Binary-Merge, Redistribution Merge-All, Partitioned Sort.\n",
    "\n",
    "---\n",
    "\n",
    "### Thought-provoking questions (Sort)\n",
    "\n",
    "- **Q: Why is Merge-All a bottleneck?**  \n",
    "  **A:** Because all intermediate results and network transfers funnel into one node, causing a hotspot.\n",
    "\n",
    "- **Q: When is Binary Merge preferable to k-way?**  \n",
    "  **A:** When memory is too small to hold $k$ input buffers or open many runs simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## Week 4b — Parallel GroupBy\n",
    "\n",
    "### Serial GroupBy (hashing in memory)\n",
    "- Records are hashed into a hash table keyed by group attribute.\n",
    "- Aggregates (e.g. COUNT, SUM) are updated bucket by bucket.\n",
    "\n",
    "---\n",
    "\n",
    "### Parallel GroupBy — Methods\n",
    "\n",
    "1. **Traditional “Merge-All”**  \n",
    "   - Local partial aggregates first.  \n",
    "   - One coordinator gathers and merges everything.  \n",
    "   - *Limitations*: single node does final work, network bottleneck, no parallelism in global stage.\n",
    "\n",
    "   **Exercise (slide question): What are its limitations?**  \n",
    "   **Answer:** All of the above — bottleneck, single node workload, no parallelism.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Two-Phase Method**  \n",
    "   - **Phase 1**: local aggregate on each processor.  \n",
    "   - **Phase 2**: redistribute these condensed results by group key, then final aggregation in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Redistribution Method**  \n",
    "   - **Step 1**: redistribute **raw** tuples by group key (partitioning).  \n",
    "   - **Step 2**: each processor aggregates its partition.\n",
    "\n",
    "   **Slide prompt: “What is the problem here?”**  \n",
    "   **Answer:** Skew / load imbalance — some partitions receive many more keys/tuples.\n",
    "\n",
    "   **Load-balancing fix**:  \n",
    "   - Over-partition into more buckets than processors.  \n",
    "   - Allow *task stealing*: idle processors take buckets from overloaded ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary rule of thumb (from lecture)\n",
    "- **Two-Phase**: good when number of groups is **small** (local aggregation shrinks data a lot before shuffle).  \n",
    "- **Redistribution**: good when number of groups is **large** (local aggregation shrinks little, direct partitioning distributes work more evenly).\n",
    "\n",
    "---\n",
    "\n",
    "### Thought-provoking questions (GroupBy)\n",
    "\n",
    "- **Q: Why Two-Phase for few groups, but Redistribution for many groups?**  \n",
    "  **A:** Few groups → heavy reduction locally, so network cost is small and final aggregation balanced.  \n",
    "  Many groups → little reduction locally, so better to partition raw data evenly from the start.\n",
    "\n",
    "- **Q: Where can super-linear speedup occur?**  \n",
    "  **A:** If parallelization lets each processor’s hash table fit in memory (while serial spills), I/O is avoided and speedup > N.\n",
    "\n",
    "- **Q: True/False: Redistribution has a load-balancing option (task stealing), Two-Phase has no load-balancing problem.**  \n",
    "  **A:** False. Redistribution explicitly uses task stealing, but Two-Phase can still face skew if group key distribution is uneven.\n",
    "\n",
    "---\n",
    "\n",
    "## Quick glossary (this week’s variables)\n",
    "\n",
    "- $B$: number of buffer pages (for sort).  \n",
    "- “page”: disk I/O unit.  \n",
    "- “run”: sorted subfile created during external sort.  \n",
    "- “pass”: one complete round of sorting/merging.  \n",
    "- $k$: degree of k-way merge.  \n",
    "- “group key”: attribute(s) in `GROUP BY`.  \n",
    "- “local aggregation”: per-processor partial grouping.  \n",
    "- “redistribution”: network shuffle by group key.  \n",
    "- “task stealing”: reassigning buckets to balance load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3029144",
   "metadata": {},
   "source": [
    "Sorting and Serial Sorting\n",
    "Serial Sorting - Internal\n",
    "- Data fits entirely into main memory\n",
    "    - Bubble sort\n",
    "    - Insertion sort\n",
    "    - Quick sort\n",
    "\n",
    "Serial Sorting - External\n",
    "- Data does NOT fit entirely into main memory\n",
    "\n",
    "Sort by is to sort a result by list of column - allows duplication\n",
    "Group by is for you to create a unique combination of group of columns - unique columns only"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
