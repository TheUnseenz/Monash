{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# FIT5202 – Big Data  \n",
    "## Week 4a & 4b – Parallel Sort & Parallel GroupBy\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Notation & Variable Glossary (used throughout formulas)\n",
    "\n",
    "- **N** — number of processors.\n",
    "- **P** — page size in bytes.\n",
    "- **R** — size of relation R in bytes.\n",
    "- **|R|** — number of records in relation R.\n",
    "- **Ri** — fragment of R at processor *i* (bytes).\n",
    "- **|Ri|** — number of records in Ri.\n",
    "- **IO** — time to read/write one page from/to disk.\n",
    "- **tr** — CPU time to read one record from memory into a tuple buffer.\n",
    "- **tw** — CPU time to write one record to memory/output.\n",
    "- **mp** — per-page network message protocol cost.\n",
    "- **ml** — per-page network message latency cost.\n",
    "- **ts** — CPU time to sort one record in memory.\n",
    "- **B** — number of available memory buffers per processor (pages).\n",
    "- **G** — number of groups after grouping (GroupBy result cardinality).\n",
    "- **σg** — group selectivity ratio (fraction of records producing distinct groups).\n",
    "- **πR** — projectivity ratio for R after grouping.\n",
    "- **M** — main memory available for sort or aggregation (bytes).\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Parallel Sort – Overview\n",
    "\n",
    "### Goal\n",
    "Sort a large relation R using **multiple processors** to speed up.\n",
    "\n",
    "### High-level steps (common pattern)\n",
    "1. **Data partitioning** – so each processor gets a slice of the data for its sorting responsibility.\n",
    "2. **Local sort** – each processor sorts its data in memory (with external sort if needed).\n",
    "3. **Merge** – combine sorted partitions into final global order.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Partitioning Methods in Parallel Sort\n",
    "\n",
    "### 2.1 Range Partitioning\n",
    "- Decide split points in key space.\n",
    "- Each processor gets records in its range.\n",
    "- **Problem:** *Data skew* if key distribution is uneven.\n",
    "\n",
    "### 2.2 Hash Partitioning\n",
    "- Hash the key to assign to processors.\n",
    "- Produces balanced load if hash is uniform.\n",
    "- **Problem:** Output is not globally sorted; requires extra merging.\n",
    "\n",
    "### 2.3 Sampling for Range Partition Boundaries\n",
    "- Randomly sample keys → sort sample → pick split points so partitions are balanced.\n",
    "- Avoids skew from naive ranges.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Parallel Sort Cost Model (Range-partition-based)\n",
    "\n",
    "### Phase 1 – Local Sort\n",
    "\n",
    "**Scan cost**\n",
    "$$\n",
    "\\frac{R_i}{P} \\times IO\n",
    "$$  \n",
    "*Meaning:* Read the local fragment from disk into memory.  \n",
    "\n",
    "**Select cost**\n",
    "$$\n",
    "|R_i| \\times (t_r + t_w)\n",
    "$$  \n",
    "*Meaning:* CPU read/write of all records in memory buffers.  \n",
    "\n",
    "**Sort cost**\n",
    "$$\n",
    "|R_i| \\times t_s\n",
    "$$  \n",
    "*Meaning:* CPU cost to sort all records locally.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2 – Redistribution (send sorted partitions to correct processors)\n",
    "\n",
    "**Transfer cost**\n",
    "$$\n",
    "\\frac{R_i}{P} \\times (N-1) \\times (m_p + m_l)\n",
    "$$  \n",
    "*Meaning:* Each processor sends part of its data to all other processors (N-1 destinations).\n",
    "\n",
    "**Receive cost**\n",
    "$$\n",
    "\\left( \\frac{R}{P} - \\frac{R_i}{P} \\right) \\times m_p\n",
    "$$  \n",
    "*Meaning:* Receive all pages from others, excluding your own.\n",
    "\n",
    "**Slide question:** *Why `(mp)` only for receive cost?*  \n",
    "**Answer:** Latency `ml` is charged at sender side; receiver only pays protocol handling per page.  \n",
    "**Reasoning:** Avoid double-counting network latency.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3 – Merge locally received partitions\n",
    "\n",
    "**Scan cost**\n",
    "$$\n",
    "\\frac{R}{P} \\times IO\n",
    "$$  \n",
    "*Meaning:* Read all your received data pages from disk.\n",
    "\n",
    "**Merge CPU cost**\n",
    "$$\n",
    "|R| \\times (t_r + t_w)\n",
    "$$  \n",
    "*Meaning:* Read & write all records while merging.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Parallel Sort Optimizations\n",
    "\n",
    "- **Main memory merge**: Keep incoming partitions in memory to skip merge I/O if M is large enough.\n",
    "- **Balanced partitioning**: Use sampling to avoid skew.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Parallel GroupBy – Overview\n",
    "\n",
    "### Goal\n",
    "Aggregate data into groups in parallel.\n",
    "\n",
    "### High-level steps\n",
    "1. **Partition by group key** – hash or range.\n",
    "2. **Local aggregation** – aggregate per processor.\n",
    "3. **Merge partial aggregates** – final group results.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Parallel GroupBy Cost Model (Hash-based)\n",
    "\n",
    "### Phase 1 – Data Loading\n",
    "\n",
    "**Scan cost**\n",
    "$$\n",
    "\\frac{R_i}{P} \\times IO\n",
    "$$  \n",
    "*Meaning:* Read local fragment from disk.\n",
    "\n",
    "**Select cost**\n",
    "$$\n",
    "|R_i| \\times (t_r + t_w)\n",
    "$$  \n",
    "*Meaning:* CPU read/write from memory buffers.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2 – Redistribution by Group Key\n",
    "\n",
    "**Transfer cost**\n",
    "$$\n",
    "\\frac{R_i}{P} \\times (N-1) \\times (m_p + m_l)\n",
    "$$  \n",
    "*Meaning:* Send group-key partitions to all other processors.\n",
    "\n",
    "**Receive cost**\n",
    "$$\n",
    "\\left( \\frac{R}{P} - \\frac{R_i}{P} \\right) \\times m_p\n",
    "$$  \n",
    "*Meaning:* Receive other processors’ partitions.  \n",
    "\n",
    "**Slide question:** *Why might we only send group key and aggregate value during redistribution?*  \n",
    "**Answer:** To reduce data volume.  \n",
    "**Reasoning:** Full tuples are not needed for final aggregation; only key and partial aggregate suffice.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3 – Local Aggregation of Received Data\n",
    "\n",
    "**CPU aggregation cost**\n",
    "$$\n",
    "|R| \\times (t_r + t_w)\n",
    "$$  \n",
    "*Meaning:* Read and combine partial aggregates.\n",
    "\n",
    "**Output size**\n",
    "$$\n",
    "G \\times \\text{(size per group)}\n",
    "$$  \n",
    "*Meaning:* Final output depends on number of groups G.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Parallel GroupBy Optimizations\n",
    "\n",
    "- **Pre-aggregation** before send: Reduce tuples sent over network by combining duplicates locally.\n",
    "- **Two-phase aggregation**: Local → redistribute → final aggregation.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Slide Questions with Answers\n",
    "\n",
    "- **Q:** Why does range partitioning risk skew?  \n",
    "  **A:** If key distribution is uneven, some processors get more data.  \n",
    "  **Reasoning:** All keys in a range may be concentrated in one partition.\n",
    "\n",
    "- **Q:** Why use sampling before setting range split points?  \n",
    "  **A:** To estimate key distribution and choose balanced split points.  \n",
    "  **Reasoning:** Prevents data skew without scanning all data.\n",
    "\n",
    "- **Q:** In GroupBy, why might final aggregation be faster than initial local aggregation?  \n",
    "  **A:** Fewer records to process (only partial aggregates).  \n",
    "  **Reasoning:** Local phase deals with all input; final phase deals with already reduced data.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Summary\n",
    "\n",
    "- **Parallel Sort**:\n",
    "  - Partition → Local sort → Merge.\n",
    "  - Watch for skew in range partitioning.\n",
    "  - Sampling helps balance.\n",
    "  - Cost model includes scan, select, sort, transfer, receive, merge.\n",
    "\n",
    "- **Parallel GroupBy**:\n",
    "  - Partition → Local aggregate → Merge aggregates.\n",
    "  - Pre-aggregation saves network bandwidth.\n",
    "  - Cost model mirrors parallel join’s structure.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
