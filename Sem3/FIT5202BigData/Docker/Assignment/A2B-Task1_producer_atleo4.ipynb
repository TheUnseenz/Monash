{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data  \n",
    "In this task, we will implement Apache Kafka producers to simulate real-time data streaming. Spark and parallel data processing should not be used in this section, as we are simulating sensors that often lack processing capabilities.  \n",
    "\n",
    "1.\tEvery 5 seconds, load 5 days of weather data from the CSV file. We refer to this as weather5s to explain the tasks; feel free to use your own variable name. You should keep a pointer in the file reading process and advance it per read. The data reading should be in chronological order.\n",
    "2.\tAdd the current timestamp (weather_ts) to the weather5s and spread your batch out evenly for 5 seconds for each day. Since the weather data is hourly readings, each day you shall have 24 records (120 records in total for 5 days).\n",
    "For example, assume you send the records at 2025-01-26 00:00:00 (ISO format: YYYY-MM-DD HH:MM:SS) -> (ts = 1737810000):  \n",
    "Day 1(records 1-24): ts = 1737810000  \n",
    "Day 2(records 25-48): ts = 1737810001  \n",
    "Day 3(records 49-72): ts = 1737810002  \n",
    "â€¦\n",
    "3.\tSend your batch of weather data to a Kafka topic with an appropriate name.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publishing records..\n",
      "5 days:\n",
      "0,2022-01-01 22:00:00.000,26.7,,18.3,1016.9,230.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-01 23:00:00.000,25.6,,18.3,1017.5,230.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 00:00:00.000,24.4,6.0,18.9,1018.1,270.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 01:00:00.000,23.9,4.0,18.3,1018.5,300.0,2.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 02:00:00.000,22.2,,19.4,,360.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 03:00:00.000,21.1,,18.9,1019.5,20.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 04:00:00.000,20.6,,17.8,1019.4,30.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 05:00:00.000,19.4,4.0,17.2,1019.3,20.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 06:00:00.000,18.9,6.0,17.2,1019.0,10.0,2.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 07:00:00.000,18.9,,17.2,1018.4,10.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 08:00:00.000,18.9,,16.7,1018.5,360.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 09:00:00.000,18.3,,16.7,1018.1,10.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 10:00:00.000,18.9,,16.7,1018.1,10.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 11:00:00.000,16.7,,13.9,1018.7,20.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 12:00:00.000,15.6,8.0,13.3,1019.5,350.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 13:00:00.000,15.0,,13.9,1020.0,360.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 14:00:00.000,16.1,,13.3,1020.9,360.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 15:00:00.000,17.2,,13.3,1021.5,360.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 16:00:00.000,17.8,,13.3,1021.5,350.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 17:00:00.000,19.4,,11.7,1020.4,360.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 18:00:00.000,20.6,6.0,11.7,1018.7,10.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 19:00:00.000,22.2,,12.8,1017.6,60.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 20:00:00.000,22.8,,13.9,1016.8,20.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 21:00:00.000,21.1,,13.9,1017.6,350.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 22:00:00.000,20.0,,13.9,1017.9,10.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-02 23:00:00.000,18.9,,13.9,1018.1,10.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 00:00:00.000,17.8,8.0,13.9,1018.2,10.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 01:00:00.000,16.7,,12.8,1018.6,20.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 02:00:00.000,16.7,,12.8,1018.3,20.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 03:00:00.000,16.1,,12.8,1018.0,350.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 04:00:00.000,16.7,,12.8,1018.3,340.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 05:00:00.000,15.6,,12.8,1019.0,330.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 06:00:00.000,15.6,8.0,13.3,1018.4,330.0,8.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 07:00:00.000,15.0,,12.2,1017.9,350.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 08:00:00.000,15.0,,12.2,1017.0,350.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 09:00:00.000,15.0,,12.2,1016.7,340.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 10:00:00.000,14.4,,11.7,1017.0,340.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 11:00:00.000,14.4,,12.2,1016.9,350.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 12:00:00.000,14.4,8.0,12.2,1017.0,10.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 13:00:00.000,14.4,,12.2,1017.9,350.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 14:00:00.000,13.3,,11.7,1019.3,340.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 15:00:00.000,13.3,,11.7,1019.1,360.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 16:00:00.000,13.3,,12.2,1016.7,110.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 17:00:00.000,13.3,,12.2,1016.9,350.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 18:00:00.000,14.4,8.0,12.8,1016.6,320.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 19:00:00.000,15.0,,12.8,1015.6,330.0,2.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 20:00:00.000,14.4,,12.8,1015.7,310.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 21:00:00.000,14.4,,13.3,1015.1,340.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 22:00:00.000,13.9,,12.2,1015.0,350.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-03 23:00:00.000,12.8,,11.7,1014.6,340.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 00:00:00.000,12.2,8.0,11.1,1015.1,350.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 01:00:00.000,11.7,,11.1,1015.8,320.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 02:00:00.000,11.1,,10.0,1016.3,320.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 03:00:00.000,11.1,,10.0,,290.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 04:00:00.000,9.4,,7.2,1015.6,360.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 05:00:00.000,10.6,,7.8,1014.9,320.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 06:00:00.000,10.0,8.0,8.3,1015.1,270.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 07:00:00.000,10.0,,7.8,1014.2,320.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 08:00:00.000,10.6,,7.8,1014.0,310.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 09:00:00.000,10.6,,6.7,1013.7,330.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 10:00:00.000,10.0,,4.4,1014.2,320.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 11:00:00.000,10.0,,4.4,1014.4,340.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 12:00:00.000,9.4,6.0,4.4,1015.3,320.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 13:00:00.000,8.9,4.0,3.9,1016.2,320.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 14:00:00.000,11.1,2.0,2.2,1016.7,320.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 15:00:00.000,13.3,4.0,2.2,1017.3,330.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 16:00:00.000,15.6,2.0,1.1,1016.9,330.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 17:00:00.000,15.6,0.0,-0.6,1016.8,320.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 18:00:00.000,17.2,0.0,-1.7,1016.0,310.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 19:00:00.000,17.8,0.0,-0.6,1015.3,300.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 20:00:00.000,17.8,0.0,-1.7,1015.6,300.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 21:00:00.000,17.8,0.0,-0.6,1016.0,300.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 22:00:00.000,17.8,0.0,-1.1,1016.5,320.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-04 23:00:00.000,14.4,0.0,2.2,1017.4,320.0,2.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 00:00:00.000,13.3,0.0,3.9,1017.9,20.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 01:00:00.000,13.3,0.0,4.4,1019.0,30.0,7.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 02:00:00.000,12.2,0.0,6.1,1020.4,30.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 03:00:00.000,11.1,2.0,6.7,1021.0,360.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 04:00:00.000,11.7,0.0,6.1,1021.2,360.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 05:00:00.000,10.6,0.0,6.1,1021.3,350.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 06:00:00.000,10.0,0.0,6.1,1020.9,360.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 07:00:00.000,10.6,0.0,6.1,1020.9,360.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 08:00:00.000,10.6,0.0,6.7,1020.8,360.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 09:00:00.000,10.0,0.0,6.7,1021.5,360.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 10:00:00.000,10.6,0.0,6.7,1022.2,360.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 11:00:00.000,11.1,0.0,6.7,1022.5,10.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 12:00:00.000,11.7,2.0,7.8,1023.1,20.0,7.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 13:00:00.000,12.2,4.0,7.8,1023.7,10.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 14:00:00.000,13.9,2.0,7.8,1024.4,10.0,8.8,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 15:00:00.000,15.6,4.0,8.9,1024.7,30.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 16:00:00.000,18.9,,9.4,1024.7,50.0,8.8,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 17:00:00.000,19.4,4.0,8.3,1024.8,40.0,8.8,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 18:00:00.000,20.0,6.0,7.8,1023.9,30.0,7.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 19:00:00.000,20.0,,5.6,1023.2,50.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 20:00:00.000,18.9,,8.9,1023.3,60.0,7.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 21:00:00.000,17.8,,7.8,1023.4,50.0,8.8,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 22:00:00.000,17.2,,8.3,1024.0,60.0,7.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-05 23:00:00.000,16.7,,8.9,1024.1,40.0,6.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 00:00:00.000,16.1,8.0,9.4,1024.6,50.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 01:00:00.000,16.1,,9.4,1025.2,50.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 02:00:00.000,15.6,,10.0,1025.8,10.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 03:00:00.000,15.0,,10.6,1025.9,40.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 04:00:00.000,13.9,,10.0,1025.2,10.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 05:00:00.000,13.9,,10.0,1024.9,20.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 06:00:00.000,13.3,8.0,11.7,1023.9,360.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 07:00:00.000,13.3,,11.7,1023.5,360.0,4.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 08:00:00.000,13.9,,12.2,1022.7,350.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 09:00:00.000,13.9,,11.7,1023.0,340.0,3.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 10:00:00.000,13.3,,11.7,1022.1,10.0,5.1,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 11:00:00.000,13.9,,12.8,1022.5,350.0,3.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 12:00:00.000,14.4,8.0,13.3,1022.5,340.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 13:00:00.000,14.4,,13.3,1022.7,350.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 14:00:00.000,12.8,,11.7,1023.2,330.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 15:00:00.000,13.9,,12.8,1023.5,350.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 16:00:00.000,15.0,,13.3,1023.3,340.0,4.6,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 17:00:00.000,17.2,,14.4,1021.8,360.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 18:00:00.000,19.4,6.0,15.6,1020.2,20.0,5.7,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 19:00:00.000,20.0,,15.6,1019.0,10.0,6.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 20:00:00.000,21.1,,13.9,1018.3,30.0,7.2,2025-10-07T08:19:42.551147\r\n",
      "0,2022-01-06 21:00:00.000,18.9,,15.0,1018.0,350.0,8.2,2025-10-07T08:19:42.551147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5 days:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(rows_to_csv_string(rows))\n\u001b[0;32m---> 71\u001b[0m \u001b[43mpublish_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparsed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows_to_csv_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m sleep(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m, in \u001b[0;36mpublish_message\u001b[0;34m(producer_instance, topic_name, key, value)\u001b[0m\n\u001b[1;32m     18\u001b[0m key_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(key, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m value_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m(value, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mproducer_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m producer_instance\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMessage published successfully. Data: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka3/producer/kafka.py:579\u001b[0m, in \u001b[0;36mKafkaProducer.send\u001b[0;34m(self, topic, value, key, headers, partition, timestamp_ms)\u001b[0m\n\u001b[1;32m    577\u001b[0m key_bytes \u001b[38;5;241m=\u001b[39m value_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_on_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_block_ms\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     key_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize(\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey_serializer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    583\u001b[0m         topic, key)\n\u001b[1;32m    584\u001b[0m     value_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialize(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue_serializer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    586\u001b[0m         topic, value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/kafka3/producer/kafka.py:702\u001b[0m, in \u001b[0;36mKafkaProducer._wait_on_metadata\u001b[0;34m(self, topic, max_wait)\u001b[0m\n\u001b[1;32m    700\u001b[0m future\u001b[38;5;241m.\u001b[39madd_both(\u001b[38;5;28;01mlambda\u001b[39;00m e, \u001b[38;5;241m*\u001b[39margs: e\u001b[38;5;241m.\u001b[39mset(), metadata_event)\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sender\u001b[38;5;241m.\u001b[39mwakeup()\n\u001b[0;32m--> 702\u001b[0m \u001b[43mmetadata_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_wait\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m begin\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metadata_event\u001b[38;5;241m.\u001b[39mis_set():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import io\n",
    "\n",
    "#configuration\n",
    "hostip = \"10.192.89.180\" #change to your machine IP address\n",
    "\n",
    "topic = 'A2B'\n",
    "\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print('Message published successfully. Data: ' + str(data))\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n",
    "        \n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "\n",
    "def read_csv_in_chunks(filepath, chunk_size):\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)  # Read header row once\n",
    "        while True:\n",
    "            chunk = []\n",
    "            now = datetime.now().isoformat() # Same timestamp for all items in chunk\n",
    "            for _ in range(chunk_size):\n",
    "                try:\n",
    "                    row = next(reader)\n",
    "#                     row.append(datetime.now().isoformat()) # Timestamp for read time of each line\n",
    "                    row.append(now) \n",
    "                    chunk.append(row)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            if not chunk:\n",
    "                break\n",
    "#             print(headers, chunk)\n",
    "            yield headers, chunk\n",
    "            \n",
    "def rows_to_csv_string(rows):\n",
    "    buf = io.StringIO()\n",
    "    writer = csv.writer(buf)\n",
    "    writer.writerows(rows)\n",
    "    return buf.getvalue().strip()\n",
    "\n",
    "if __name__ == '__main__':   \n",
    "    print('Publishing records..')\n",
    "    producer = connect_kafka_producer()\n",
    "    for headers, rows in read_csv_in_chunks(\"data/weather.csv\", chunk_size=120):\n",
    "        print(\"5 days:\")\n",
    "        print(rows_to_csv_string(rows))\n",
    "        publish_message(producer, topic, 'parsed', rows_to_csv_string(rows))\n",
    "        sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT\n",
    "import csv\n",
    "\n",
    "def read_csv_in_chunks(filepath, chunk_size):\n",
    "    with open(filepath, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        headers = next(reader)  # Read header row once\n",
    "        while True:\n",
    "            chunk = []\n",
    "            for _ in range(chunk_size):\n",
    "                try:\n",
    "                    row = next(reader)\n",
    "                    chunk.append(row)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield headers, chunk\n",
    "            \n",
    "for headers, rows in read_csv_in_chunks(\"large_file.csv\", chunk_size=1000):\n",
    "    print(f\"Read {len(rows)} rows\")\n",
    "    # process rows here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_incrementally(filepath):\n",
    "    \"\"\"\n",
    "    Reads a CSV file incrementally, yielding each row as a dictionary.\n",
    "    \"\"\"\n",
    "    with open(filepath, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            yield row  # Yield each row for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chocolate.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "        \n",
    "with open('chocolate.csv') as f:\n",
    "    dict_reader = csv.DictReader(f, delimiter=',')\n",
    "    for row in dict_reader:\n",
    "        print(\"The {} company is located in {}.\".format(row['Company'], row['Company Location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_incrementally(filepath):\n",
    "    \"\"\"\n",
    "    Reads a CSV file incrementally and processes each row.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', newline='') as csvfile:\n",
    "        csv_reader = csv.reader(csvfile)\n",
    "\n",
    "        # Skip header row if present\n",
    "        header = next(csv_reader, None)\n",
    "        if header:\n",
    "            print(f\"Header: {header}\")\n",
    "\n",
    "        # Process each data row\n",
    "        for row in csv_reader:\n",
    "            # Perform operations on the current row\n",
    "            print(f\"Processing row: {row}\")\n",
    "            # Example: Access individual elements by index\n",
    "            # name = row[0]\n",
    "            # age = int(row[1])\n",
    "            # ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
