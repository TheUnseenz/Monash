{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform a prediction.    \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2B\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name) \\\n",
    "                        .set(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWrite code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. building information) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from GPT\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType, DateType, DoubleType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(15, 4), False),\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(6, 4), True)\n",
    "])\n",
    "\n",
    "# 3. Weather Table\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"cloud_coverage\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"dew_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 3), True),\n",
    "    StructField(\"wind_direction\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"wind_speed\", DecimalType(5, 3), True),\n",
    "    StructField(\"weather_ts\", TimestampType(), False) # new field\n",
    "])\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/new_building_information.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUsing the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'weather_ts' column, you shall receive it as an Int type. Load the new building information CSV file into a dataframe. Then, the data frames should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#configuration\n",
    "hostip = \"192.168.0.6\"#\"10.192.90.63\" #change to your machine IP address\n",
    "topic = 'weather_data'\n",
    "\n",
    "df_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "df_str = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "weather_stream = (\n",
    "    df_str\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema)))\n",
    "    .select(F.explode(F.col(\"data\")).alias(\"r\"))\n",
    "    .select(\"r.*\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse a watermark on weather_ts, if data points are received 5 seconds late, discard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stream = weather_stream.withWatermark(\"weather_ts\", '5 seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform the necessary transformation you used in A2A. (note: every student may have used different features, feel free to reuse the code you have written in A2A. If you built an end-to-end pipeline, you can ignore this task.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from A2A which was from GPT\n",
    "# Get global_means, site_month_means, site_means\n",
    "# weather_df is history, weather_stream is current\n",
    "\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_df = weather_df.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Choose which columns to impute\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Compute global_means, site_month_means, site_means\n",
    "global_means = weather_df.select(\n",
    "    *[F.mean(c).alias(c) for c in impute_cols]\n",
    ").first().asDict()\n",
    "\n",
    "site_month_means = weather_df.groupBy(\"site_id\", \"month\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_month_mean\") for c in impute_cols]\n",
    ")\n",
    "\n",
    "site_means = weather_df.groupBy(\"site_id\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_mean\") for c in impute_cols]\n",
    ")\n",
    "    \n",
    "# Skip Garbage collection\n",
    "# del site_month_means\n",
    "# del site_means\n",
    "# del global_means\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# Transform weather_stream\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_stream = weather_stream.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Step 1: site_id + month\n",
    "weather_stream = weather_stream.join(site_month_means, on=[\"site_id\", \"month\"], how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_month_mean\"))\n",
    "    ).drop(f\"{c}_site_month_mean\")\n",
    "    \n",
    "# Step 2: site_id\n",
    "weather_stream = weather_stream.join(site_means, on=\"site_id\", how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_mean\"))\n",
    "    ).drop(f\"{c}_site_mean\")\n",
    "\n",
    "# Step 3: global fallback\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.lit(global_means[c]))\n",
    "    )\n",
    "# # Aggregate by time bucket\n",
    "# weather_stream = (\n",
    "#     weather_stream\n",
    "#     .groupBy(\n",
    "#         \"site_id\", \"date\", \"time\", \"month\",\n",
    "#         F.window(\"weather_ts\", \"5 seconds\")\n",
    "#     )\n",
    "#     .agg(\n",
    "#         F.mean(\"air_temperature\").cast(DecimalType(5, 3)).alias(\"air_temperature\"),\n",
    "#         F.mean(\"cloud_coverage\").cast(DecimalType(5, 3)).alias(\"cloud_coverage\"),\n",
    "#         F.mean(\"dew_temperature\").cast(DecimalType(5, 3)).alias(\"dew_temperature\"),\n",
    "#         F.mean(\"sea_level_pressure\").cast(DecimalType(8, 3)).alias(\"sea_level_pressure\"),\n",
    "#         F.mean(\"wind_direction\").cast(DecimalType(5, 3)).alias(\"wind_direction\"),\n",
    "#         F.mean(\"wind_speed\").cast(DecimalType(5, 3)).alias(\"wind_speed\")     \n",
    "# #         F.max(\"weather_ts\").alias(\"weather_ts\")   # 👈 reattach representative timestamp\n",
    "#     )    \n",
    "# #     # Extract representative event time\n",
    "# #     .withColumn(\"weather_ts\", F.col(\"window.end\"))\n",
    "# #     .drop(\"window\")\n",
    "# )\n",
    "\n",
    "# Add custom columns\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "    .withColumn(\"dew_depression\", F.col(\"air_temperature\") - F.col(\"dew_temperature\"))\n",
    "    .withColumn(\"nonideal_temp\", (F.col(\"air_temperature\") - 18)**2)\n",
    "    .drop(\"air_temperature\")\n",
    "    .drop(\"dew_temperature\")\n",
    ")\n",
    "\n",
    "# No need to add median temp and peak-offpeak as our pipeline model later does not use them\n",
    "feature_df = buildings_df.join(weather_stream, [\"site_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tLoad your pipeline model and perform the following aggregations:  \n",
    "a)\tPrint the prediction from your model as a stream comes in.  \n",
    "b)\tEvery 7 seconds, print the total energy consumption for each 6-hour interval, aggregated by building, and print 20 records. (Note: This is simulating energy data each day in a week)  \n",
    "c)\tEvery 14 seconds, for each site, print the daily total energy consumption.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Spark + model setup ---\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "import time\n",
    "\n",
    "model = PipelineModel.load(\"models/best_model_rmsle\")\n",
    "\n",
    "# --- 2. Apply model ---\n",
    "predictions = model.transform(feature_df).withColumnRenamed(\"prediction\", \"log_power_usage\")\n",
    "\n",
    "checkpoint_dir = os.path.abspath(\"checkpoints/weather_stream\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for first batch...\n",
      "+-------+-----------+------------------+\n",
      "|site_id|building_id|   log_power_usage|\n",
      "+-------+-----------+------------------+\n",
      "|      8|        861| 4.061107855980998|\n",
      "|      8|        843| 4.428725433461416|\n",
      "|      8|        822| 2.389271160411927|\n",
      "|      8|        866|   4.7190681590531|\n",
      "|      8|        865| 4.725603405704713|\n",
      "|      8|        834| 4.037425349753118|\n",
      "|      8|        811|2.6361125623621566|\n",
      "|      9|        977|3.5224304499167083|\n",
      "|      9|        993|7.8203338563111675|\n",
      "|      9|        992| 5.751885548834196|\n",
      "|      9|        990|5.9585415358470835|\n",
      "|      9|        982| 6.099074356589539|\n",
      "|      9|        981|6.5416565754162175|\n",
      "|      9|        974| 7.254738639766967|\n",
      "|      9|        973| 7.572680107210483|\n",
      "|      9|        959| 6.602787329961556|\n",
      "|      9|        957|6.6434303000431925|\n",
      "|      9|        954| 7.653788027556944|\n",
      "|      9|        952| 6.937003238959837|\n",
      "|      9|        947| 6.230174505306105|\n",
      "+-------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 6a\n",
    "# Show live predictions\n",
    "query_live = (\n",
    "    predictions\n",
    "        .select(\"site_id\", \"building_id\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"live_predictions\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"Waiting for first batch...\")\n",
    "# Wait for the first microbatch to finish\n",
    "while query_live.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)\n",
    "spark.sql(\"select * from live_predictions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for first batch...\n",
      "+-----------+----+----+--------------+\n",
      "|building_id|date|time|total_power_6h|\n",
      "+-----------+----+----+--------------+\n",
      "+-----------+----+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 6b\n",
    "# (Task 6b requires aggregation by 6-hour interval)\n",
    "building_6h = (\n",
    "    predictions\n",
    "        .groupBy(\n",
    "            \"building_id\",\n",
    "            \"date\",\n",
    "            \"time\",  # Group by 6 hours of event-time\n",
    "            F.window(\"weather_ts\", \"5 seconds\") # Watermark 5 seconds of processing time\n",
    "        )\n",
    "        .agg(F.sum(\"log_power_usage\").alias(\"total_power_6h\"))\n",
    "        .drop(\"window\")\n",
    ")\n",
    "\n",
    "# --- Print to console every 7 seconds ---\n",
    "query_building_6h = (\n",
    "    building_6h\n",
    "        .writeStream\n",
    "        .outputMode(\"update\") # 'update' mode is correct for windowed aggregations\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"building_6h\")\n",
    "        .trigger(processingTime=\"7 seconds\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"Waiting for first batch...\")\n",
    "# Wait for the first microbatch to finish\n",
    "while query_building_6h.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)    \n",
    "spark.sql(\"select * from building_6h\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for first batch...\n",
      "+-------+----------+------------------+\n",
      "|site_id|      date|   total_power_day|\n",
      "+-------+----------+------------------+\n",
      "|     12|2022-01-05| 572.7342103477073|\n",
      "|     10|2022-01-05| 469.9349332503669|\n",
      "|     12|2022-01-04|162.95446694173177|\n",
      "|     13|2022-01-05|2370.3350094118846|\n",
      "|     13|2022-01-04| 674.4028832870381|\n",
      "|      9|2022-01-04|300.89999848006653|\n",
      "|      5|2022-01-05|1124.6531168216864|\n",
      "|     14|2022-01-04|473.18583547849533|\n",
      "|      0|2022-01-05|1523.0941193922542|\n",
      "|      3|2022-01-05| 3986.921797130366|\n",
      "|      2|2022-01-04|213.10652652331166|\n",
      "|     14|2022-01-05|1666.5162041026665|\n",
      "|      6|2022-01-04| 65.19596300214755|\n",
      "|      5|2022-01-04|141.23216305172062|\n",
      "|      1|2022-01-04| 98.12558570774381|\n",
      "|     15|2022-01-04|467.56681801979727|\n",
      "|      6|2022-01-05| 516.1672388089638|\n",
      "|      4|2022-01-04| 174.6069838722403|\n",
      "|      8|2022-01-05|192.43712424573005|\n",
      "|      9|2022-01-05|2117.8548405524016|\n",
      "+-------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 6c\n",
    "# (Task 6c requires daily aggregation by site)\n",
    "site_daily = (\n",
    "    predictions\n",
    "        .groupBy(\n",
    "            \"site_id\", \n",
    "            \"date\", # Group by daily windows of event-time\n",
    "            F.window(\"weather_ts\", \"5 seconds\") # Watermark 5 seconds of processing time\n",
    "        )\n",
    "        .agg(F.sum(\"log_power_usage\").alias(\"total_power_day\"))\n",
    "        .drop(\"window\")\n",
    ")\n",
    "\n",
    "# --- Print to console every 14 seconds ---\n",
    "query_site_daily = (\n",
    "    site_daily\n",
    "        .writeStream\n",
    "        .outputMode(\"update\") # 'update' mode is correct\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"site_daily\")\n",
    "        .trigger(processingTime=\"14 seconds\")\n",
    "        .start()\n",
    ")\n",
    "print(\"Waiting for first batch...\")\n",
    "# Wait for the first microbatch to finish\n",
    "while query_site_daily.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)\n",
    "spark.sql(\"select * from site_daily\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tSave the data from 6 to Parquet files as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Parquet microbatch...\n"
     ]
    }
   ],
   "source": [
    "# 7a(save 6a)\n",
    "\n",
    "# Save predictions to Parquet incrementally\n",
    "query_live_parquet = (\n",
    "    predictions\n",
    "        .select(\"site_id\", \"building_id\", \"time\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/live_predictions\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/live_predictions\")\n",
    "        .start()\n",
    ")\n",
    "print(\"Waiting for Parquet microbatch...\")\n",
    "while query_live_parquet.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Parquet microbatch...\n"
     ]
    }
   ],
   "source": [
    "# 7b(save 6b)\n",
    "query_building_6h_parquet = (\n",
    "    building_6h\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/building_6h\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/building_6h\")\n",
    "        .start()\n",
    ")\n",
    "print(\"Waiting for Parquet microbatch...\")\n",
    "while query_building_6h_parquet.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Parquet microbatch...\n"
     ]
    }
   ],
   "source": [
    "# 7c(save 6c)\n",
    "query_site_daily_parquet = (\n",
    "    site_daily\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/site_daily\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/site_daily\")\n",
    "        .trigger(processingTime=\"14 seconds\")\n",
    "        .start()\n",
    ")\n",
    "print(\"Waiting for Parquet microbatch...\")\n",
    "while query_site_daily_parquet.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tRead the parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "kafka_ip = hostip + \":9092\"\n",
    "\n",
    "# Schema for 6a/7a/8a (live_predictions)\n",
    "live_pred_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"log_power_usage\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema for 6b/7b/8b (building_6h)\n",
    "building_6h_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "#     StructField(\"window\", StructType([\n",
    "#         StructField(\"start\", TimestampType(), True),\n",
    "#         StructField(\"end\", TimestampType(), True)\n",
    "#     ]), True),\n",
    "    StructField(\"total_power_6h\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema for 6c/7c/8c (site_daily)\n",
    "site_daily_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "#     StructField(\"window\", StructType([\n",
    "#         StructField(\"start\", TimestampType(), True),\n",
    "#         StructField(\"end\", TimestampType(), True)\n",
    "#     ]), True),\n",
    "    StructField(\"total_power_day\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Stream 1\n",
    "read_parquet_live_predictions = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(live_pred_schema)\n",
    "         .load(\"data/live_predictions\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_live_predictions = (\n",
    "    read_parquet_live_predictions\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                    \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"live_predictions\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/live_predictions\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n",
    "read_parquet_building_6h = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(building_6h_schema)\n",
    "         .load(\"data/building_6h\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_building_6h = (\n",
    "    read_parquet_building_6h\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                        \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"building_6h\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/building_6h\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 3\n",
    "read_parquet_site_daily = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(site_daily_schema)\n",
    "         .load(\"data/site_daily\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_site_daily = (\n",
    "    read_parquet_site_daily\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                        \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"site_daily\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/site_daily\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Is active? True\n",
      "Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n",
      "<bound method StreamingQuery.exception of <pyspark.sql.streaming.query.StreamingQuery object at 0x71f5abf6c3a0>>\n"
     ]
    }
   ],
   "source": [
    "while query_site_daily_parquet.lastProgress is None:\n",
    "    time.sleep(1)\n",
    "print(kafka_site_daily.lastProgress)\n",
    "print(\"Is active?\", kafka_site_daily.isActive)\n",
    "print(\"Status:\", kafka_site_daily.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n",
      "null\n",
      "null\n",
      "null\n",
      "{\n",
      "  \"id\": \"25a09da5-e0db-49d8-b517-fbbb795da9c6\",\n",
      "  \"runId\": \"484509b0-772b-41ed-9bc6-fb201e01cec7\",\n",
      "  \"name\": null,\n",
      "  \"timestamp\": \"2025-10-24T05:48:53.160Z\",\n",
      "  \"batchId\": 0,\n",
      "  \"numInputRows\": 0,\n",
      "  \"inputRowsPerSecond\": 0.0,\n",
      "  \"processedRowsPerSecond\": 0.0,\n",
      "  \"durationMs\": {\n",
      "    \"addBatch\": 12257,\n",
      "    \"commitOffsets\": 289,\n",
      "    \"getBatch\": 81,\n",
      "    \"latestOffset\": 519,\n",
      "    \"queryPlanning\": 2,\n",
      "    \"triggerExecution\": 13428,\n",
      "    \"walCommit\": 259\n",
      "  },\n",
      "  \"stateOperators\": [],\n",
      "  \"sources\": [\n",
      "    {\n",
      "      \"description\": \"FileStreamSource[file:/home/student/Assignment/data/site_daily]\",\n",
      "      \"startOffset\": null,\n",
      "      \"endOffset\": {\n",
      "        \"logOffset\": 0\n",
      "      },\n",
      "      \"latestOffset\": null,\n",
      "      \"numInputRows\": 0,\n",
      "      \"inputRowsPerSecond\": 0.0,\n",
      "      \"processedRowsPerSecond\": 0.0\n",
      "    }\n",
      "  ],\n",
      "  \"sink\": {\n",
      "    \"description\": \"org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@48fc4c72\",\n",
      "    \"numOutputRows\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "\n",
    "for i in range(5):\n",
    "    time.sleep(3)\n",
    "    print(json.dumps(kafka_site_daily.lastProgress, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+------------------+\n",
      "|site_id|building_id|  time|   log_power_usage|\n",
      "+-------+-----------+------+------------------+\n",
      "|      6|        766|18-24h| 5.477508030876975|\n",
      "|      6|        764|18-24h|5.1696915897919995|\n",
      "|      6|        760|18-24h| 5.685122953716053|\n",
      "|      6|        753|18-24h| 5.766261711592816|\n",
      "|      6|        750|18-24h| 5.142296791595273|\n",
      "+-------+-----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+----------+------+------------------+\n",
      "|building_id|      date|  time|    total_power_6h|\n",
      "+-----------+----------+------+------------------+\n",
      "|       1185|2022-01-09|18-24h|17.251736703858818|\n",
      "|        678|2022-01-11|  0-6h|  22.8457083760589|\n",
      "|         97|2022-01-11|  0-6h| 45.60107833960657|\n",
      "|       1083|2022-01-11|  0-6h|26.684846194365164|\n",
      "|         15|2022-01-10|12-18h| 7.049988453300645|\n",
      "+-----------+----------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+----------+------------------+\n",
      "|site_id|      date|   total_power_day|\n",
      "+-------+----------+------------------+\n",
      "|      5|2022-01-12| 877.7260330655303|\n",
      "|      2|2022-01-13|1735.6175441931327|\n",
      "|      1|2022-01-13| 688.2700453396908|\n",
      "|      8|2022-01-12|161.04774827170831|\n",
      "|     13|2022-01-12|  1988.32783116533|\n",
      "+-------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"data/live_predictions\").show(5)\n",
    "spark.read.parquet(\"data/building_6h\").show(5)\n",
    "spark.read.parquet(\"data/site_daily\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt: \n",
    " i have the following code file, Task3_consumer, meant to get 2 kafka consumers to consume the 2 aggregated data streams. i want to visualize data flowing in, and then later plot the predicted vs actual energy consumption. this code block plots fine, but it keeps flickering so much you can barely actually see anything. the problem is that it keeps redrawing everything, especially as the incoming date changes quite frequently. for the site_daily stream, the site_id is statically 0-15 and can be kept there, but for the building_6h stream, the incoming building_ids vary widely and the rankings would keep changing. for both streams, i want the max value on the y axis to be set to at least 500 and 5000 for building_6h and site_daily streams respectively, updating to be more if higher values come in. help me to fix these issues.\n",
    "\n",
    "\n",
    "to fix the redrawing problem, i want to collect all the data continuously, but to only redraw these graphs every 5 seconds. this should help fix the other problem where data spanning a few days come in out of order, due to lag or other limitations, so the kafka consumer can collect the full day's data before redrawing the graph. it is okay for the graphs to be late, as long as it can collect all the data first. it should store data until 7 days of data at once are stored, and then only start plotting 1 week old data, 1 day at a time (2 days of history for site_daily). plotting only 1 week old data should ensure that all the data we want to plot for that day has arrived. do not skip days of data being plotted, after a day's data has finished its plot and is no longer in use, it can be dropped.\n",
    "the part where it redraws a graphs every 5 seconds should be in sync with it plotting data 1 week old, 1 day at a time. the producer publishes every 5 seconds, but it may not be a full day's data, and it may arrive out of order, hence the waiting for a whole day's data to come before redrawing the graph.\n",
    "\n",
    "\n",
    "i would also like a debugger to show how many data points for each date is currently being stored. \n",
    "the A2B-Task3_consumer_atleo4 copy.ipynb file contains the code, the A2B_Specification_2025S2.pdf describes what was supposed to be done. \n",
    "\n",
    "note that \"weather_ts\" represents the processor timestamp of the producer, while \"date\" refers to the date of the data collection time, and \"time\" is a string corresponding to whether it is 0-6h, 6-12h, 12-18h, 18-24h.  \n",
    "\n",
    "the current setup already receives data, and is able to plot some of it, so it can be assumed that the producer is mostly correct.  \n",
    "the schema for the kafka stream that the kafka consumer receives, as per end of Task2_spark_streaming, is:\n",
    "import json\n",
    "kafka_ip = hostip + \":9092\"\n",
    "\n",
    "# Schema for 6a/7a/8a (live_predictions)\n",
    "live_pred_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "    StructField(\"log_power_usage\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema for 6b/7b/8b (building_6h)\n",
    "building_6h_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"time\", StringType(), True),\n",
    "#     StructField(\"window\", StructType([\n",
    "#         StructField(\"start\", TimestampType(), True),\n",
    "#         StructField(\"end\", TimestampType(), True)\n",
    "#     ]), True),\n",
    "    StructField(\"total_power_6h\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Schema for 6c/7c/8c (site_daily)\n",
    "site_daily_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "#     StructField(\"window\", StructType([\n",
    "#         StructField(\"start\", TimestampType(), True),\n",
    "#         StructField(\"end\", TimestampType(), True)\n",
    "#     ]), True),\n",
    "    StructField(\"total_power_day\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "and they are written as such:\n",
    "# Stream 3\n",
    "read_parquet_site_daily = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(site_daily_schema)\n",
    "         .load(\"data/site_daily\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_site_daily = (\n",
    "    read_parquet_site_daily\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                        \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"site_daily\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/site_daily\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n",
    "with an identical one for building_6h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
