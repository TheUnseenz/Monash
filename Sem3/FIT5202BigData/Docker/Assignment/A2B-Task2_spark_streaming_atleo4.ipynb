{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform a prediction.    \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2B\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name) \\\n",
    "                        .set(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWrite code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. building information) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from GPT\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(15, 4), False),\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(6, 4), True)\n",
    "])\n",
    "\n",
    "# 3. Weather Table\n",
    "# weather_schema = StructType([\n",
    "#     StructField(\"site_id\", StringType(), False),\n",
    "#     StructField(\"timestamp\", StringType(), False),\n",
    "#     StructField(\"air_temperature\", StringType(), True),\n",
    "#     StructField(\"cloud_coverage\", StringType(), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "#     StructField(\"dew_temperature\", StringType(), True),\n",
    "#     StructField(\"sea_level_pressure\", StringType(), True),\n",
    "#     StructField(\"wind_direction\", StringType(), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "#     StructField(\"wind_speed\", StringType(), True),\n",
    "#     StructField(\"weather_ts\", StringType(), False) # new field\n",
    "# ])\n",
    "\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"cloud_coverage\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"dew_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 3), True),\n",
    "    StructField(\"wind_direction\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"wind_speed\", DecimalType(5, 3), True),\n",
    "    StructField(\"weather_ts\", TimestampType(), False) # new field\n",
    "])\n",
    "\n",
    "\n",
    "# buildings_df = spark.read.csv(\n",
    "#     \"data/building_information.csv\",\n",
    "#     header=True,\n",
    "#     schema=buildings_schema\n",
    "# )\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/new_building_information.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUsing the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'weather_ts' column, you shall receive it as an Int type. Load the new building information CSV file into a dataframe. Then, the data frames should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#configuration\n",
    "hostip = \"192.168.0.6\"#\"10.192.90.63\" #change to your machine IP address\n",
    "topic = 'weather_data'\n",
    "\n",
    "df_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "df_str = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "# df_parsed = df_str.withColumn(\n",
    "#     \"data\",\n",
    "#     F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema))\n",
    "# )\n",
    "# df_exploded = df_parsed.select(F.explode(F.col(\"data\")).alias(\"record\"))\n",
    "# df_final = df_exploded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather_stream = (\n",
    "    df_str\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema)))\n",
    "    .select(F.explode(F.col(\"data\")).alias(\"r\"))\n",
    "    .select(\"r.*\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse a watermark on weather_ts, if data points are received 5 seconds late, discard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stream = weather_stream.withWatermark(\"weather_ts\", '5 seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform the necessary transformation you used in A2A. (note: every student may have used different features, feel free to reuse the code you have written in A2A. If you built an end-to-end pipeline, you can ignore this task.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from A2A which was from GPT\n",
    "# Get global_means, site_month_means, site_means\n",
    "# weather_df is history, weather_stream is current\n",
    "\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_df = weather_df.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Choose which columns to impute\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Compute global_means, site_month_means, site_means\n",
    "global_means = weather_df.select(\n",
    "    *[F.mean(c).alias(c) for c in impute_cols]\n",
    ").first().asDict()\n",
    "\n",
    "site_month_means = weather_df.groupBy(\"site_id\", \"month\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_month_mean\") for c in impute_cols]\n",
    ")\n",
    "\n",
    "site_means = weather_df.groupBy(\"site_id\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_mean\") for c in impute_cols]\n",
    ")\n",
    "    \n",
    "# Skip Garbage collection\n",
    "# del site_month_means\n",
    "# del site_means\n",
    "# del global_means\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# Transform weather_stream\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_stream = weather_stream.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Step 1: site_id + month\n",
    "weather_stream = weather_stream.join(site_month_means, on=[\"site_id\", \"month\"], how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_month_mean\"))\n",
    "    ).drop(f\"{c}_site_month_mean\")\n",
    "    \n",
    "# Step 2: site_id\n",
    "weather_stream = weather_stream.join(site_means, on=\"site_id\", how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_mean\"))\n",
    "    ).drop(f\"{c}_site_mean\")\n",
    "\n",
    "# Step 3: global fallback\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.lit(global_means[c]))\n",
    "    )\n",
    "# Aggregate by time bucket\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "#     .withWatermark(\"weather_ts\", \"5 seconds\")\n",
    "    .groupBy(\n",
    "        \"site_id\", \"date\", \"time\", \"month\",\n",
    "        F.window(\"weather_ts\", \"5 seconds\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.mean(\"air_temperature\").cast(DecimalType(5, 3)).alias(\"air_temperature\"),\n",
    "        F.mean(\"cloud_coverage\").cast(DecimalType(5, 3)).alias(\"cloud_coverage\"),\n",
    "        F.mean(\"dew_temperature\").cast(DecimalType(5, 3)).alias(\"dew_temperature\"),\n",
    "        F.mean(\"sea_level_pressure\").cast(DecimalType(8, 3)).alias(\"sea_level_pressure\"),\n",
    "        F.mean(\"wind_direction\").cast(DecimalType(5, 3)).alias(\"wind_direction\"),\n",
    "        F.mean(\"wind_speed\").cast(DecimalType(5, 3)).alias(\"wind_speed\"),     \n",
    "    )\n",
    ")\n",
    "\n",
    "# Add custom columns\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "    .withColumn(\"dew_depression\", F.col(\"air_temperature\") - F.col(\"dew_temperature\"))\n",
    "    .withColumn(\"nonideal_temp\", (F.col(\"air_temperature\") - 18)**2)\n",
    "    .drop(\"air_temperature\")\n",
    "    .drop(\"dew_temperature\")\n",
    ")\n",
    "\n",
    "# No need to add median temp and peak-offpeak as our pipeline model later does not use them\n",
    "feature_df = buildings_df.join(weather_stream, [\"site_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tLoad your pipeline model and perform the following aggregations:  \n",
    "a)\tPrint the prediction from your model as a stream comes in.  \n",
    "b)\tEvery 7 seconds, print the total energy consumption for each 6-hour interval, aggregated by building, and print 20 records. (Note: This is simulating energy data each day in a week)  \n",
    "c)\tEvery 14 seconds, for each site, print the daily total energy consumption.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to predict (and print) from my earlier created model as the stream comes in. the model was earlier saved as:\n",
    "# ```\n",
    "# # Define the path to save the model\n",
    "# model_path = \"models/best_model_rmsle\"\n",
    "\n",
    "# # Save the better trained model. \n",
    "# if rf_cv_metrics[\"rmsle\"] < gbt_cv_metrics[\"rmsle\"]:\n",
    "#     rf_cv_model.bestModel.save(model_path)\n",
    "# else:\n",
    "#     gbt_cv_model.bestModel.save(model_path)\n",
    "# ```\n",
    "# where exactly do i load and run the model (was it model.transform(feature_df)?) to predict and append another column \"value\"?\n",
    "# feature_df is df_final as above, with some imputation and a join done. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Spark + model setup ---\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model = PipelineModel.load(\"models/best_model_rmsle\")\n",
    "\n",
    "# --- 2. Apply model ---\n",
    "predictions = model.transform(feature_df).withColumnRenamed(\"prediction\", \"log_power_usage\")\n",
    "\n",
    "checkpoint_dir = os.path.abspath(\"checkpoints/weather_stream\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a\n",
    "live_predictions = (\n",
    "    predictions\n",
    "        .select(\"site_id\", \"building_id\", \"weather_ts\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "        .format(\"memory\")\n",
    "        .queryName(\"live_predictions\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir)\n",
    "        .option(\"truncate\", False)\n",
    "        .start()\n",
    ")\n",
    "spark.sql(\"select * from live_predictions\").show()\n",
    "\n",
    "\n",
    "\n",
    "# Show live predictions\n",
    "query_live = (\n",
    "    predictions\n",
    "        .select(\"site_id\", \"building_id\", \"weather_ts\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .option(\"numRows\", 10)\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/live_predictions_console\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "# Save predictions to Parquet incrementally\n",
    "query_live_parquet = (\n",
    "    predictions\n",
    "        .select(\"site_id\", \"building_id\", \"weather_ts\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "        .outputMode(\"append\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/live_predictions\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/live_predictions\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n",
    "building_6h = (\n",
    "    predictions\n",
    "#         .withWatermark(\"weather_ts\", \"10 minutes\")\n",
    "        .groupBy(\n",
    "            \"building_id\",\n",
    "            F.window(\"weather_ts\", \"6 hours\")\n",
    "        )\n",
    "        .agg(F.sum(\"log_power_usage\").alias(\"total_power_6h\"))\n",
    "        .select(\n",
    "            \"building_id\",\n",
    "            F.col(\"window.start\").alias(\"window_start\"),\n",
    "            F.col(\"window.end\").alias(\"window_end\"),\n",
    "            \"total_power_6h\"\n",
    "        )\n",
    ")\n",
    "\n",
    "# --- Print to console every 7 seconds ---\n",
    "building_6h_console = (\n",
    "    building_6h\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .option(\"numRows\", 10)\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/agg_building_6h_console\")\n",
    "        .trigger(processingTime=\"7 seconds\")\n",
    "        .start()\n",
    ")\n",
    "query_building = (\n",
    "    building_6h\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/agg_building_6h\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/agg_building_6h\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c\n",
    "site_daily = (\n",
    "    predictions\n",
    "#         .withWatermark(\"weather_ts\", \"1 day\")\n",
    "        .groupBy(\"site_id\", F.to_date(\"weather_ts\").alias(\"date\"))\n",
    "        .agg(Fsum(\"log_power_usage\").alias(\"total_power_day\"))\n",
    ")\n",
    "\n",
    "query_site = (\n",
    "    site_daily\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", \"data/agg_site_daily\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/agg_site_daily\")\n",
    "        .trigger(processingTime=\"14 seconds\")\n",
    "        .start()\n",
    ")\n",
    "# --- Print to console every 14 seconds ---\n",
    "site_daily_console = (\n",
    "    site_daily\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .option(\"numRows\", 10)\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/agg_site_daily_console\")\n",
    "        .trigger(processingTime=\"14 seconds\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tSave the data from 6 to Parquet files as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Spark Streaming Watermark Demo for watermark and Q6,Q7\n",
    "# Granularity Reduction Consumer for print to memory (in jupyter notebook instead of terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a(save 6a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b(save 6b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c(save 6c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.awaitAnyTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tRead the parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
