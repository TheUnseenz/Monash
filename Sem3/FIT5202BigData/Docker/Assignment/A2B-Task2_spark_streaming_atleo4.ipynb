{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform a prediction.    \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2B\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name) \\\n",
    "                        .set(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWrite code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. building information) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from GPT\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(15, 4), False),\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(6, 4), True)\n",
    "])\n",
    "\n",
    "# 3. Weather Table\n",
    "# weather_schema = StructType([\n",
    "#     StructField(\"site_id\", StringType(), False),\n",
    "#     StructField(\"timestamp\", StringType(), False),\n",
    "#     StructField(\"air_temperature\", StringType(), True),\n",
    "#     StructField(\"cloud_coverage\", StringType(), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "#     StructField(\"dew_temperature\", StringType(), True),\n",
    "#     StructField(\"sea_level_pressure\", StringType(), True),\n",
    "#     StructField(\"wind_direction\", StringType(), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "#     StructField(\"wind_speed\", StringType(), True),\n",
    "#     StructField(\"weather_ts\", StringType(), False) # new field\n",
    "# ])\n",
    "\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"cloud_coverage\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"dew_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 3), True),\n",
    "    StructField(\"wind_direction\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"wind_speed\", DecimalType(5, 3), True),\n",
    "    StructField(\"weather_ts\", TimestampType(), False) # new field\n",
    "])\n",
    "\n",
    "\n",
    "# buildings_df = spark.read.csv(\n",
    "#     \"data/building_information.csv\",\n",
    "#     header=True,\n",
    "#     schema=buildings_schema\n",
    "# )\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/new_building_information.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUsing the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'weather_ts' column, you shall receive it as an Int type. Load the new building information CSV file into a dataframe. Then, the data frames should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#configuration\n",
    "hostip = \"192.168.0.6\"#\"10.192.90.63\" #change to your machine IP address\n",
    "topic = 'weather_data'\n",
    "\n",
    "df_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "df_str = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "# df_parsed = df_str.withColumn(\n",
    "#     \"data\",\n",
    "#     F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema))\n",
    "# )\n",
    "# df_exploded = df_parsed.select(F.explode(F.col(\"data\")).alias(\"record\"))\n",
    "# df_final = df_exploded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weather_stream = (\n",
    "    df_str\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema)))\n",
    "    .select(F.explode(F.col(\"data\")).alias(\"r\"))\n",
    "    .select(\"r.*\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse a watermark on weather_ts, if data points are received 5 seconds late, discard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stream = weather_stream.withWatermark(\"weather_ts\", '5 seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform the necessary transformation you used in A2A. (note: every student may have used different features, feel free to reuse the code you have written in A2A. If you built an end-to-end pipeline, you can ignore this task.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from A2A which was from GPT\n",
    "# Get global_means, site_month_means, site_means\n",
    "# weather_df is history, weather_stream is current\n",
    "\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_df = weather_df.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Choose which columns to impute\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Compute global_means, site_month_means, site_means\n",
    "global_means = weather_df.select(\n",
    "    *[F.mean(c).alias(c) for c in impute_cols]\n",
    ").first().asDict()\n",
    "\n",
    "site_month_means = weather_df.groupBy(\"site_id\", \"month\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_month_mean\") for c in impute_cols]\n",
    ")\n",
    "\n",
    "site_means = weather_df.groupBy(\"site_id\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_mean\") for c in impute_cols]\n",
    ")\n",
    "    \n",
    "# Skip Garbage collection\n",
    "# del site_month_means\n",
    "# del site_means\n",
    "# del global_means\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# Transform weather_stream\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_stream = weather_stream.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Step 1: site_id + month\n",
    "weather_stream = weather_stream.join(site_month_means, on=[\"site_id\", \"month\"], how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_month_mean\"))\n",
    "    ).drop(f\"{c}_site_month_mean\")\n",
    "    \n",
    "# Step 2: site_id\n",
    "weather_stream = weather_stream.join(site_means, on=\"site_id\", how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_mean\"))\n",
    "    ).drop(f\"{c}_site_mean\")\n",
    "\n",
    "# Step 3: global fallback\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.lit(global_means[c]))\n",
    "    )\n",
    "# Aggregate by time bucket\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "#     .withWatermark(\"weather_ts\", \"5 seconds\")\n",
    "    .groupBy(\n",
    "        \"site_id\", \"date\", \"time\", \"month\",\n",
    "        F.window(\"weather_ts\", \"5 seconds\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.mean(\"air_temperature\").cast(DecimalType(5, 3)).alias(\"air_temperature\"),\n",
    "        F.mean(\"cloud_coverage\").cast(DecimalType(5, 3)).alias(\"cloud_coverage\"),\n",
    "        F.mean(\"dew_temperature\").cast(DecimalType(5, 3)).alias(\"dew_temperature\"),\n",
    "        F.mean(\"sea_level_pressure\").cast(DecimalType(8, 3)).alias(\"sea_level_pressure\"),\n",
    "        F.mean(\"wind_direction\").cast(DecimalType(5, 3)).alias(\"wind_direction\"),\n",
    "        F.mean(\"wind_speed\").cast(DecimalType(5, 3)).alias(\"wind_speed\"),     \n",
    "    )\n",
    ")\n",
    "\n",
    "# Add custom columns\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "    .withColumn(\"dew_depression\", F.col(\"air_temperature\") - F.col(\"dew_temperature\"))\n",
    "    .withColumn(\"nonideal_temp\", (F.col(\"air_temperature\") - 18)**2)\n",
    "    .drop(\"air_temperature\")\n",
    "    .drop(\"dew_temperature\")\n",
    ")\n",
    "\n",
    "# No need to add median temp and peak-offpeak as our pipeline model later does not use them\n",
    "feature_df = buildings_df.join(weather_stream, [\"site_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tLoad your pipeline model and perform the following aggregations:  \n",
    "a)\tPrint the prediction from your model as a stream comes in.  \n",
    "b)\tEvery 7 seconds, print the total energy consumption for each 6-hour interval, aggregated by building, and print 20 records. (Note: This is simulating energy data each day in a week)  \n",
    "c)\tEvery 14 seconds, for each site, print the daily total energy consumption.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to predict (and print) from my earlier created model as the stream comes in. the model was earlier saved as:\n",
    "# ```\n",
    "# # Define the path to save the model\n",
    "# model_path = \"models/best_model_rmsle\"\n",
    "\n",
    "# # Save the better trained model. \n",
    "# if rf_cv_metrics[\"rmsle\"] < gbt_cv_metrics[\"rmsle\"]:\n",
    "#     rf_cv_model.bestModel.save(model_path)\n",
    "# else:\n",
    "#     gbt_cv_model.bestModel.save(model_path)\n",
    "# ```\n",
    "# where exactly do i load and run the model (was it model.transform(feature_df)?) to predict and append another column \"value\"?\n",
    "# feature_df is df_final as above, with some imputation and a join done. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a\n",
    "# --- 1. Spark + model setup ---\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "model = PipelineModel.load(\"models/best_model_rmsle\")\n",
    "\n",
    "# --- 3. Apply model ---\n",
    "predictions = model.transform(feature_df).withColumnRenamed(\"prediction\", \"log_power_usage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 19\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# --- 4. Output ---\u001b[39;00m\n\u001b[1;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      8\u001b[0m     predictions\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_power_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     17\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.abspath(\"checkpoints/weather_stream\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(10,False)\n",
    "# --- 4. Output ---\n",
    "query = (\n",
    "    predictions\n",
    "        .select(\"features\", \"log_power_usage\")\n",
    "        .writeStream\n",
    "#         .foreachBatch(foreach_batch_function)\n",
    "        .format(\"console\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir)\n",
    "        .option(\"truncate\", False)\n",
    "        .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def foreach_batch_function(df, epoch_id):\n",
    "#     df.show(10,False)\n",
    "# query = (\n",
    "#     weather_stream.writeStream\n",
    "#         .format(\"console\")\n",
    "#         .foreachBatch(foreach_batch_function)\n",
    "#         .outputMode(\"append\")\n",
    "#         .start()\n",
    "# )\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tSave the data from 6 to Parquet files as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a(save 6a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b(save 6b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c(save 6c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tRead the parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
