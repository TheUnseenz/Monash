{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 2025 S2 Assignment 1 : Analysing Australian Property Market Data\n",
    "\n",
    "## Table of Contents\n",
    "* [Part 1 : Working with RDD](#part-1)  \n",
    "    - [1.1 Data Preparation and Loading](#1.1)  \n",
    "    - [1.2 Data Partitioning in RDD](#1.2)  \n",
    "    - [1.3 Query/Analysis](#1.3)  \n",
    "* [Part 2 : Working with DataFrames](#2-dataframes)  \n",
    "    - [2.1 Data Preparation and Loading](#2-dataframes)  \n",
    "    - [2.2 Query/Analysis](#2.2)  \n",
    "* [Part 3 :  RDDs vs DataFrame vs Spark SQL](#part-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Feel free to add Code/Markdown cells as you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Working with RDDs (30%) <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Working with RDD\n",
    "In this section, you will need to create RDDs from the given datasets, perform partitioning in these RDDs and use various RDD operations to answer the queries. \n",
    "\n",
    "1.1.1 Data Preparation and Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.\tWrite the code to create a SparkContext object using SparkSession. To create a SparkSession, you first need to build a SparkConf object that contains information about your application. Use Melbourne time as the session timezone. Give your application an appropriate name and run Spark locally with 4 cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Load the CSV and JSON files into multiple RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [\"data/council.json\", \"data/nsw_property_price.csv\", \"data/property_purpose.json\", \"data/zoning.json\"]\n",
    "rdds = []  \n",
    "headers = {}\n",
    "for file in files:\n",
    "    # get file extension\n",
    "    ext = os.path.splitext(file)[1].lower()  \n",
    "    # filter out whitespace\n",
    "    rdd = (\n",
    "        sc.textFile(file)\n",
    "          .map(lambda x: x.strip().rstrip(\",\").replace(\"{\", \"\").replace(\"}\", \"\"))\n",
    "          .filter(lambda x: x != \"\")\n",
    "    )\n",
    "\n",
    "    if ext == \".json\":\n",
    "        rdds.append((rdd, \"json\", file))\n",
    "    elif ext == \".csv\":\n",
    "        rdds.append((rdd, \"csv\", file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 For each RDD, remove the header rows and display the total count and the first 8 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/council.json: Total count=220, first 8 rows:\n",
      "[{'council_id': '1', 'council_name': '003'}, {'council_id': '3', 'council_name': '013'}, {'council_id': '5', 'council_name': '020'}, {'council_id': '7', 'council_name': '022'}, {'council_id': '9', 'council_name': '026'}, {'council_id': '11', 'council_name': '029'}, {'council_id': '13', 'council_name': '034'}, {'council_id': '15', 'council_name': '037'}]\n",
      "\n",
      "data/nsw_property_price.csv: Total count=4854814, first 8 rows:\n",
      "[{'property_id': '4270509', 'purchase_price': '1400000.00', 'address': '8 C NYARI RD, KENTHURST', 'post_code': '2156', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '2.044', 'area_type': 'H', 'iso_contract_date': '2023-12-14', 'iso_settlement_date': '2024-02-14', 'nature_of_property': 'V', 'legal_description': '2/1229857', 'id': '142', 'council_id': '200', 'purpose_id': '9922', 'zone_id': '53'}, {'property_id': '4329326', 'purchase_price': '1105000.00', 'address': '82 CAMARERO ST, BOX HILL', 'post_code': '2765', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '300.2', 'area_type': 'M', 'iso_contract_date': '2024-01-12', 'iso_settlement_date': '2024-02-09', 'nature_of_property': 'R', 'legal_description': '1119/1256791', 'id': '143', 'council_id': '200', 'purpose_id': '7071', 'zone_id': '41'}, {'property_id': '1864112', 'purchase_price': '55000.00', 'address': '321 AUBURN ST, MOREE', 'post_code': '2400', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '847.3', 'area_type': 'M', 'iso_contract_date': '2023-09-15', 'iso_settlement_date': '2024-01-29', 'nature_of_property': 'R', 'legal_description': '17/36061', 'id': '192', 'council_id': '168', 'purpose_id': '7071', 'zone_id': '40'}, {'property_id': '1869899', 'purchase_price': '680000.00', 'address': '207 GWYDIRFIELD RD, MOREE', 'post_code': '2400', 'property_type': 'house', 'strata_lot_number': '', 'property_name': 'SPRINGVALE', 'area': '2.023', 'area_type': 'H', 'iso_contract_date': '2024-01-19', 'iso_settlement_date': '2024-02-09', 'nature_of_property': 'R', 'legal_description': '6/251911', 'id': '193', 'council_id': '168', 'purpose_id': '7071', 'zone_id': '48'}, {'property_id': '1867775', 'purchase_price': '220000.00', 'address': '90 MERRIWA ST, BOGGABILLA', 'post_code': '2409', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '2023.0', 'area_type': 'M', 'iso_contract_date': '2023-12-08', 'iso_settlement_date': '2024-02-09', 'nature_of_property': 'R', 'legal_description': '1/1/758127', 'id': '194', 'council_id': '168', 'purpose_id': '7071', 'zone_id': '52'}, {'property_id': '2738374', 'purchase_price': '690000.00', 'address': '10 PETOSTRUM PL, PORT MACQUARIE', 'post_code': '2444', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '672.8', 'area_type': 'M', 'iso_contract_date': '2023-12-14', 'iso_settlement_date': '2024-02-14', 'nature_of_property': 'R', 'legal_description': '94/815767', 'id': '242', 'council_id': '184', 'purpose_id': '7071', 'zone_id': '40'}, {'property_id': '1608665', 'purchase_price': '661000.00', 'address': '71 MULYAN ST, COMO', 'post_code': '2226', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '561.7', 'area_type': 'M', 'iso_contract_date': '2013-03-23', 'iso_settlement_date': '2013-05-09', 'nature_of_property': '3', 'legal_description': '2/11301', 'id': '26440', 'council_id': '196', 'purpose_id': '4301', 'zone_id': '2'}, {'property_id': '638909', 'purchase_price': '780208.00', 'address': '38 DUFFY AVE, THORNLEIGH', 'post_code': '2120', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '3113.2', 'area_type': 'M', 'iso_contract_date': '2023-06-27', 'iso_settlement_date': '2024-02-09', 'nature_of_property': 'V', 'legal_description': '6, 7/533837 3, 4/1047718', 'id': '440', 'council_id': '147', 'purpose_id': '9922', 'zone_id': '23'}]\n",
      "\n",
      "data/property_purpose.json: Total count=865, first 8 rows:\n",
      "[{'purpose_id': '1', 'primary_purpose': ''}, {'purpose_id': '29', 'primary_purpose': '10 FLATS'}, {'purpose_id': '115', 'primary_purpose': '2'}, {'purpose_id': '167', 'primary_purpose': '2 FLATS'}, {'purpose_id': '193', 'primary_purpose': '2 SHOPS'}, {'purpose_id': '273', 'primary_purpose': '3 FLATS'}, {'purpose_id': '312', 'primary_purpose': '4 FLATS'}, {'purpose_id': '361', 'primary_purpose': '6 FLATS'}]\n",
      "\n",
      "data/zoning.json: Total count=71, first 8 rows:\n",
      "[{'zoning_id': '1', 'zoning': ''}, {'zoning_id': '3', 'zoning': 'AGB'}, {'zoning_id': '5', 'zoning': 'B1'}, {'zoning_id': '7', 'zoning': 'B3'}, {'zoning_id': '9', 'zoning': 'B5'}, {'zoning_id': '11', 'zoning': 'B7'}, {'zoning_id': '13', 'zoning': 'C'}, {'zoning_id': '15', 'zoning': 'C2'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def safe_dict(it):\n",
    "    \"\"\"Safely turn iterable of kv pairs into dict, skipping malformed entries.\"\"\"\n",
    "    d = {}\n",
    "    for kv in it:\n",
    "        if isinstance(kv, tuple) and len(kv) == 2:\n",
    "            k, v = kv\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "def parse_csv_line(line: str):\n",
    "    \"\"\"Safely parse a CSV line, handling commas inside quoted fields.\"\"\"\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', delimiter=',')\n",
    "    return next(reader)\n",
    "\n",
    "def process_rdd(rdd, ext, filename):\n",
    "    # remove header\n",
    "    header = rdd.first()\n",
    "    clean_header = header.split(\"\\\\n\", 1)[0]\n",
    "    if clean_header.startswith('\"') and not clean_header.endswith('\"'):\n",
    "        clean_header += '\"'  # restore the closing quote\n",
    "    headers[filename] = clean_header\n",
    "    lines = rdd.filter(lambda s: s != header)\n",
    "\n",
    "    \n",
    "    if ext == \"json\":        \n",
    "        # robust key:value parsing (handles both \"key : value\" and \"key\": \"value\")\n",
    "        kv = (\n",
    "            lines\n",
    "            .map(lambda s: s.strip())\n",
    "            .filter(lambda s: \":\" in s)              # accept any colon, with or without spaces\n",
    "            .map(lambda s: s.split(\":\", 1))          # split once, keep right side intact\n",
    "            .filter(lambda kv: len(kv) == 2)         # keep only well-formed pairs\n",
    "            .map(lambda kv: (kv[0].strip(' \"\\',{}'), kv[1].strip(' \"\\',{}')))\n",
    "        )\n",
    "\n",
    "        # group into records (assumes each record spans 2 lines)\n",
    "        grouped = (\n",
    "            kv.zipWithIndex()\n",
    "              .map(lambda x: (x[1] // 2, x[0]))\n",
    "              .groupByKey()\n",
    "              .mapValues(safe_dict)\n",
    "              .values()\n",
    "        )\n",
    "\n",
    "        print(f\"{filename}: Total count={grouped.count()}, first 8 rows:\\n{grouped.take(8)}\\n\")\n",
    "        return grouped\n",
    "    \n",
    "    if ext == \"csv\":\n",
    "        # use robust CSV parsing instead of naive split\n",
    "        fieldnames = parse_csv_line(headers[filename])\n",
    "        lines = (\n",
    "            lines.map(lambda row: dict(zip(fieldnames, parse_csv_line(row))))\n",
    "        )\n",
    "        \n",
    "        print(f\"{filename}: Total count={lines.count()}, first 8 rows:\\n{lines.take(8)}\\n\")\n",
    "        return lines\n",
    "\n",
    "# Replace items in rdds\n",
    "rdds = [\n",
    "    (process_rdd(rdd, ext, filename), ext, filename)\n",
    "    for (rdd, ext, filename) in rdds\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.4 Drop records with invalid information: purpose_id or council_id is null, empty, or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/council.json: Raw=220, Filtered=220\n",
      "data/nsw_property_price.csv: Raw=4854814, Filtered=4828278\n",
      "data/property_purpose.json: Raw=865, Filtered=865\n",
      "data/zoning.json: Raw=71, Filtered=71\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def valid_record(rec):\n",
    "    for k, v in rec.items():\n",
    "        if k.endswith(\"_id\"):\n",
    "            if v is None:\n",
    "                return False\n",
    "            s = str(v).strip()\n",
    "\n",
    "            # Must be digits only\n",
    "            if not re.fullmatch(r\"[0-9]+\", s):\n",
    "                return False\n",
    "\n",
    "            try:\n",
    "                if int(s) < 1:\n",
    "                    return False\n",
    "            except ValueError:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_rdd(rdd, ext, filename):\n",
    "    filtered = rdd.filter(valid_record)    \n",
    "    print(f\"{filename}: Raw={rdd.count()}, Filtered={filtered.count()}\")\n",
    "    return filtered\n",
    "\n",
    "# Apply filtering\n",
    "rdds = [\n",
    "    (filter_rdd(rdd, ext, filename), ext, filename)\n",
    "    for (rdd, ext, filename) in rdds\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD <a class=\"anchor\" name=\"1.2\"></a>\n",
    "1.2.1 For each RDD, using Spark’s default partitioning, print out the total number of partitions and the number of records in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/council.json default partitions: 2,\n",
      "'data/nsw_property_price.csv default partitions: 19,\n",
      "'data/property_purpose.json default partitions: 2,\n",
      "'data/zoning.json default partitions: 2,\n"
     ]
    }
   ],
   "source": [
    "for rdd, ext, filename in rdds:\n",
    "    print(f\"'{filename} default partitions: {rdd.getNumPartitions()},\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Answer the following questions:   \n",
    "a) How many partitions do the above RDDs have?  \n",
    "b) How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy? Can you explain why it is partitioned in this number?   \n",
    "c) Assuming we are querying the dataset based on <strong> Property Price</strong>, can you think of a better strategy for partitioning the data based on your available hardware resources?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for a)  \n",
    "The csv file has 19 partitions, while all of the json files have 2 partitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for b)  \n",
    "The data in these RDDs is partitioned according to their file size by default - to have up to 32 MB of data per partition, while the json files were merged together for 2 partitions in total due to their small file size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for c)\n",
    "Since the dataset would be queried by property price, the data could be partitioned into price buckets, which would prune operations to match user request patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 Create a user-defined function (UDF) to transform the date strings from ISO format (YYYY-MM-DD) (e.g. 2025-01-01) to Australian format (DD/Mon/YYYY) (e.g. 01/Jan/2025), then call the UDF to transform two date columns (iso_contract_date and iso_settlement_date) to contract_date and settlement_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def iso_to_aus(iso_date: str) -> str:\n",
    "    try:\n",
    "        dt = datetime.strptime(iso_date, \"%Y-%m-%d\")\n",
    "        return dt.strftime(\"%d/%b/%Y\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "property_price_rdd = (\n",
    "    next(rdd for (rdd, ext, fname) in rdds if fname == \"data/nsw_property_price.csv\")\n",
    "    .map(lambda row: {\n",
    "        **row,\n",
    "        **({\"contract_date\": iso_to_aus(row[\"iso_contract_date\"])} if \"iso_contract_date\" in row else {}),\n",
    "        **({\"settlement_date\": iso_to_aus(row[\"iso_settlement_date\"])} if \"iso_settlement_date\" in row else {}),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis <a class=\"anchor\" name=\"1.3\"></a>\n",
    "For this part, write relevant RDD operations to answer the following queries.\n",
    "\n",
    "1.3.1 Extract the Month (Jan-Dec) information and print the total number of sales by contract date for each Month. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan: Number of sales=231293, Total sales value=149286257291.0\n",
      "Feb: Number of sales=385415, Total sales value=283257505935.0\n",
      "Mar: Number of sales=460686, Total sales value=362170672422.0\n",
      "Apr: Number of sales=382178, Total sales value=288007815243.0\n",
      "May: Number of sales=449308, Total sales value=374297279808.0\n",
      "Jun: Number of sales=407721, Total sales value=369932834504.0\n",
      "Jul: Number of sales=404384, Total sales value=356746327190.0\n",
      "Aug: Number of sales=413422, Total sales value=355777547916.0\n",
      "Sep: Number of sales=423248, Total sales value=346086151514.0\n",
      "Oct: Number of sales=432387, Total sales value=346873898401.0\n",
      "Nov: Number of sales=446805, Total sales value=414619066176.0\n",
      "Dec: Number of sales=390848, Total sales value=432696382032.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_month_name(date_str: str) -> str:\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str, \"%d/%b/%Y\")  # AUS format\n",
    "        return dt.strftime(\"%b\")   # \"Jan\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_float(x: str) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "# Map to (month, (count, total_purchase_price)), filtering out blanks\n",
    "month_metrics_rdd = (\n",
    "    property_price_rdd\n",
    "    .map(lambda row: (\n",
    "        extract_month_name(row[\"contract_date\"]),\n",
    "        (1, safe_float(row[\"purchase_price\"]))\n",
    "    ))\n",
    "    .filter(lambda x: x[0] is not None and x[1] is not None)\n",
    ")\n",
    "\n",
    "# Reduce: sum counts and purchase prices\n",
    "monthly_metrics = month_metrics_rdd.reduceByKey(\n",
    "    lambda a, b: (a[0] + b[0], a[1] + b[1])\n",
    ")\n",
    "\n",
    "# Collect and sort by calendar order\n",
    "month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "monthly_metrics_sorted = sorted(\n",
    "    monthly_metrics.collect(),\n",
    "    key=lambda x: month_order.index(x[0])\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for month, (count, total) in monthly_metrics_sorted:\n",
    "    print(f\"{month}: Number of sales={count}, Total sales value={total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 Which 5 councils have the largest number of houses? Show their name and the total number of houses. (Note: Each house may appear multiple times if there are more than one sales, you should only count them once.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACKTOWN (council_id=100): 89814 houses\n",
      "LAKE MACQUARIE (council_id=157): 57690 houses\n",
      "THE HILLS SHIRE (council_id=200): 54157 houses\n",
      "LIVERPOOL (council_id=162): 48081 houses\n",
      "PENRITH (council_id=183): 45283 houses\n"
     ]
    }
   ],
   "source": [
    "# Count unique properties per council\n",
    "council_house_counts = (\n",
    "    property_price_rdd\n",
    "    .filter(lambda row: row[\"property_type\"] == \"house\") # Restrict to houses\n",
    "    .map(lambda row: (row[\"council_id\"], row[\"property_id\"])) # Extract (council_id, property_id) pairs\n",
    "    .distinct() # Deduplicate by council_id + property_id\n",
    "    .map(lambda x: (x[0], 1))   # Count unique properties per council\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "# Load council_id → council_name mapping from JSON RDD\n",
    "council_rdd = next(rdd for (rdd, ext, fname) in rdds if fname == \"data/council.json\")\n",
    "council_name_map = council_rdd.map(\n",
    "    lambda row: (row[\"council_id\"], row[\"council_name\"])\n",
    ")\n",
    "\n",
    "# Join counts with names\n",
    "council_with_names = council_house_counts.join(council_name_map)\n",
    "\n",
    "# Get top 5 councils by number of houses\n",
    "top5_councils = council_with_names.takeOrdered(\n",
    "    5,\n",
    "    key=lambda x: -x[1][0]   # sort by house count descending\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for council_id, (count, name) in top5_councils:\n",
    "    print(f\"{name} (council_id={council_id}): {count} houses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Working with DataFrames (45%) <a class=\"anchor\" name=\"2-dataframes\"></a>\n",
    "In this section, you need to load the given datasets into PySpark DataFrames and use DataFrame functions to answer the queries.\n",
    "### 2.1 Data Preparation and Loading\n",
    "\n",
    "2.1.1. Load the CSV/JSON files into separate dataframes. When you create your dataframes, please refer to the metadata file and think about the appropriate data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import explode\n",
    "files = [\"data/council.json\", \"data/nsw_property_price.csv\", \"data/property_purpose.json\", \"data/zoning.json\"]\n",
    "dfs = []  \n",
    "for file in files:\n",
    "    ext = os.path.splitext(file)[1].lower()  # get file extension\n",
    "    \n",
    "    if ext == \".json\":        \n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file)\n",
    "        # Root is a single array column, so flatten it\n",
    "        df = df.select(explode(df[df.columns[0]]).alias(\"data\")).select(\"data.*\")\n",
    "        dfs.append((df, \"json\", file))\n",
    "        \n",
    "        if file == \"data/property_purpose.json\":\n",
    "            property_purpose_df = df\n",
    "    elif ext == \".csv\":\n",
    "        df = spark.read.csv(\n",
    "            file,\n",
    "            header=True,\n",
    "            inferSchema=True,\n",
    "            quote='\"',\n",
    "            escape='\"',\n",
    "            multiLine=True\n",
    "        )\n",
    "        dfs.append((df, \"csv\", file))\n",
    "        if file == \"data/nsw_property_price.csv\":\n",
    "            property_price_df = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Display the schema of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[council_id: bigint, council_name: string]\n",
      "root\n",
      " |-- council_id: long (nullable = true)\n",
      " |-- council_name: string (nullable = true)\n",
      "\n",
      "DataFrame[property_id: int, purchase_price: double, address: string, post_code: int, property_type: string, strata_lot_number: int, property_name: string, area: double, area_type: string, iso_contract_date: date, iso_settlement_date: date, nature_of_property: string, legal_description: string, id: int, council_id: int, purpose_id: int, zone_id: int]\n",
      "root\n",
      " |-- property_id: integer (nullable = true)\n",
      " |-- purchase_price: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- post_code: integer (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- strata_lot_number: integer (nullable = true)\n",
      " |-- property_name: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- area_type: string (nullable = true)\n",
      " |-- iso_contract_date: date (nullable = true)\n",
      " |-- iso_settlement_date: date (nullable = true)\n",
      " |-- nature_of_property: string (nullable = true)\n",
      " |-- legal_description: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- council_id: integer (nullable = true)\n",
      " |-- purpose_id: integer (nullable = true)\n",
      " |-- zone_id: integer (nullable = true)\n",
      "\n",
      "DataFrame[primary_purpose: string, purpose_id: bigint]\n",
      "root\n",
      " |-- primary_purpose: string (nullable = true)\n",
      " |-- purpose_id: long (nullable = true)\n",
      "\n",
      "DataFrame[zoning: string, zoning_id: bigint]\n",
      "root\n",
      " |-- zoning: string (nullable = true)\n",
      " |-- zoning_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df, ext, filename in dfs:\n",
    "    print(df)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is large, do you need all columns? How to optimize memory usage? Do you need a customized data partitioning strategy? (Note: Think about those questions but you don’t need to answer these questions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 QueryAnalysis  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "Implement the following queries using dataframes. You need to be able to perform operations like transforming, filtering, sorting, joining and group by using the functions provided by the DataFrame API. For each task, display the first 5 results where no output is specified.\n",
    "\n",
    "2.2.1. The area column has two types: (H, A and M): 1 H is one hectare = 10000 sqm, 1A is one acre = 4000 sqm, 1 M is one sqm. Unify the unit to sqm and create a new column called area_sqm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/council.json: Raw=220, Filtered=220\n",
      "data/nsw_property_price.csv: Raw=4854814, Filtered=4828278\n",
      "data/property_purpose.json: Raw=865, Filtered=865\n",
      "data/zoning.json: Raw=71, Filtered=71\n"
     ]
    }
   ],
   "source": [
    "# Filter all dataframes first\n",
    "from pyspark.sql.functions import col, trim, length, regexp_replace\n",
    "\n",
    "def filter_df(df, ext, filename):\n",
    "    id_cols = [c for c in df.columns if c.endswith(\"_id\")]\n",
    "\n",
    "    condition = None\n",
    "    for id_col in id_cols:\n",
    "        # Force to string and trim\n",
    "        id_str = trim(col(id_col).cast(\"string\"))\n",
    "\n",
    "        # Must be only digits (no \"/\" or other chars)\n",
    "        # length > 0 to reject empty\n",
    "        this_cond = (\n",
    "            id_str.isNotNull() &\n",
    "            (length(id_str) > 0) &\n",
    "            id_str.rlike(\"^[0-9]+$\") &\n",
    "            (id_str.cast(\"bigint\") >= 1)\n",
    "        )\n",
    "\n",
    "        # Combine conditions: ALL *_id columns must satisfy\n",
    "        condition = this_cond if condition is None else (condition & this_cond)\n",
    "\n",
    "    filtered_df = df.filter(condition) if condition is not None else df\n",
    "\n",
    "    print(f\"{filename}: Raw={df.count()}, Filtered={filtered_df.count()}\")\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Apply filtering to all DataFrames\n",
    "dfs = [\n",
    "    (filter_df(df, ext, filename), ext, filename)\n",
    "    for (df, ext, filename) in dfs\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+---------+-------------+-----------------+-------------+------+---------+-----------------+-------------------+------------------+-----------------+---+----------+----------+-------+--------+\n",
      "|property_id|purchase_price|             address|post_code|property_type|strata_lot_number|property_name|  area|area_type|iso_contract_date|iso_settlement_date|nature_of_property|legal_description| id|council_id|purpose_id|zone_id|area_sqm|\n",
      "+-----------+--------------+--------------------+---------+-------------+-----------------+-------------+------+---------+-----------------+-------------------+------------------+-----------------+---+----------+----------+-------+--------+\n",
      "|    4270509|     1400000.0|8 C NYARI RD, KEN...|     2156|        house|             NULL|         NULL| 2.044|        H|       2023-12-14|         2024-02-14|                 V|        2/1229857|142|       200|      9922|     53| 20440.0|\n",
      "|    4329326|     1105000.0|82 CAMARERO ST, B...|     2765|        house|             NULL|         NULL| 300.2|        M|       2024-01-12|         2024-02-09|                 R|     1119/1256791|143|       200|      7071|     41|   300.2|\n",
      "|    1864112|       55000.0|321 AUBURN ST, MOREE|     2400|        house|             NULL|         NULL| 847.3|        M|       2023-09-15|         2024-01-29|                 R|         17/36061|192|       168|      7071|     40|   847.3|\n",
      "|    1869899|      680000.0|207 GWYDIRFIELD R...|     2400|        house|             NULL|   SPRINGVALE| 2.023|        H|       2024-01-19|         2024-02-09|                 R|         6/251911|193|       168|      7071|     48| 20230.0|\n",
      "|    1867775|      220000.0|90 MERRIWA ST, BO...|     2409|        house|             NULL|         NULL|2023.0|        M|       2023-12-08|         2024-02-09|                 R|       1/1/758127|194|       168|      7071|     52|  2023.0|\n",
      "+-----------+--------------+--------------------+---------+-------------+-----------------+-------------+------+---------+-----------------+-------------------+------------------+-----------------+---+----------+----------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "# Convert area + area_type to a unified area_sqm column in sqm.\n",
    "property_price_df = property_price_df.withColumn(\n",
    "    \"area_sqm\",\n",
    "    when(col(\"area_type\") == \"H\", col(\"area\") * 10000)\n",
    "    .when(col(\"area_type\") == \"A\", col(\"area\") * 4000)\n",
    "    .when(col(\"area_type\") == \"M\", col(\"area\"))\n",
    "    .otherwise(None)\n",
    ")    \n",
    "property_price_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2. <pre>The top five property types are: Residence, Vacant Land, Commercial, Farm and Industrial.\n",
    "However, for historical reason, they may have different strings in the database. Please update the primary_purpose with the following rules:\n",
    "a)\tAny purpose that has “HOME”, “HOUSE”, “UNIT” is classified as “Residence”;\n",
    "b)\t“Warehouse”, “Factory”,  “INDUST” should be changed to “Industrial”;\n",
    "c)\tAnything that contains “FARM”(i.e. FARMING), should be changed to “FARM”;\n",
    "d)\t“Vacant”, “Land” should be “Vacant Land”;\n",
    "e)\tAnything that has “COMM”, “Retail”, “Shop” or “Office” are “Cmmercial”.\n",
    "f)\tAll remaining properties, including null and empty purposes, are classified as “Others”.\n",
    "Show the count of each type in a table.\n",
    "(note: Some properties are multi-purpose, e.g. “House & Farm”, it’s fine to count them multiple times.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Purpose Counts (sorted) ===\n",
      "+----------+-----------------------+-------+\n",
      "|purpose_id|primary_purpose        |count  |\n",
      "+----------+-----------------------+-------+\n",
      "|12000     |Residence              |3887062|\n",
      "|12003     |Vacant Land            |553277 |\n",
      "|12005     |Others                 |166607 |\n",
      "|12004     |Commercial             |136655 |\n",
      "|12002     |Farm                   |67703  |\n",
      "|12001     |Industrial             |37017  |\n",
      "|12013     |Farm + Vacant Land     |6048   |\n",
      "|12009     |Residence + Commercial |197    |\n",
      "|12007     |Residence + Farm       |156    |\n",
      "|12012     |Industrial + Commercial|53     |\n",
      "|12008     |Residence + Vacant Land|39     |\n",
      "+----------+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# am i supposed to edit property_purpose JSON to consolidate their IDs into the 5 property type IDs?\n",
    "# Residence -> 7071\n",
    "# Industrial -> 4778\n",
    "# Farm -> 2941\n",
    "# Vacant Land -> 9922\n",
    "# Commercial -> 1704\n",
    "# Others -> 12000\n",
    "# Permutations of the above -> 12001 above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Find the top 20 properties that make the largest value gain, show their address, suburb, and value increased. To calculate the value gain, the property must have been sold multiple times, “value increase” can be calculated with the last sold price – first sold price, regardless the transactions in between. Print all 20 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.4 For each season, plot the median house price trend over the years. Seasons in Australia are defined as: (Spring: Sep-Nov, Summer: Dec-Feb, Autumn: Mar-May, Winter: Jun-Aug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 (Open Question) Explore the dataset freely and plot one diagram of your choice. Which columns (at least 2) are highly correlated to the sales price? Discuss the steps of your exploration and the results. (No word limit, please keep concise.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:  \n",
    "Intuitively, the variables we expect to affect property price are the year, the size of the property, the location \n",
    "of the property, and the type of property it is. However, measuring the effects of location on the property proved \n",
    "to be difficult for a number of reasons:\n",
    "    1. zone_id, council_id have many items in them with no clear description on what they are, \n",
    "    yet they contain many items in the dataset.\n",
    "    2. post_code has too many variables in it, making it difficult to analyze the entire dataset by postcode due \n",
    "    to processing power limitations.\n",
    "    3. many erroneous outliers exist in the dataset, which skews the trends towards \"which areas had erroneous entries\" \n",
    "    rather than actual meaningful data.\n",
    "To avoid this, we only analyze numeric variables and categorical variables with larger buckets to act as a noise \n",
    "averaging filter, which leads to measuring year, area_sqm and the updated primary_purpose from Q2.2.2.\n",
    "\n",
    "Even after that, with area_sqm showing a negative correlation, it shows that other conflating factors like location,\n",
    "type of property or simply the trend of inflation over the years is dominating the price analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 RDDs vs DataFrame vs Spark SQL (25%) <a class=\"anchor\" name=\"part-3\"></a>\n",
    "Implement the following complex queries using RDD, DataFrame in SparkSQL separately(choose two). Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference between these 2 approaches of your choice.\n",
    "(notes: You can write a multi-step query or a single complex query, the choice is yours. You can reuse the data frame in Part 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Query:\n",
    "<pre>\n",
    "A property investor wants to understand whether the property price and the settlement date are correlated. Here is the conditions:\n",
    "1)\tThe investor is only interested in the last 2 years of the dataset.\n",
    "2)\tThe investor is looking at houses under $2 million.\n",
    "3)\tPerform a bucketing of the settlement date (settlement – contract date\n",
    "range (15, 30, 45, 60, 90 days).\n",
    "4)\tPerform a bucketing of property prices in $500K(e.g. 0-$500K, $500K-$1M, $1M-$1.5M, $1.5-$2M)\n",
    "5)\tCount the number of transactions in each combination and print the result in the following format\n",
    "(Note: It’s fine to count the same property multiple times in this task, it’s based on sales transactions).\n",
    "(Note: You shall show the full table with 40 rows, 2 years *4 price bucket * 5 settlement bucket; 0 count should be displayed as 0, not omitted.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\tImplement the above query using two approaches of your choice separately and print the results. (Note: Outputs from both approaches of your choice are required, and the results should be the same.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----------------+-----------------+\n",
      "|year|price_bucket|settlement_bucket|transaction_count|\n",
      "+----+------------+-----------------+-----------------+\n",
      "|2023|0–500K      |16–30d           |5633             |\n",
      "|2023|0–500K      |31–45d           |6375             |\n",
      "|2023|0–500K      |46–60d           |1392             |\n",
      "|2023|0–500K      |61–90d           |1107             |\n",
      "|2023|0–500K      |≤15d             |3123             |\n",
      "|2023|1.5M–2M     |16–30d           |923              |\n",
      "|2023|1.5M–2M     |31–45d           |3889             |\n",
      "|2023|1.5M–2M     |46–60d           |1952             |\n",
      "|2023|1.5M–2M     |61–90d           |2149             |\n",
      "|2023|1.5M–2M     |≤15d             |316              |\n",
      "|2023|1M–1.5M     |16–30d           |2522             |\n",
      "|2023|1M–1.5M     |31–45d           |8417             |\n",
      "|2023|1M–1.5M     |46–60d           |3499             |\n",
      "|2023|1M–1.5M     |61–90d           |3310             |\n",
      "|2023|1M–1.5M     |≤15d             |699              |\n",
      "|2023|500K–1M     |16–30d           |8586             |\n",
      "|2023|500K–1M     |31–45d           |18100            |\n",
      "|2023|500K–1M     |46–60d           |5042             |\n",
      "|2023|500K–1M     |61–90d           |3636             |\n",
      "|2023|500K–1M     |≤15d             |2398             |\n",
      "|2024|0–500K      |16–30d           |269              |\n",
      "|2024|0–500K      |31–45d           |74               |\n",
      "|2024|0–500K      |46–60d           |2                |\n",
      "|2024|0–500K      |61–90d           |0                |\n",
      "|2024|0–500K      |≤15d             |251              |\n",
      "|2024|1.5M–2M     |16–30d           |36               |\n",
      "|2024|1.5M–2M     |31–45d           |25               |\n",
      "|2024|1.5M–2M     |46–60d           |0                |\n",
      "|2024|1.5M–2M     |61–90d           |0                |\n",
      "|2024|1.5M–2M     |≤15d             |24               |\n",
      "|2024|1M–1.5M     |16–30d           |123              |\n",
      "|2024|1M–1.5M     |31–45d           |97               |\n",
      "|2024|1M–1.5M     |46–60d           |2                |\n",
      "|2024|1M–1.5M     |61–90d           |0                |\n",
      "|2024|1M–1.5M     |≤15d             |73               |\n",
      "|2024|500K–1M     |16–30d           |433              |\n",
      "|2024|500K–1M     |31–45d           |237              |\n",
      "|2024|500K–1M     |46–60d           |4                |\n",
      "|2024|500K–1M     |61–90d           |0                |\n",
      "|2024|500K–1M     |≤15d             |233              |\n",
      "+----+------------+-----------------+-----------------+\n",
      "\n",
      "CPU times: user 41.4 ms, sys: 25.8 ms, total: 67.2 ms\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# === Parse contract & settlement dates ===\n",
    "df_dates = (\n",
    "    property_price_df\n",
    "    .withColumn(\"settlement_date\", F.col(\"iso_settlement_date\").cast(\"date\"))\n",
    "    .withColumn(\"contract_date\", F.col(\"iso_contract_date\").cast(\"date\"))\n",
    "    .withColumn(\"purchase_price\", F.col(\"purchase_price\").cast(\"double\"))\n",
    "    .withColumn(\"settlement_days\", F.datediff(\"settlement_date\", \"contract_date\"))\n",
    ")\n",
    "\n",
    "# === Restrict to houses ≤ 2025 ===\n",
    "df_dates = df_dates.filter(\n",
    "    (F.year(\"settlement_date\") <= 2025) & \n",
    "    (F.col(\"property_type\") == \"house\")\n",
    ")\n",
    "\n",
    "# === Find latest settlement_date and define cutoff (last 2 years) ===\n",
    "latest_date = df_dates.agg(F.max(\"settlement_date\")).collect()[0][0]\n",
    "\n",
    "if latest_date is None:\n",
    "    print(\"No valid settlement_date values found after filtering (<=2025).\")\n",
    "else:\n",
    "    cutoff_date = F.add_months(F.lit(latest_date), -24)\n",
    "    df_last2y = df_dates.filter(F.col(\"settlement_date\") >= cutoff_date)\n",
    "\n",
    "    # === Restrict to properties under $2M ===\n",
    "    df_under2m = df_last2y.filter(F.col(\"purchase_price\") < 2000000)\n",
    "\n",
    "    # === Settlement gap in days ===\n",
    "    df_gap = df_under2m.withColumn(\n",
    "        \"settlement_days\",\n",
    "        F.datediff(F.col(\"settlement_date\"), F.col(\"contract_date\"))\n",
    "    )\n",
    "\n",
    "    # === Settlement buckets ===\n",
    "    df_buckets = df_gap.withColumn(\n",
    "        \"settlement_bucket\",\n",
    "        F.when(F.col(\"settlement_days\") <= 15, \"≤15d\")\n",
    "         .when(F.col(\"settlement_days\") <= 30, \"16–30d\")\n",
    "         .when(F.col(\"settlement_days\") <= 45, \"31–45d\")\n",
    "         .when(F.col(\"settlement_days\") <= 60, \"46–60d\")\n",
    "         .when(F.col(\"settlement_days\") <= 90, \"61–90d\")\n",
    "    ).filter(F.col(\"settlement_bucket\").isNotNull())\n",
    "\n",
    "    # === Price buckets in 500K steps ===\n",
    "    df_buckets = df_buckets.withColumn(\n",
    "        \"price_bucket\",\n",
    "        F.when(F.col(\"purchase_price\") < 500000, \"0–500K\")\n",
    "         .when(F.col(\"purchase_price\") < 1000000, \"500K–1M\")\n",
    "         .when(F.col(\"purchase_price\") < 1500000, \"1M–1.5M\")\n",
    "         .when(F.col(\"purchase_price\") < 2000000, \"1.5M–2M\")\n",
    "    )\n",
    "\n",
    "    # === Year of sale from contract_date ===\n",
    "    df_buckets = df_buckets.withColumn(\"year\", F.year(\"contract_date\"))\n",
    "\n",
    "    # === Count transactions ===\n",
    "    df_counts = (\n",
    "        df_buckets\n",
    "        .groupBy(\"year\", \"price_bucket\", \"settlement_bucket\")\n",
    "        .agg(F.count(\"*\").alias(\"transaction_count\"))\n",
    "    )\n",
    "\n",
    "    # === Build full 40-row grid ===\n",
    "    years = [latest_date.year, latest_date.year - 1]\n",
    "    price_buckets = [\"0–500K\", \"500K–1M\", \"1M–1.5M\", \"1.5M–2M\"]\n",
    "    settlement_buckets = [\"≤15d\", \"16–30d\", \"31–45d\", \"46–60d\", \"61–90d\"]\n",
    "\n",
    "    years_df = spark.createDataFrame([(y,) for y in years], [\"year\"])\n",
    "    prices_df = spark.createDataFrame([(p,) for p in price_buckets], [\"price_bucket\"])\n",
    "    settles_df = spark.createDataFrame([(s,) for s in settlement_buckets], [\"settlement_bucket\"])\n",
    "\n",
    "    grid_df = years_df.crossJoin(prices_df).crossJoin(settles_df)\n",
    "\n",
    "    # === Join with counts, fill 0s ===\n",
    "    final_df = (\n",
    "        grid_df\n",
    "        .join(df_counts, [\"year\",\"price_bucket\",\"settlement_bucket\"], \"left\")\n",
    "        .fillna(0, subset=[\"transaction_count\"])\n",
    "        .orderBy(\"year\", \"price_bucket\", \"settlement_bucket\")\n",
    "    )\n",
    "\n",
    "    final_df.show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+-----------------+-----------------+\n",
      "|year|price_bucket|settlement_bucket|transaction_count|\n",
      "+----+------------+-----------------+-----------------+\n",
      "|2023|0–500K      |16–30d           |5633             |\n",
      "|2023|0–500K      |31–45d           |6375             |\n",
      "|2023|0–500K      |46–60d           |1392             |\n",
      "|2023|0–500K      |61–90d           |1107             |\n",
      "|2023|0–500K      |≤15d             |3123             |\n",
      "|2023|1.5M–2M     |16–30d           |923              |\n",
      "|2023|1.5M–2M     |31–45d           |3889             |\n",
      "|2023|1.5M–2M     |46–60d           |1952             |\n",
      "|2023|1.5M–2M     |61–90d           |2149             |\n",
      "|2023|1.5M–2M     |≤15d             |316              |\n",
      "|2023|1M–1.5M     |16–30d           |2522             |\n",
      "|2023|1M–1.5M     |31–45d           |8417             |\n",
      "|2023|1M–1.5M     |46–60d           |3499             |\n",
      "|2023|1M–1.5M     |61–90d           |3310             |\n",
      "|2023|1M–1.5M     |≤15d             |699              |\n",
      "|2023|500K–1M     |16–30d           |8586             |\n",
      "|2023|500K–1M     |31–45d           |18100            |\n",
      "|2023|500K–1M     |46–60d           |5042             |\n",
      "|2023|500K–1M     |61–90d           |3636             |\n",
      "|2023|500K–1M     |≤15d             |2398             |\n",
      "|2024|0–500K      |16–30d           |269              |\n",
      "|2024|0–500K      |31–45d           |74               |\n",
      "|2024|0–500K      |46–60d           |2                |\n",
      "|2024|0–500K      |61–90d           |0                |\n",
      "|2024|0–500K      |≤15d             |251              |\n",
      "|2024|1.5M–2M     |16–30d           |36               |\n",
      "|2024|1.5M–2M     |31–45d           |25               |\n",
      "|2024|1.5M–2M     |46–60d           |0                |\n",
      "|2024|1.5M–2M     |61–90d           |0                |\n",
      "|2024|1.5M–2M     |≤15d             |24               |\n",
      "|2024|1M–1.5M     |16–30d           |123              |\n",
      "|2024|1M–1.5M     |31–45d           |97               |\n",
      "|2024|1M–1.5M     |46–60d           |2                |\n",
      "|2024|1M–1.5M     |61–90d           |0                |\n",
      "|2024|1M–1.5M     |≤15d             |73               |\n",
      "|2024|500K–1M     |16–30d           |433              |\n",
      "|2024|500K–1M     |31–45d           |237              |\n",
      "|2024|500K–1M     |46–60d           |4                |\n",
      "|2024|500K–1M     |61–90d           |0                |\n",
      "|2024|500K–1M     |≤15d             |233              |\n",
      "+----+------------+-----------------+-----------------+\n",
      "\n",
      "CPU times: user 23.4 ms, sys: 17.8 ms, total: 41.2 ms\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Register the DataFrame as a SQL temp view\n",
    "property_price_df.createOrReplaceTempView(\"property_price\")\n",
    "\n",
    "# === Parse dates and restrict to houses ≤ 2025 ===\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW parsed AS\n",
    "    SELECT\n",
    "        CAST(iso_settlement_date AS DATE) AS settlement_date,\n",
    "        CAST(iso_contract_date AS DATE) AS contract_date,\n",
    "        CAST(purchase_price AS DOUBLE) AS purchase_price,\n",
    "        property_type\n",
    "    FROM property_price\n",
    "    WHERE year(CAST(iso_settlement_date AS DATE)) <= 2025\n",
    "      AND property_type = 'house'\n",
    "\"\"\")\n",
    "\n",
    "# === Latest settlement date ===\n",
    "latest_date = spark.sql(\"SELECT max(settlement_date) as max_date FROM parsed\").collect()[0][0]\n",
    "\n",
    "if latest_date is not None:\n",
    "    cutoff_date = latest_date.replace(year=latest_date.year - 2)\n",
    "\n",
    "    # === Filter last 2 years, < $2M ===\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW filtered AS\n",
    "        SELECT *,\n",
    "               datediff(settlement_date, contract_date) as settlement_days\n",
    "        FROM parsed\n",
    "        WHERE settlement_date >= DATE('{cutoff_date}')\n",
    "          AND purchase_price < 2000000\n",
    "    \"\"\")\n",
    "    # === Settlement bucketing ===\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW settlement_bucketed AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN settlement_days <= 15 THEN '≤15d'\n",
    "                WHEN settlement_days <= 30 THEN '16–30d'\n",
    "                WHEN settlement_days <= 45 THEN '31–45d'\n",
    "                WHEN settlement_days <= 60 THEN '46–60d'\n",
    "                WHEN settlement_days <= 90 THEN '61–90d'\n",
    "            END AS settlement_bucket\n",
    "        FROM filtered\n",
    "        WHERE settlement_days IS NOT NULL\n",
    "          AND settlement_days <= 90\n",
    "    \"\"\")\n",
    "\n",
    "    # === Price bucketing ===\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW bucketed AS\n",
    "        SELECT *,\n",
    "            CASE\n",
    "                WHEN purchase_price <  500000 THEN '0–500K'\n",
    "                WHEN purchase_price < 1000000 THEN '500K–1M'\n",
    "                WHEN purchase_price < 1500000 THEN '1M–1.5M'\n",
    "                WHEN purchase_price < 2000000 THEN '1.5M–2M'\n",
    "            END AS price_bucket,\n",
    "            year(contract_date) as year\n",
    "        FROM settlement_bucketed\n",
    "    \"\"\")\n",
    "\n",
    "    # === Group counts ===\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW counts AS\n",
    "        SELECT\n",
    "            year,\n",
    "            price_bucket,\n",
    "            settlement_bucket,\n",
    "            COUNT(*) AS transaction_count\n",
    "        FROM bucketed\n",
    "        GROUP BY year, price_bucket, settlement_bucket\n",
    "    \"\"\")\n",
    "\n",
    "    # === Build 40-row grid ===\n",
    "    years = [latest_date.year, latest_date.year - 1]\n",
    "    price_buckets = [\"0–500K\", \"500K–1M\", \"1M–1.5M\", \"1.5M–2M\"]\n",
    "    settlement_buckets = [\"≤15d\", \"16–30d\", \"31–45d\", \"46–60d\", \"61–90d\"]\n",
    "\n",
    "    years_df = spark.createDataFrame([(y,) for y in years], [\"year\"])\n",
    "    prices_df = spark.createDataFrame([(p,) for p in price_buckets], [\"price_bucket\"])\n",
    "    settles_df = spark.createDataFrame([(s,) for s in settlement_buckets], [\"settlement_bucket\"])\n",
    "\n",
    "    grid_df = years_df.crossJoin(prices_df).crossJoin(settles_df)\n",
    "    grid_df.createOrReplaceTempView(\"grid\")\n",
    "\n",
    "    # === Left join with counts ===\n",
    "    final_df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            g.year,\n",
    "            g.price_bucket,\n",
    "            g.settlement_bucket,\n",
    "            COALESCE(c.transaction_count, 0) as transaction_count\n",
    "        FROM grid g\n",
    "        LEFT JOIN counts c\n",
    "          ON g.year = c.year\n",
    "         AND g.price_bucket = c.price_bucket\n",
    "         AND g.settlement_bucket = c.settlement_bucket\n",
    "        ORDER BY g.year, g.price_bucket, g.settlement_bucket\n",
    "    \"\"\")\n",
    "\n",
    "    final_df.show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tWhich one is easier to implement, in your opinion? Log the time taken for each query, and observe the query execution time, among DataFrame and SparkSQL, which is faster and why? Please include proper references. (Maximum 500 words.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames and SparkSQL are quite similar in operations, and while I think SparkSQL has a more intuitive syntax usage, I am more used to DataFrames as they are more similar to the typical Python programming I am experienced in. Moreover, it's easier for me to debug errors in DataFrames, as they aren't wrapped in wrappers and I can access variables to check more directly.\n",
    "\n",
    "The query execution time is similar, as they have the same execution engine (Spark SQL and DataFrames - SpArk 4.0.1 Documentation, n.d.), so any difference in query speeds are just differences in how the logical plan was expressed. \n",
    "\n",
    "Spark SQL and DataFrames - SpArk 4.0.1 Documentation. (n.d.). https://spark.apache.org/docs/latest/sql-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ideas on the comparison\n",
    "\n",
    "Armbrust, M., Huai, Y., Liang, C., Xin, R., & Zaharia, M. (2015). Deep Dive into Spark SQL’s Catalyst Optimizer. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "Damji, J. (2016). A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets. Retrieved September 28, 2017, from https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "Data Flair (2017a). Apache Spark RDD vs DataFrame vs DataSet. Retrieved September 28, 2017, from http://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset\n",
    "\n",
    "Prakash, C. (2016). Apache Spark: RDD vs Dataframe vs Dataset. Retrieved September 28, 2017, from http://why-not-learn-something.blogspot.com.au/2016/07/apache-spark-rdd-vs-dataframe-vs-dataset.html\n",
    "\n",
    "Xin, R., & Rosen, J. (2015). Project Tungsten: Bringing Apache Spark Closer to Bare Metal. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Declaration:  \n",
    "I used ChatGPT to stitch my messy code blocks together to form neatly organized code, and to find cleaner ways of doing the same thing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
