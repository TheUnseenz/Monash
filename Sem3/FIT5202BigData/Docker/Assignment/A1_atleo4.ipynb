{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 2025 S2 Assignment 1 : Analysing Australian Property Market Data\n",
    "\n",
    "## Table of Contents\n",
    "* [Part 1 : Working with RDD](#part-1)  \n",
    "    - [1.1 Data Preparation and Loading](#1.1)  \n",
    "    - [1.2 Data Partitioning in RDD](#1.2)  \n",
    "    - [1.3 Query/Analysis](#1.3)  \n",
    "* [Part 2 : Working with DataFrames](#2-dataframes)  \n",
    "    - [2.1 Data Preparation and Loading](#2-dataframes)  \n",
    "    - [2.2 Query/Analysis](#2.2)  \n",
    "* [Part 3 :  RDDs vs DataFrame vs Spark SQL](#part-3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Feel free to add Code/Markdown cells as you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Working with RDDs (30%) <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Working with RDD\n",
    "In this section, you will need to create RDDs from the given datasets, perform partitioning in these RDDs and use various RDD operations to answer the queries. \n",
    "\n",
    "1.1.1 Data Preparation and Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.\tWrite the code to create a SparkContext object using SparkSession. To create a SparkSession, you first need to build a SparkConf object that contains information about your application. Use Melbourne time as the session timezone. Give your application an appropriate name and run Spark locally with 4 cores on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment1\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Load the CSV and JSON files into multiple RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "files = [\"data/council.json\", \"data/nsw_property_price.csv\", \"data/property_purpose.json\", \"data/zoning.json\"]\n",
    "rdds = []  \n",
    "headers = {}\n",
    "for file in files:\n",
    "    # get file extension\n",
    "    ext = os.path.splitext(file)[1].lower()  \n",
    "    # filter out whitespace\n",
    "    rdd = (\n",
    "        sc.textFile(file)\n",
    "          .map(lambda x: x.strip().rstrip(\",\").replace(\"{\", \"\").replace(\"}\", \"\"))\n",
    "          .filter(lambda x: x != \"\")\n",
    "    )\n",
    "\n",
    "    if ext == \".json\":\n",
    "        rdds.append((rdd, \"json\", file))\n",
    "    elif ext == \".csv\":\n",
    "        rdds.append((rdd, \"csv\", file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 For each RDD, remove the header rows and display the total count and the first 8 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/council.json: grouped=220\n",
      "data/nsw_property_price.csv: grouped=4854814\n",
      "data/property_purpose.json: grouped=865\n",
      "data/zoning.json: grouped=71\n",
      "Header for data/council.json = \"council_id,council_name\"\n",
      "Header for data/nsw_property_price.csv = \"property_id\",\"purchase_price\",\"address\",\"post_code\",\"property_type\",\"strata_lot_number\",\"property_name\",\"area\",\"area_type\",\"iso_contract_date\",\"iso_settlement_date\",\"nature_of_property\",\"legal_description\",\"id\",\"council_id\",\"purpose_id\",\"zone_id\"\n",
      "Header for data/property_purpose.json = \"purpose_id, primary_purpose\"\n",
      "Header for data/zoning.json = \"zoning_id, zoning\"\n",
      "[{'council_id': '1', 'council_name': '003'}, {'council_id': '3', 'council_name': '013'}]\n",
      "[{'property_id': '4270509', 'purchase_price': '1400000.00', 'address': '8 C NYARI RD, KENTHURST', 'post_code': '2156', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '2.044', 'area_type': 'H', 'iso_contract_date': '2023-12-14', 'iso_settlement_date': '2024-02-14', 'nature_of_property': 'V', 'legal_description': '2/1229857', 'id': '142', 'council_id': '200', 'purpose_id': '9922', 'zone_id': '53'}, {'property_id': '4329326', 'purchase_price': '1105000.00', 'address': '82 CAMARERO ST, BOX HILL', 'post_code': '2765', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '300.2', 'area_type': 'M', 'iso_contract_date': '2024-01-12', 'iso_settlement_date': '2024-02-09', 'nature_of_property': 'R', 'legal_description': '1119/1256791', 'id': '143', 'council_id': '200', 'purpose_id': '7071', 'zone_id': '41'}]\n",
      "[{'purpose_id': '1', 'primary_purpose': ''}, {'purpose_id': '29', 'primary_purpose': '10 FLATS'}]\n",
      "[{'zoning_id': '1', 'zoning': ''}, {'zoning_id': '3', 'zoning': 'AGB'}]\n"
     ]
    }
   ],
   "source": [
    "# used GPT to assemble the messy logic together cleanly\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def safe_dict(it):\n",
    "    \"\"\"Safely turn iterable of kv pairs into dict, skipping malformed entries.\"\"\"\n",
    "    d = {}\n",
    "    for kv in it:\n",
    "        if isinstance(kv, tuple) and len(kv) == 2:\n",
    "            k, v = kv\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "def parse_csv_line(line: str):\n",
    "    \"\"\"Safely parse a CSV line, handling commas inside quoted fields.\"\"\"\n",
    "    reader = csv.reader(StringIO(line), quotechar='\"', delimiter=',')\n",
    "    return next(reader)\n",
    "\n",
    "def process_rdd(rdd, ext, filename):\n",
    "    # remove header\n",
    "    header = rdd.first()\n",
    "    clean_header = header.split(\"\\\\n\", 1)[0]\n",
    "    if clean_header.startswith('\"') and not clean_header.endswith('\"'):\n",
    "        clean_header += '\"'  # restore the closing quote\n",
    "    headers[filename] = clean_header\n",
    "    lines = rdd.filter(lambda s: s != header)\n",
    "\n",
    "    \n",
    "    if ext == \"json\":        \n",
    "        # robust key:value parsing (handles both \"key : value\" and \"key\": \"value\")\n",
    "        kv = (\n",
    "            lines\n",
    "            .map(lambda s: s.strip())\n",
    "            .filter(lambda s: \":\" in s)              # accept any colon, with or without spaces\n",
    "            .map(lambda s: s.split(\":\", 1))          # split once, keep right side intact\n",
    "            .filter(lambda kv: len(kv) == 2)         # keep only well-formed pairs\n",
    "            .map(lambda kv: (kv[0].strip(' \"\\',{}'), kv[1].strip(' \"\\',{}')))\n",
    "        )\n",
    "\n",
    "        # group into records (assumes each record spans 2 lines)\n",
    "        grouped = (\n",
    "            kv.zipWithIndex()\n",
    "              .map(lambda x: (x[1] // 2, x[0]))\n",
    "              .groupByKey()\n",
    "              .mapValues(safe_dict)\n",
    "              .values()\n",
    "        )\n",
    "\n",
    "        print(f\"{filename}: grouped={grouped.count()}\")\n",
    "        return grouped\n",
    "    \n",
    "    if ext == \"csv\":\n",
    "        # use robust CSV parsing instead of naive split\n",
    "        fieldnames = parse_csv_line(headers[filename])\n",
    "        lines = (\n",
    "            lines.map(lambda row: dict(zip(fieldnames, parse_csv_line(row))))\n",
    "        )\n",
    "        \n",
    "#         fieldnames = [h.strip().strip('\"') for h in headers[filename].split(\",\")]\n",
    "#         lines = (\n",
    "#             lines.map(lambda row: dict(\n",
    "#                 zip(\n",
    "#                     fieldnames,\n",
    "#                     [val.strip().strip('\"') for val in row.split(\",\")]\n",
    "#                 )\n",
    "#             ))\n",
    "#         )\n",
    "        print(f\"{filename}: grouped={lines.count()}\")\n",
    "        return lines\n",
    "\n",
    "# Replace items in rdds\n",
    "rdds = [\n",
    "    (process_rdd(rdd, ext, filename), ext, filename)\n",
    "    for (rdd, ext, filename) in rdds\n",
    "]\n",
    "\n",
    "for fname, header in headers.items():\n",
    "    print(\"Header for\", fname, \"=\", header)\n",
    "    \n",
    "for (rdd, ext, filename) in rdds:\n",
    "    print(rdd.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.4 Drop records with invalid information: purpose_id or council_id is null, empty, or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/council.json: raw=220, filtered=220\n",
      "--- Example invalid rows from data/council.json ---\n",
      "data/nsw_property_price.csv: raw=4854814, filtered=4828278\n",
      "--- Example invalid rows from data/nsw_property_price.csv ---\n",
      "{'property_id': '', 'purchase_price': '195000.00', 'address': '14 FAIRFAX RD, BELLEVUE HILL', 'post_code': '2023', 'property_type': 'house', 'strata_lot_number': '9', 'property_name': '', 'area': '', 'area_type': '', 'iso_contract_date': '2022-11-11', 'iso_settlement_date': '2022-11-17', 'nature_of_property': '3', 'legal_description': '9/SP104887', 'id': '640243', 'council_id': '219', 'purpose_id': '3746', 'zone_id': '1'}\n",
      "{'property_id': '', 'purchase_price': '410000.00', 'address': ' FAIRFAX RD RD, BELLEVUE HILL', 'post_code': '2023', 'property_type': 'house', 'strata_lot_number': '10', 'property_name': 'LOT 10, 14', 'area': '', 'area_type': '', 'iso_contract_date': '2022-11-15', 'iso_settlement_date': '2022-11-28', 'nature_of_property': '3', 'legal_description': '10/SP104887', 'id': '640244', 'council_id': '219', 'purpose_id': '3746', 'zone_id': '1'}\n",
      "{'property_id': '', 'purchase_price': '800000.00', 'address': '5433/180 GEORGE ST, PARRAMATTA', 'post_code': '2150', 'property_type': 'unit', 'strata_lot_number': '', 'property_name': '', 'area': '', 'area_type': '', 'iso_contract_date': '2023-09-13', 'iso_settlement_date': '2023-10-10', 'nature_of_property': 'R', 'legal_description': '65/SP106623', 'id': '752392', 'council_id': '119', 'purpose_id': '7071', 'zone_id': '1'}\n",
      "{'property_id': '', 'purchase_price': '347000.00', 'address': '5/30 SALISBURY RD, ROSE BAY', 'post_code': '2029', 'property_type': 'unit', 'strata_lot_number': '', 'property_name': '', 'area': '', 'area_type': '', 'iso_contract_date': '2007-03-15', 'iso_settlement_date': '2007-04-27', 'nature_of_property': 'R', 'legal_description': '', 'id': '1125185', 'council_id': '219', 'purpose_id': '7071', 'zone_id': '1'}\n",
      "{'property_id': '', 'purchase_price': '390000.00', 'address': '312/ , ', 'post_code': '', 'property_type': 'unit', 'strata_lot_number': '312', 'property_name': '', 'area': '', 'area_type': '', 'iso_contract_date': '2001-10-05', 'iso_settlement_date': '2001-11-02', 'nature_of_property': 'R', 'legal_description': '312//SP 65785', 'id': '1417869', 'council_id': '70', 'purpose_id': '7071', 'zone_id': '1'}\n",
      "data/property_purpose.json: raw=865, filtered=865\n",
      "--- Example invalid rows from data/property_purpose.json ---\n",
      "data/zoning.json: raw=71, filtered=71\n",
      "--- Example invalid rows from data/zoning.json ---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def valid_record(rec):\n",
    "    for k, v in rec.items():\n",
    "        if k.endswith(\"_id\"):\n",
    "            if v is None:\n",
    "                return False\n",
    "            s = str(v).strip()\n",
    "\n",
    "            # Must be digits only\n",
    "            if not re.fullmatch(r\"[0-9]+\", s):\n",
    "                return False\n",
    "\n",
    "            try:\n",
    "                if int(s) < 1:\n",
    "                    return False\n",
    "            except ValueError:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def filter_rdd(rdd, ext, filename):\n",
    "    filtered = rdd.filter(valid_record)\n",
    "    # invalid rows (inverse filter)\n",
    "    invalid = rdd.filter(lambda rec: not valid_record(rec))\n",
    "    \n",
    "    print(f\"{filename}: raw={rdd.count()}, filtered={filtered.count()}\")\n",
    "    # Show invalid data\n",
    "    print(f\"--- Example invalid rows from {filename} ---\")\n",
    "    for row in invalid.take(5):\n",
    "        print(row)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# Apply filtering\n",
    "rdds = [\n",
    "    (filter_rdd(rdd, ext, filename), ext, filename)\n",
    "    for (rdd, ext, filename) in rdds\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD <a class=\"anchor\" name=\"1.2\"></a>\n",
    "1.2.1 For each RDD, using Spark’s default partitioning, print out the total number of partitions and the number of records in each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default partitions:  2\n",
      "Default partitions:  19\n",
      "Default partitions:  2\n",
      "Default partitions:  2\n"
     ]
    }
   ],
   "source": [
    "for rdd, ext, filename in rdds:\n",
    "    print('Default partitions: ',rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 Answer the following questions:   \n",
    "a) How many partitions do the above RDDs have?  \n",
    "b) How is the data in these RDDs partitioned by default, when we do not explicitly specify any partitioning strategy? Can you explain why it is partitioned in this number?   \n",
    "c) Assuming we are querying the dataset based on <strong> Property Price</strong>, can you think of a better strategy for partitioning the data based on your available hardware resources?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for a)  \n",
    "The csv file has 19 partitions, while all of the json files have 2 partitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer for b)  \n",
    "The data in these RDDs is partitioned according to their file size by default. It appears that they are partitioned to have up to 32 MB of data per partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer for c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 Create a user-defined function (UDF) to transform the date strings from ISO format (YYYY-MM-DD) (e.g. 2025-01-01) to Australian format (DD/Mon/YYYY) (e.g. 01/Jan/2025), then call the UDF to transform two date columns (iso_contract_date and iso_settlement_date) to contract_date and settlement_date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example transformed row: {'property_id': '4270509', 'purchase_price': '1400000.00', 'address': '8 C NYARI RD, KENTHURST', 'post_code': '2156', 'property_type': 'house', 'strata_lot_number': '', 'property_name': '', 'area': '2.044', 'area_type': 'H', 'nature_of_property': 'V', 'legal_description': '2/1229857', 'id': '142', 'council_id': '200', 'purpose_id': '9922', 'zone_id': '53', 'contract_date': '14/Dec/2023', 'settlement_date': '14/Feb/2024'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def iso_to_aus(iso_date: str) -> str:\n",
    "    try:\n",
    "        dt = datetime.strptime(iso_date, \"%Y-%m-%d\")\n",
    "        return dt.strftime(\"%d/%b/%Y\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def transform_property_price_rdd(rdd, filename):\n",
    "    \"\"\"Transform date fields in NSW property price RDD of dicts.\"\"\"\n",
    "    if filename != \"data/nsw_property_price.csv\":\n",
    "        return rdd  # skip other files\n",
    "\n",
    "    def transform_row(row):\n",
    "        new_row = dict(row)  # copy\n",
    "        if \"iso_contract_date\" in new_row:\n",
    "            new_row[\"contract_date\"] = iso_to_aus(new_row.pop(\"iso_contract_date\"))\n",
    "        if \"iso_settlement_date\" in new_row:\n",
    "            new_row[\"settlement_date\"] = iso_to_aus(new_row.pop(\"iso_settlement_date\"))\n",
    "        return new_row\n",
    "\n",
    "    transformed = rdd.map(transform_row)\n",
    "\n",
    "    # Debug output\n",
    "    print(\"Example transformed row:\", transformed.first())\n",
    "    return transformed\n",
    "\n",
    "rdds = [\n",
    "    (transform_property_price_rdd(rdd, filename), ext, filename)\n",
    "    if filename == \"data/nsw_property_price.csv\"\n",
    "    else (rdd, ext, filename)\n",
    "    for (rdd, ext, filename) in rdds\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis <a class=\"anchor\" name=\"1.3\"></a>\n",
    "For this part, write relevant RDD operations to answer the following queries.\n",
    "\n",
    "1.3.1 Extract the Month (Jan-Dec) information and print the total number of sales by contract date for each Month. (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dict_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Filter out header-like rows and bad values\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m clean_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mdict_rdd\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontract_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontract_date\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     22\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpurchase_price\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpurchase_price\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Extract (month, purchase_price)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m month_price_rdd \u001b[38;5;241m=\u001b[39m clean_rdd\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: (\n\u001b[1;32m     28\u001b[0m         extract_month(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontract_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     29\u001b[0m         safe_float(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpurchase_price\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     31\u001b[0m )\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# def extract_month(date_str: str) -> str:\n",
    "#     try:\n",
    "#         dt = datetime.strptime(date_str, \"%d/%b/%Y\")  # AUS format\n",
    "#         return dt.strftime(\"%b-%Y\")  # \"Jan-2025\"\n",
    "#     except Exception:\n",
    "#         return None\n",
    "\n",
    "# def parse_month(month_str: str) -> datetime:\n",
    "#     return datetime.strptime(month_str, \"%b-%Y\")\n",
    "\n",
    "# def safe_float(x: str) -> float:\n",
    "#     try:\n",
    "#         return float(x)\n",
    "#     except Exception:\n",
    "#         return 0.0\n",
    "\n",
    "# # Filter out header-like rows and bad values\n",
    "# clean_rdd = dict_rdd.filter(\n",
    "#     lambda row: row[\"contract_date\"] not in (None, \"\", \"contract_date\") \n",
    "#                 and row[\"purchase_price\"] not in (None, \"\", \"purchase_price\")\n",
    "# )\n",
    "\n",
    "# # Extract (month, purchase_price)\n",
    "# month_price_rdd = clean_rdd.map(\n",
    "#     lambda row: (\n",
    "#         extract_month(row[\"contract_date\"]),\n",
    "#         safe_float(row[\"purchase_price\"])\n",
    "#     )\n",
    "# ).filter(lambda x: x[0] is not None)\n",
    "\n",
    "# # Reduce by key (sum per month)\n",
    "# monthly_totals = month_price_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# # Collect and sort chronologically\n",
    "# monthly_totals_sorted = sorted(\n",
    "#     monthly_totals.collect(),\n",
    "#     key=lambda x: parse_month(x[0])\n",
    "# )\n",
    "\n",
    "# for month, total in monthly_totals_sorted:\n",
    "#     print(month, \"=\", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jan: Sales=231293, Total Purchase Price=149286257291.0\n",
      "Feb: Sales=385415, Total Purchase Price=283257505935.0\n",
      "Mar: Sales=460686, Total Purchase Price=362170672422.0\n",
      "Apr: Sales=382178, Total Purchase Price=288007815243.0\n",
      "May: Sales=449308, Total Purchase Price=374297279808.0\n",
      "Jun: Sales=407721, Total Purchase Price=369932834504.0\n",
      "Jul: Sales=404384, Total Purchase Price=356746327190.0\n",
      "Aug: Sales=413422, Total Purchase Price=355777547916.0\n",
      "Sep: Sales=423248, Total Purchase Price=346086151514.0\n",
      "Oct: Sales=432387, Total Purchase Price=346873898401.0\n",
      "Nov: Sales=446805, Total Purchase Price=414619066176.0\n",
      "Dec: Sales=390848, Total Purchase Price=432696382032.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def extract_month_name(date_str: str) -> str:\n",
    "    try:\n",
    "        dt = datetime.strptime(date_str, \"%d/%b/%Y\")  # AUS format\n",
    "        return dt.strftime(\"%b\")   # \"Jan\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_float(x: str) -> float:\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "    \n",
    "dict_rdd = next(rdd for (rdd, ext, fname) in rdds if fname == \"data/nsw_property_price.csv\")\n",
    "\n",
    "# Filter out header remnants / blanks\n",
    "clean_rdd = dict_rdd.filter(\n",
    "    lambda row: row[\"contract_date\"] not in (None, \"\", \"contract_date\")\n",
    ")\n",
    "\n",
    "# Map to (month, (count, total_purchase_price))\n",
    "month_metrics_rdd = clean_rdd.map(\n",
    "    lambda row: (\n",
    "        extract_month_name(row[\"contract_date\"]),\n",
    "        (1, safe_float(row[\"purchase_price\"]))\n",
    "    )\n",
    ").filter(lambda x: x[0] is not None)\n",
    "\n",
    "# Reduce: sum counts and purchase prices\n",
    "monthly_metrics = month_metrics_rdd.reduceByKey(\n",
    "    lambda a, b: (a[0] + b[0], a[1] + b[1])\n",
    ")\n",
    "\n",
    "# Collect and sort by calendar order\n",
    "month_order = [\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]\n",
    "monthly_metrics_sorted = sorted(\n",
    "    monthly_metrics.collect(),\n",
    "    key=lambda x: month_order.index(x[0])\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for month, (count, total) in monthly_metrics_sorted:\n",
    "    print(f\"{month}: Sales={count}, Total Purchase Price={total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 Which 5 councils have the largest number of houses? Show their name and the total number of houses. (Note: Each house may appear multiple times if there are more than one sales, you should only count them once.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLACKTOWN (council_id=100): 91213 houses\n",
      "LAKE MACQUARIE (council_id=157): 59117 houses\n",
      "THE HILLS SHIRE (council_id=200): 55032 houses\n",
      "LIVERPOOL (council_id=162): 49053 houses\n",
      "PENRITH (council_id=183): 46840 houses\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract (council_id, property_id) pairs\n",
    "council_property_rdd = dict_rdd.map(\n",
    "    lambda row: (row[\"council_id\"], row[\"property_id\"])\n",
    ")\n",
    "\n",
    "# Step 2: Deduplicate by council_id + property_id\n",
    "unique_council_property_rdd = council_property_rdd.distinct()\n",
    "\n",
    "# Step 3: Count unique properties per council\n",
    "council_house_counts = unique_council_property_rdd \\\n",
    "    .map(lambda x: (x[0], 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Step 4: Load council_id → council_name mapping from JSON RDD\n",
    "# Assume json_rdd is already parsed into dicts like: {\"council_id\": \"123\", \"council_name\": \"City Council\"}\n",
    "json_rdd = next(rdd for (rdd, ext, fname) in rdds if fname == \"data/council.json\")\n",
    "council_name_map = json_rdd.map(\n",
    "    lambda row: (row[\"council_id\"], row[\"council_name\"])\n",
    ")\n",
    "\n",
    "# Step 5: Join counts with names\n",
    "council_with_names = council_house_counts.join(council_name_map)\n",
    "# => (council_id, (house_count, council_name))\n",
    "\n",
    "# Step 6: Get top 5 councils by number of houses\n",
    "top5_councils = council_with_names.takeOrdered(\n",
    "    5,\n",
    "    key=lambda x: -x[1][0]   # sort by house_count descending\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for council_id, (count, name) in top5_councils:\n",
    "    print(f\"{name} (council_id={council_id}): {count} houses\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Working with DataFrames (45%) <a class=\"anchor\" name=\"2-dataframes\"></a>\n",
    "In this section, you need to load the given datasets into PySpark DataFrames and use DataFrame functions to answer the queries.\n",
    "### 2.1 Data Preparation and Loading\n",
    "\n",
    "2.1.1. Load the CSV/JSON files into separate dataframes. When you create your dataframes, please refer to the metadata file and think about the appropriate data type for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql.functions import explode\n",
    "files = [\"data/council.json\", \"data/nsw_property_price.csv\", \"data/property_purpose.json\", \"data/zoning.json\"]\n",
    "dfs = []  \n",
    "for file in files:\n",
    "    ext = os.path.splitext(file)[1].lower()  # get file extension\n",
    "    \n",
    "    if ext == \".json\":\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file)\n",
    "        # If schema shows a single array column, explode it\n",
    "        df_flat = df.select(explode(df[df.columns[0]]).alias(\"data\"))\n",
    "\n",
    "        # Now pull fields out of the struct\n",
    "        df_flat = df_flat.select(\"data.*\")\n",
    "\n",
    "\n",
    "        dfs.append((df_flat, \"json\", file))\n",
    "    elif ext == \".csv\":\n",
    "        # rdd = spark.read.csv(file, header=True, inferSchema=True)\n",
    "        df = spark.read.csv(file, header=True, inferSchema=True)\n",
    "        dfs.append((df, \"csv\", file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Display the schema of the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[council_id: bigint, council_name: string]\n",
      "root\n",
      " |-- council_id: long (nullable = true)\n",
      " |-- council_name: string (nullable = true)\n",
      "\n",
      "DataFrame[property_id: int, purchase_price: double, address: string, post_code: string, property_type: string, strata_lot_number: string, property_name: string, area: string, area_type: string, iso_contract_date: string, iso_settlement_date: string, nature_of_property: string, legal_description: string, id: string, council_id: string, purpose_id: string, zone_id: int]\n",
      "root\n",
      " |-- property_id: integer (nullable = true)\n",
      " |-- purchase_price: double (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- post_code: string (nullable = true)\n",
      " |-- property_type: string (nullable = true)\n",
      " |-- strata_lot_number: string (nullable = true)\n",
      " |-- property_name: string (nullable = true)\n",
      " |-- area: string (nullable = true)\n",
      " |-- area_type: string (nullable = true)\n",
      " |-- iso_contract_date: string (nullable = true)\n",
      " |-- iso_settlement_date: string (nullable = true)\n",
      " |-- nature_of_property: string (nullable = true)\n",
      " |-- legal_description: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- council_id: string (nullable = true)\n",
      " |-- purpose_id: string (nullable = true)\n",
      " |-- zone_id: integer (nullable = true)\n",
      "\n",
      "DataFrame[primary_purpose: string, purpose_id: bigint]\n",
      "root\n",
      " |-- primary_purpose: string (nullable = true)\n",
      " |-- purpose_id: long (nullable = true)\n",
      "\n",
      "DataFrame[zoning: string, zoning_id: bigint]\n",
      "root\n",
      " |-- zoning: string (nullable = true)\n",
      " |-- zoning_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df, ext, filename in dfs:\n",
    "    print(df)\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the dataset is large, do you need all columns? How to optimize memory usage? Do you need a customized data partitioning strategy? (Note: Think about those questions but you don’t need to answer these questions.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 QueryAnalysis  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "Implement the following queries using dataframes. You need to be able to perform operations like transforming, filtering, sorting, joining and group by using the functions provided by the DataFrame API. For each task, display the first 5 results where no output is specified.\n",
    "\n",
    "2.2.1. The area column has two types: (H, A and M): 1 H is one hectare = 10000 sqm, 1A is one acre = 4000 sqm, 1 M is one sqm. Unify the unit to sqm and create a new column called area_sqm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, length, regexp_replace\n",
    "\n",
    "def filter_df(df, ext, filename):\n",
    "    id_cols = [c for c in df.columns if c.endswith(\"_id\")]\n",
    "\n",
    "    condition = None\n",
    "    for id_col in id_cols:\n",
    "        # Force to string and trim\n",
    "        id_str = trim(col(id_col).cast(\"string\"))\n",
    "\n",
    "        # Must be only digits (no \"/\" or other chars)\n",
    "        # length > 0 to reject empty\n",
    "        this_cond = (\n",
    "            id_str.isNotNull() &\n",
    "            (length(id_str) > 0) &\n",
    "            id_str.rlike(\"^[0-9]+$\") &\n",
    "            (id_str.cast(\"bigint\") >= 1)\n",
    "        )\n",
    "\n",
    "        # Combine conditions: ALL *_id columns must satisfy\n",
    "        condition = this_cond if condition is None else (condition & this_cond)\n",
    "\n",
    "    filtered_df = df.filter(condition) if condition is not None else df\n",
    "\n",
    "    print(f\"{filename}: raw={df.count()}, filtered={filtered_df.count()}\")\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Apply filtering to all DataFrames\n",
    "dfs = [\n",
    "    (filter_df(df, ext, filename), ext, filename)\n",
    "    for (df, ext, filename) in dfs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "def normalize_area(df, ext, filename):\n",
    "    \"\"\"\n",
    "    Convert area + area_type to a unified area_sqm column in sqm.\n",
    "    \"\"\"\n",
    "    if ext != \"csv\":\n",
    "        return df\n",
    "    df.show(3)\n",
    "    if \"area\" in df.columns and \"area_type\" in df.columns:\n",
    "        df = df.withColumn(\n",
    "            \"area_sqm\",\n",
    "            when(col(\"area_type\") == \"H\", col(\"area\") * 10000)\n",
    "            .when(col(\"area_type\") == \"A\", col(\"area\") * 4000)\n",
    "            .when(col(\"area_type\") == \"M\", col(\"area\"))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "        print(f\"{filename}: added area_sqm column\")\n",
    "    else:\n",
    "        print(f\"{filename}: no area/area_type columns found\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply to all dataframes\n",
    "dfs = [\n",
    "    (normalize_area(df, ext, filename), ext, filename)\n",
    "    for (df, ext, filename) in dfs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2. <pre>The top five property types are: Residence, Vacant Land, Commercial, Farm and Industrial.\n",
    "However, for historical reason, they may have different strings in the database. Please update the primary_purpose with the following rules:\n",
    "a)\tAny purpose that has “HOME”, “HOUSE”, “UNIT” is classified as “Residence”;\n",
    "b)\t“Warehouse”, “Factory”,  “INDUST” should be changed to “Industrial”;\n",
    "c)\tAnything that contains “FARM”(i.e. FARMING), should be changed to “FARM”;\n",
    "d)\t“Vacant”, “Land” should be “Vacant Land”;\n",
    "e)\tAnything that has “COMM”, “Retail”, “Shop” or “Office” are “Cmmercial”.\n",
    "f)\tAll remaining properties, including null and empty purposes, are classified as “Others”.\n",
    "Show the count of each type in a table.\n",
    "(note: Some properties are multi-purpose, e.g. “House & Farm”, it’s fine to count them multiple times.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the dataframe for primary_purpose to consolidate their IDs into the 6 property type IDs.\n",
    "# for each word, it should strip whitespace (spaces)\n",
    "# if it should have nothing else, then having any characters disqualifies it, unless it has &,and/,- in it, then anything goes\n",
    "# by defualt, it can have other stuff\n",
    "# Residence -> 7071. HOME, HOUSE, UNIT, has nothing else\n",
    "# Industrial -> 4778. WAREHOUSE, FACTORY, INDUST\n",
    "# Farm -> 2941. FARM\n",
    "# Vacant Land -> 9922. VACANT, LAND\n",
    "# Commercial -> 1704. COMM, RETAIL, SHOP, OFFICE\n",
    "# Others -> 12000. Anything that doesn't match any of the above.\n",
    "# Permutations of the above -> 12001 above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.3 Find the top 20 properties that make the largest value gain, show their address, suburb, and value increased. To calculate the value gain, the property must have been sold multiple times, “value increase” can be calculated with the last sold price – first sold price, regardless the transactions in between. Print all 20 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address has the key \"address\"\n",
    "# Suburb is second half of the key \"address\" as denoted by after comma. create a new column in the dataframe for this.\n",
    "# Value determined with the key \"purchase_price\", and it must be the same \"property_id\" showing up multiple times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.4 For each season, plot the median house price trend over the years. Seasons in Australia are defined as: (Spring: Sep-Nov, Summer: Dec-Feb, Autumn: Mar-May, Winter: Jun-Aug). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.5 (Open Question) Explore the dataset freely and plot one diagram of your choice. Which columns (at least 2) are highly correlated to the sales price? Discuss the steps of your exploration and the results. (No word limit, please keep concise.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to explore the dataset and identify which columns are highly correlated to sales price with a multivariate analysis. recall that the columns available are: \n",
    "# \"property_id\",\"purchase_price\",\"address\",\"post_code\",\"property_type\",\"strata_lot_number\",\"property_name\",\"area\",\"area_type\",\"iso_contract_date\",\"iso_settlement_date\",\"nature_of_property\",\"legal_description\",\"id\",\"council_id\",\"purpose_id\",\"zone_id\"\n",
    "# intuitively, i expect that the relevant columns for identifying purchase_price are address, post_code, property_type, strata_lot_number, area_sqm (as created earlier), iso_contract_date, nature_of_property, legal_description, council_id, purpose_id, zone_id\n",
    "\n",
    "\n",
    "# I now want to plot these variables against purchase_price.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your dicsussion here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 RDDs vs DataFrame vs Spark SQL (25%) <a class=\"anchor\" name=\"part-3\"></a>\n",
    "Implement the following complex queries using RDD, DataFrame in SparkSQL separately(choose two). Log the time taken for each query in each approach using the “%%time” built-in magic command in Jupyter Notebook and discuss the performance difference between these 2 approaches of your choice.\n",
    "(notes: You can write a multi-step query or a single complex query, the choice is yours. You can reuse the data frame in Part 2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex Query:\n",
    "<pre>\n",
    "A property investor wants to understand whether the property price and the settlement date are correlated. Here is the conditions:\n",
    "1)\tThe investor is only interested in the last 2 years of the dataset.\n",
    "2)\tThe investor is looking at houses under $2 million.\n",
    "3)\tPerform a bucketing of the settlement date (settlement – contract date\n",
    "range (15, 30, 45, 60, 90 days).\n",
    "4)\tPerform a bucketing of property prices in $500K(e.g. 0-$500K, $500K-$1M, $1M-$1.5M, $1.5-$2M)\n",
    "5)\tCount the number of transactions in each combination and print the result in the following format\n",
    "(Note: It’s fine to count the same property multiple times in this task, it’s based on sales transactions).\n",
    "(Note: You shall show the full table with 40 rows, 2 years *4 price bucket * 5 settlement bucket; 0 count should be displayed as 0, not omitted.)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\tImplement the above query using two approaches of your choice separately and print the results. (Note: Outputs from both approaches of your choice are required, and the results should be the same.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Implementation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tWhich one is easier to implement, in your opinion? Log the time taken for each query, and observe the query execution time, among DataFrame and SparkSQL, which is faster and why? Please include proper references. (Maximum 500 words.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ideas on the comparison\n",
    "\n",
    "Armbrust, M., Huai, Y., Liang, C., Xin, R., & Zaharia, M. (2015). Deep Dive into Spark SQL’s Catalyst Optimizer. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html\n",
    "\n",
    "Damji, J. (2016). A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets. Retrieved September 28, 2017, from https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html\n",
    "\n",
    "Data Flair (2017a). Apache Spark RDD vs DataFrame vs DataSet. Retrieved September 28, 2017, from http://data-flair.training/blogs/apache-spark-rdd-vs-dataframe-vs-dataset\n",
    "\n",
    "Prakash, C. (2016). Apache Spark: RDD vs Dataframe vs Dataset. Retrieved September 28, 2017, from http://why-not-learn-something.blogspot.com.au/2016/07/apache-spark-rdd-vs-dataframe-vs-dataset.html\n",
    "\n",
    "Xin, R., & Rosen, J. (2015). Project Tungsten: Bringing Apache Spark Closer to Bare Metal. Retrieved September 30, 2017, from https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
