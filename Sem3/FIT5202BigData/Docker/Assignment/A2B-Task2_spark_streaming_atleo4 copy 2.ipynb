{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming  \n",
    "In this task, you will implement Spark Structured Streaming to consume the data from task 1 and perform a prediction.    \n",
    "Important:   \n",
    "-\tThis task uses PySpark Structured Streaming with PySpark Dataframe APIs and PySpark ML.  \n",
    "-\tYou also need your pipeline model from A2A to make predictions and persist the results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[4]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2B\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name) \\\n",
    "                        .set(\"spark.sql.streaming.checkpointLocation\", \"checkpoints\")\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWrite code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. building information) into data frames. (You can reuse your code from 2A.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from GPT\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(15, 4), False),\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(6, 4), True)\n",
    "])\n",
    "\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"cloud_coverage\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"dew_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 3), True),\n",
    "    StructField(\"wind_direction\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"wind_speed\", DecimalType(5, 3), True),\n",
    "    StructField(\"weather_ts\", TimestampType(), False) # new field\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/new_building_information.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tUsing the Kafka topic from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'weather_ts' column, you shall receive it as an Int type. Load the new building information CSV file into a dataframe. Then, the data frames should be transformed into the proper formats following the metadata file schema, similar to assignment 2A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#configuration\n",
    "hostip = \"192.168.0.6\"\n",
    "topic = 'weather_data'\n",
    "\n",
    "df_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f'{hostip}:9092') \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "df_str = df_raw.selectExpr(\"CAST(value AS STRING) as json_str\")\n",
    "\n",
    "weather_stream = (\n",
    "    df_str\n",
    "    .withColumn(\"data\", F.from_json(F.col(\"json_str\"), F.ArrayType(weather_schema)))\n",
    "    .select(F.explode(F.col(\"data\")).alias(\"r\"))\n",
    "    .select(\"r.*\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tUse a watermark on weather_ts, if data points are received 5 seconds late, discard the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_stream = weather_stream.withWatermark(\"weather_ts\", '5 seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tPerform the necessary transformation you used in A2A. (note: every student may have used different features, feel free to reuse the code you have written in A2A. If you built an end-to-end pipeline, you can ignore this task.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from A2A which was from GPT\n",
    "# Get global_means, site_month_means, site_means\n",
    "# weather_df is history, weather_stream is current\n",
    "\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_df = weather_df.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Choose which columns to impute\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Compute global_means, site_month_means, site_means\n",
    "global_means = weather_df.select(\n",
    "    *[F.mean(c).alias(c) for c in impute_cols]\n",
    ").first().asDict()\n",
    "\n",
    "site_month_means = weather_df.groupBy(\"site_id\", \"month\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_month_mean\") for c in impute_cols]\n",
    ")\n",
    "\n",
    "site_means = weather_df.groupBy(\"site_id\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_mean\") for c in impute_cols]\n",
    ")\n",
    "    \n",
    "# Data imputation\n",
    "# Transform weather_stream\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_stream = weather_stream.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "\n",
    "# Step 1: site_id + month\n",
    "weather_stream = weather_stream.join(site_month_means, on=[\"site_id\", \"month\"], how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_month_mean\"))\n",
    "    ).drop(f\"{c}_site_month_mean\")\n",
    "    \n",
    "# Step 2: site_id\n",
    "weather_stream = weather_stream.join(site_means, on=\"site_id\", how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_mean\"))\n",
    "    ).drop(f\"{c}_site_mean\")\n",
    "\n",
    "# Step 3: global fallback\n",
    "for c in impute_cols:\n",
    "    weather_stream = weather_stream.withColumn(\n",
    "        c, F.coalesce(c, F.lit(global_means[c]))\n",
    "    )\n",
    "\n",
    "# Add custom columns\n",
    "weather_stream = (\n",
    "    weather_stream\n",
    "    .withColumn(\"dew_depression\", F.col(\"air_temperature\") - F.col(\"dew_temperature\"))\n",
    "    .withColumn(\"nonideal_temp\", (F.col(\"air_temperature\") - 18)**2)\n",
    "    .drop(\"air_temperature\")\n",
    "    .drop(\"dew_temperature\")\n",
    ")\n",
    "\n",
    "# No need to add median temp and peak-offpeak as our pipeline model later does not use them\n",
    "feature_df = buildings_df.join(weather_stream, [\"site_id\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tLoad your pipeline model and perform the following aggregations:  \n",
    "a)\tPrint the prediction from your model as a stream comes in.  \n",
    "b)\tEvery 7 seconds, print the total energy consumption for each 6-hour interval, aggregated by building, and print 20 records. (Note: This is simulating energy data each day in a week)  \n",
    "c)\tEvery 14 seconds, for each site, print the daily total energy consumption.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. Spark + model setup ---\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "import time\n",
    "\n",
    "model = PipelineModel.load(\"models/best_model_rmsle\")\n",
    "\n",
    "# --- 2. Apply model ---\n",
    "predictions = model.transform(feature_df).withColumnRenamed(\"prediction\", \"log_power_usage\")\n",
    "\n",
    "checkpoint_dir = os.path.abspath(\"checkpoints/weather_stream\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a, 7a\n",
    "def write_and_debug_live(batch_df, batch_id):\n",
    "    # 1️⃣ Write batch incrementally to Parquet\n",
    "    (\n",
    "        batch_df\n",
    "        .select(\"site_id\", \"building_id\", \"log_power_usage\")\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .parquet(\"data/live_predictions\")\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Print a few rows for debugging\n",
    "    print(f\"\\n=== Microbatch {batch_id} ===\")\n",
    "    batch_df.select(\"site_id\", \"building_id\", \"log_power_usage\").show(5, truncate=False)\n",
    "\n",
    "query_live_combined = (\n",
    "    predictions\n",
    "        .writeStream\n",
    "        .foreachBatch(write_and_debug_live)\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/live_predictions\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Task 6b, 7b requires aggregation by 6-hour interval)\n",
    "building_6h = (\n",
    "    predictions\n",
    "        .groupBy(\n",
    "            \"building_id\",\n",
    "            \"time\",  # The 6-hour interval column you created in Task 5\n",
    "            F.window(\"weather_ts\", \"5 seconds\") # Group by 6-hour windows of event-time\n",
    "        )\n",
    "        .agg(F.sum(\"log_power_usage\").alias(\"total_power_6h\"))\n",
    ")\n",
    "\n",
    "def write_and_debug_building(batch_df, batch_id):\n",
    "    # 1️⃣ Write batch incrementally to Parquet\n",
    "    (\n",
    "        batch_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .parquet(\"data/building_6h\")\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Print a few rows for debugging\n",
    "    print(f\"\\n=== Microbatch {batch_id} ===\")\n",
    "    batch_df.show(5, truncate=False)\n",
    "\n",
    "query_building_6h = (\n",
    "    building_6h\n",
    "        .writeStream\n",
    "        .foreachBatch(write_and_debug_building)\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/building_6h\")\n",
    "        .trigger(processingTime=\"7 seconds\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Task 6c, 7c requires daily aggregation by site)\n",
    "site_daily = (\n",
    "    predictions\n",
    "        .groupBy(\n",
    "            \"site_id\", \n",
    "            \"date\", # The date column you created in Task 5\n",
    "            F.window(\"weather_ts\", \"5 seconds\") # Group by daily windows of event-time\n",
    "        )\n",
    "        .agg(F.sum(\"log_power_usage\").alias(\"total_power_day\"))\n",
    ")\n",
    "\n",
    "\n",
    "def write_and_debug_site(batch_df, batch_id):\n",
    "    # 1️⃣ Write batch incrementally to Parquet\n",
    "    (\n",
    "        batch_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .parquet(\"data/site_daily\")\n",
    "    )\n",
    "\n",
    "    # 2️⃣ Print a few rows for debugging\n",
    "    print(f\"\\n=== Microbatch {batch_id} ===\")\n",
    "    batch_df.show(5, truncate=False)\n",
    "\n",
    "query_site_daily = (\n",
    "    site_daily\n",
    "        .writeStream\n",
    "        .foreachBatch(write_and_debug_site)\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/site_daily\")\n",
    "        .trigger(processingTime=\"14 seconds\")\n",
    "        .start()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 6a\n",
    "# # Show live predictions\n",
    "# query_live = (\n",
    "#     predictions\n",
    "#         .select(\"site_id\", \"building_id\", \"log_power_usage\")\n",
    "#         .writeStream\n",
    "#         .outputMode(\"append\")\n",
    "#         .format(\"memory\")\n",
    "#         .queryName(\"live_predictions\")\n",
    "#         .start()\n",
    "# )\n",
    "\n",
    "# print(\"Waiting for first batch...\")\n",
    "# # Wait for the first microbatch to finish\n",
    "# while query_live.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)\n",
    "# spark.sql(\"select * from live_predictions\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 6b\n",
    "# # (Task 6b requires aggregation by 6-hour interval)\n",
    "# building_6h = (\n",
    "#     predictions\n",
    "#         .groupBy(\n",
    "#             \"building_id\",\n",
    "#             \"time\",  # The 6-hour interval column you created in Task 5\n",
    "#             F.window(\"weather_ts\", \"5 seconds\") # Group by 6-hour windows of event-time\n",
    "#         )\n",
    "#         .agg(F.sum(\"log_power_usage\").alias(\"total_power_6h\"))\n",
    "# )\n",
    "\n",
    "# # --- Print to console every 7 seconds ---\n",
    "# query_building_6h = (\n",
    "#     building_6h\n",
    "#         .writeStream\n",
    "#         .outputMode(\"update\") # 'update' mode is correct for windowed aggregations\n",
    "#         .format(\"memory\")\n",
    "#         .queryName(\"building_6h\")\n",
    "#         .trigger(processingTime=\"7 seconds\")\n",
    "#         .start()\n",
    "# )\n",
    "\n",
    "# print(\"Waiting for first batch...\")\n",
    "# # Wait for the first microbatch to finish\n",
    "# while query_building_6h.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)    \n",
    "# spark.sql(\"select * from building_6h\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 6c\n",
    "# # (Task 6c requires daily aggregation by site)\n",
    "# site_daily = (\n",
    "#     predictions\n",
    "#         .groupBy(\n",
    "#             \"site_id\", \n",
    "#             \"date\", # The date column you created in Task 5\n",
    "#             F.window(\"weather_ts\", \"5 seconds\") # Group by daily windows of event-time\n",
    "#         )\n",
    "#         .agg(F.sum(\"log_power_usage\").alias(\"total_power_day\"))\n",
    "# )\n",
    "\n",
    "# # --- Print to console every 14 seconds ---\n",
    "# query_site_daily = (\n",
    "#     site_daily\n",
    "#         .writeStream\n",
    "#         .outputMode(\"update\") # 'update' mode is correct\n",
    "#         .format(\"memory\")\n",
    "#         .queryName(\"site_daily\")\n",
    "#         .trigger(processingTime=\"14 seconds\")\n",
    "#         .start()\n",
    "# )\n",
    "# print(\"Waiting for first batch...\")\n",
    "# # Wait for the first microbatch to finish\n",
    "# while query_site_daily.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)\n",
    "# spark.sql(\"select * from site_daily\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tSave the data from 6 to Parquet files as streams. (Hint: Parquet files support streaming writing/reading. The file keeps updating while new batches arrive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7a(save 6a)\n",
    "\n",
    "# # Save predictions to Parquet incrementally\n",
    "# query_live_parquet = (\n",
    "#     predictions\n",
    "#         .select(\"site_id\", \"building_id\", \"time\", \"log_power_usage\")\n",
    "#         .writeStream\n",
    "#         .outputMode(\"append\")\n",
    "#         .format(\"parquet\")\n",
    "#         .option(\"path\", \"data/live_predictions\")\n",
    "#         .option(\"checkpointLocation\", checkpoint_dir + \"/live_predictions\")\n",
    "#         .start()\n",
    "# )\n",
    "# print(\"Waiting for Parquet microbatch...\")\n",
    "# while query_live_parquet.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7b(save 6b)\n",
    "\n",
    "# query_building_6h_parquet = (\n",
    "#     building_6h\n",
    "#         .writeStream\n",
    "#         .outputMode(\"append\")\n",
    "#         .format(\"parquet\")\n",
    "#         .option(\"path\", \"data/building_6h\")\n",
    "#         .option(\"checkpointLocation\", checkpoint_dir + \"/building_6h\")\n",
    "#         .start()\n",
    "# )\n",
    "# print(\"Waiting for Parquet microbatch...\")\n",
    "# while query_building_6h_parquet.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 7c(save 6c)\n",
    "\n",
    "\n",
    "# query_site_daily_parquet = (\n",
    "#     site_daily\n",
    "#         .writeStream\n",
    "#         .outputMode(\"append\")\n",
    "#         .format(\"parquet\")\n",
    "#         .option(\"path\", \"data/site_daily\")\n",
    "#         .option(\"checkpointLocation\", checkpoint_dir + \"/site_daily\")\n",
    "#         .trigger(processingTime=\"14 seconds\")\n",
    "#         .start()\n",
    "# )\n",
    "# print(\"Waiting for Parquet microbatch...\")\n",
    "# while query_site_daily_parquet.lastProgress is None:\n",
    "#     time.sleep(1)\n",
    "# time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_first_batches(queries, label=\"\"):\n",
    "    print(f\"⏳ Waiting for first microbatches {label}...\")\n",
    "    for q in queries:\n",
    "        print(f\"  ↳ Waiting for {q.name or 'unnamed query'}...\")\n",
    "        while q.lastProgress is None:\n",
    "            time.sleep(1)\n",
    "        print(f\"  ✅ {q.name or 'query'} has processed its first batch.\")\n",
    "    # small grace period for filesystem to flush parquet files\n",
    "    time.sleep(3)\n",
    "    print(f\"✅ All prerequisite batches for {label} completed.\\n\")\n",
    "    \n",
    "wait_for_first_batches(\n",
    "    [query_live_combined, query_building_6h, query_site_daily],\n",
    "    label=\"pre-Kafka publication\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tRead the parquet files from task 7 as data streams and send them to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "kafka_ip = hostip + \":9092\"\n",
    "# Stream 1\n",
    "live_predictions = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(predictions.schema)\n",
    "         .load(\"data/live_predictions\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_live_predictions = (\n",
    "    live_predictions\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                    \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"live_predictions\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/live_predictions\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n",
    "building_6h = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(predictions.schema)\n",
    "         .load(\"data/building_6h\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_building_6h = (\n",
    "    building_6h\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                        \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"building_6h\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/building_6h\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 3\n",
    "site_daily = (\n",
    "    spark.readStream\n",
    "         .format(\"parquet\")\n",
    "         .schema(predictions.schema)\n",
    "         .load(\"data/site_daily\")\n",
    ")\n",
    "\n",
    "# send predictions to Kafka\n",
    "kafka_site_daily = (\n",
    "    site_daily\n",
    "        .selectExpr(\"\\\"predictions\\\" AS key\", \n",
    "                        \"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", kafka_ip)\n",
    "        .option(\"topic\", \"site_daily\")\n",
    "        .option(\"checkpointLocation\", checkpoint_dir + \"/kafka/site_daily\")\n",
    "        .outputMode(\"append\")\n",
    "        .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kafka_site_daily.lastProgress)\n",
    "print(kafka_site_daily.status)\n",
    "# this shows it's inactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "\n",
    "for i in range(10):\n",
    "    time.sleep(3)\n",
    "    print(json.dumps(kafka_live_predictions.lastProgress, indent=2))\n",
    "# this show they are nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"data/live_predictions\").show(5)\n",
    "spark.read.parquet(\"data/building_6h\").show(5)\n",
    "spark.read.parquet(\"data/site_daily\").show(5)\n",
    "# this shows they exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
