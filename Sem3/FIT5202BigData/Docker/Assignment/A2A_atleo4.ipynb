{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assignment 2A : Building Models for Building Energy Prediction\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Hyperparameter Tuning and Model Optimisation](#part-3)  \n",
    "Please add code/markdown cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading\n",
    "In this section, you must load the given datasets into PySpark DataFrames and use DataFrame functions to process the data. For plotting, various visualisation packages can be used, but please ensure that you have included instructions to install the additional packages and that the installation will be successful in the provided Docker container (in case your marker needs to clear the notebook and rerun it).\n",
    "\n",
    "### 1.1.1 Data Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.1.1 Write the code to create a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to ensure the maximum partition size does not exceed 32MB, and to run locally with all CPU cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2A\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "# spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.files.maxPartitionBytes\", \"33554432\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Write code to define the schemas for the datasets, following the data types suggested in the metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from GPT\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(15, 4), False),\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(6, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(6, 4), True)\n",
    "])\n",
    "\n",
    "# 3. Weather Table\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"cloud_coverage\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"dew_temperature\", DecimalType(5, 3), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 3), True),\n",
    "    StructField(\"wind_direction\", DecimalType(5, 3), True), # Is an Integer, but ends with a \".0\", so read as a DecimalType\n",
    "    StructField(\"wind_speed\", DecimalType(5, 3), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 Using your schemas, load the CSV files into separate data frames. Print the schemas of all data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meters DF:\n",
      "root\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      " |-- value: decimal(15,4) (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      "\n",
      "Buildings DF:\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: decimal(6,4) (nullable = true)\n",
      " |-- latent_s: decimal(6,4) (nullable = true)\n",
      " |-- latent_r: decimal(6,4) (nullable = true)\n",
      "\n",
      "Weather DF\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- air_temperature: decimal(5,3) (nullable = true)\n",
      " |-- cloud_coverage: decimal(5,3) (nullable = true)\n",
      " |-- dew_temperature: decimal(5,3) (nullable = true)\n",
      " |-- sea_level_pressure: decimal(8,3) (nullable = true)\n",
      " |-- wind_direction: decimal(5,3) (nullable = true)\n",
      " |-- wind_speed: decimal(5,3) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from GPT\n",
    "meters_df = spark.read.csv(\n",
    "    \"data/meters.csv\",\n",
    "    header=True,\n",
    "    schema=meters_schema\n",
    ")\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/building_information.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n",
    "print(\"Meters DF:\")\n",
    "meters_df.printSchema()\n",
    "print(\"Buildings DF:\")\n",
    "buildings_df.printSchema()\n",
    "print(\"Weather DF\")\n",
    "weather_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation to Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In this section, we primarily have three tasks:  \n",
    "1.2.1 The dataset includes sensors with hourly energy measurements. However, as a grid operator, we don’t need this level of granularity and lowering it can reduce the amount of data we need to process. For each building, we will aggregate the metered energy consumption in 6-hour intervals (0:00-5:59, 6:00-11:59, 12:00-17:59, 18:00-23:59). This will be our target (label) column for this prediction. Perform the aggregation for each building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+------+-----------+\n",
      "|building_id|meter_type|      date|  time|power_usage|\n",
      "+-----------+----------+----------+------+-----------+\n",
      "|        244|         c|2022-01-01| 6-12h|    36.3642|\n",
      "|       1214|         c|2022-01-01| 6-12h|   444.3222|\n",
      "|       1259|         c|2022-01-01|12-18h|   385.0202|\n",
      "+-----------+----------+----------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adapted from A1 and GPT\n",
    "# Meters df\n",
    "# Split timestamp to date and time bucket\n",
    "meters_df = meters_df.withColumn(\"date\", F.to_date(\"ts\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"ts\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"ts\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"ts\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"ts\") <= 23, \"18-24h\")\n",
    "     .otherwise(\"unknown\")   # catch any unexpected cases\n",
    ")\n",
    "\n",
    "# Aggregate by time bucket\n",
    "meters_df = (\n",
    "    meters_df.groupBy(\"building_id\", \"meter_type\", \"date\", \"time\")\n",
    "    .agg(F.sum(\"value\").cast(DecimalType(15, 4)).alias(\"power_usage\"))\n",
    ")\n",
    "meters_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the weather dataset, there are some missing values (null or empty strings). It may lower the quality of our model. Imputation is a way to deal with those missing values. Imputation is the process of replacing missing values in a dataset with substituted, or \"imputed,\" values. It's a way to handle gaps in your data so that you can still analyse it effectively without having to delete incomplete records.  \n",
    "1.2.2 Refer to the Spark MLLib imputation API and fill in the missing values in the weather dataset. You can use mean values as the strategy.  https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.ml.feature.Imputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|site_id|      date|  time|month|air_temperature|cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|\n",
      "+-------+----------+------+-----+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "|      1|2022-01-01|  0-6h|    1|          2.800|         0.000|          1.950|          1022.067|        49.722|     1.617|\n",
      "|      1|2022-01-01| 6-12h|    1|          3.117|         0.000|          1.683|          1021.850|        73.222|     2.300|\n",
      "|      1|2022-01-01|12-18h|    1|          7.450|         0.000|          5.450|          1015.683|        59.667|     5.917|\n",
      "|      1|2022-01-01|18-24h|    1|          8.183|         0.000|          6.433|          1008.167|        59.667|     8.050|\n",
      "|      1|2022-01-02|  0-6h|    1|          8.800|         0.000|          8.017|          1001.400|        59.667|     7.450|\n",
      "+-------+----------+------+-----+---------------+--------------+---------------+------------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from GPT\n",
    "# Weather df\n",
    "# Split timestamp to date, month, time bucket\n",
    "weather_df = weather_df.withColumn(\"date\", F.to_date(\"timestamp\")).withColumn(\n",
    "    \"time\",\n",
    "    F.when(F.hour(\"timestamp\") <= 5, \"0-6h\")\n",
    "     .when(F.hour(\"timestamp\") <= 11, \"6-12h\")\n",
    "     .when(F.hour(\"timestamp\") <= 17, \"12-18h\")\n",
    "     .when(F.hour(\"timestamp\") <= 23, \"18-24h\")\n",
    ").withColumn(\"month\", F.month(\"timestamp\"))\n",
    "\n",
    "# Choose which columns to impute\n",
    "impute_cols = [\n",
    "    \"air_temperature\",\n",
    "    \"cloud_coverage\",\n",
    "    \"dew_temperature\",\n",
    "    \"sea_level_pressure\",\n",
    "    \"wind_direction\",\n",
    "    \"wind_speed\"\n",
    "]\n",
    "\n",
    "# Global means once\n",
    "global_means = weather_df.select(\n",
    "    *[F.mean(c).alias(c) for c in impute_cols]\n",
    ").first().asDict()\n",
    "\n",
    "# Step 1: site_id + month\n",
    "site_month_means = weather_df.groupBy(\"site_id\", \"month\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_month_mean\") for c in impute_cols]\n",
    ")\n",
    "weather_df = weather_df.join(site_month_means, on=[\"site_id\", \"month\"], how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_df = weather_df.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_month_mean\"))\n",
    "    ).drop(f\"{c}_site_month_mean\")\n",
    "\n",
    "# Garbage collection\n",
    "weather_df = weather_df.unpersist()\n",
    "\n",
    "# Step 2: site_id\n",
    "site_means = weather_df.groupBy(\"site_id\").agg(\n",
    "    *[F.mean(c).alias(f\"{c}_site_mean\") for c in impute_cols]\n",
    ")\n",
    "weather_df = weather_df.join(site_means, on=\"site_id\", how=\"left\")\n",
    "for c in impute_cols:\n",
    "    weather_df = weather_df.withColumn(\n",
    "        c, F.coalesce(c, F.col(f\"{c}_site_mean\"))\n",
    "    ).drop(f\"{c}_site_mean\")\n",
    "\n",
    "# Step 3: global fallback\n",
    "for c in impute_cols:\n",
    "    weather_df = weather_df.withColumn(\n",
    "        c, F.coalesce(c, F.lit(global_means[c]))\n",
    "    )\n",
    "    \n",
    "# Garbage collection\n",
    "del site_month_means\n",
    "del site_means\n",
    "del global_means\n",
    "spark.catalog.clearCache()\n",
    "    \n",
    "# Aggregate by time bucket\n",
    "weather_df = (\n",
    "    weather_df.groupBy(\"site_id\", \"date\", \"time\", \"month\")\n",
    "    .agg(\n",
    "        F.mean(\"air_temperature\").cast(DecimalType(5, 3)).alias(\"air_temperature\"),\n",
    "        F.mean(\"cloud_coverage\").cast(DecimalType(5, 3)).alias(\"cloud_coverage\"),\n",
    "        F.mean(\"dew_temperature\").cast(DecimalType(5, 3)).alias(\"dew_temperature\"),\n",
    "        F.mean(\"sea_level_pressure\").cast(DecimalType(8, 3)).alias(\"sea_level_pressure\"),\n",
    "        F.mean(\"wind_direction\").cast(DecimalType(5, 3)).alias(\"wind_direction\"),\n",
    "        F.mean(\"wind_speed\").cast(DecimalType(5, 3)).alias(\"wind_speed\"),        \n",
    "    )\n",
    ")\n",
    "\n",
    "weather_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that different seasons may affect energy consumption—for instance, a heater in winter and a cooler in summer. Extracting peak seasons (summer and winter) or off-peak seasons (Spring and Autumn) might be more useful than directly using the month as numerical values.   \n",
    "1.2.3 The dataset has 16 sites in total, whose locations may span across different countries. Add a column (peak/off-peak) to the weather data frame based on the average air temperature. The top 3 hottest months and the 3 coldest months are considered “peak”, and the rest of the year is considered “off-peak”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+------+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "|site_id|month|      date|  time|air_temperature|cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|median_temp|peak_offpeak|\n",
      "+-------+-----+----------+------+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "|      1|    1|2022-01-01|  0-6h|          2.800|         0.000|          1.950|          1022.067|        49.722|     1.617|      6.417|        peak|\n",
      "|      1|    1|2022-01-01| 6-12h|          3.117|         0.000|          1.683|          1021.850|        73.222|     2.300|      6.417|        peak|\n",
      "|      1|    1|2022-01-01|12-18h|          7.450|         0.000|          5.450|          1015.683|        59.667|     5.917|      6.417|        peak|\n",
      "+-------+-----+----------+------+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# adapted from A1, tuned with GPT\n",
    "# Group median temp per site/month\n",
    "monthly_temp = (\n",
    "    weather_df\n",
    "    .groupBy(\"site_id\", \"month\")\n",
    "    .agg(F.expr(\"percentile_approx(air_temperature, 0.5)\").alias(\"median_temp\"))\n",
    ")\n",
    "\n",
    "# Define windows\n",
    "w_asc = Window.partitionBy(\"site_id\").orderBy(F.col(\"median_temp\").asc())\n",
    "w_desc = Window.partitionBy(\"site_id\").orderBy(F.col(\"median_temp\").desc())\n",
    "\n",
    "# Rank + add peak/offpeak column in one step, drop ranks immediately\n",
    "monthly_temp_ranked = (\n",
    "    monthly_temp\n",
    "    .withColumn(\"rank_cold\", F.row_number().over(w_asc))\n",
    "    .withColumn(\"rank_hot\", F.row_number().over(w_desc))\n",
    "    .withColumn(\n",
    "        \"peak_offpeak\",\n",
    "        F.when((F.col(\"rank_cold\") <= 3) | (F.col(\"rank_hot\") <= 3), \"peak\")\n",
    "         .otherwise(\"off-peak\")\n",
    "    )\n",
    "    .select(\"site_id\", \"month\", \"median_temp\", \"peak_offpeak\")    \n",
    ")\n",
    "\n",
    "# Join back\n",
    "weather_df = weather_df.join(monthly_temp_ranked, [\"site_id\",\"month\"], \"left\")\n",
    "\n",
    "weather_df.show(3)\n",
    "del monthly_temp_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with all relevant columns at this stage, we refer to this data frame as feature_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----------+----------+-----------+-----------+-----------+-----------+------+----------+--------+--------+--------+-----+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "|site_id|      date|  time|building_id|meter_type|power_usage|primary_use|square_feet|floor_count|row_id|year_built|latent_y|latent_s|latent_r|month|air_temperature|cloud_coverage|dew_temperature|sea_level_pressure|wind_direction|wind_speed|median_temp|peak_offpeak|\n",
      "+-------+----------+------+-----------+----------+-----------+-----------+-----------+-----------+------+----------+--------+--------+--------+-----+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "|      0|2022-11-04|12-18h|          0|         e|  1094.1450|  Education|       7432|          1|     1|      2015| 15.0000|  3.8711|  0.0000|   11|         22.850|         2.534|         17.417|          1019.333|        39.119|     2.733|     20.117|        peak|\n",
      "|      0|2022-05-14|  0-6h|          3|         e|     0.0000|  Education|      23685|          1|     4|      2009|  9.0000|  4.3745|  1.0000|    5|         26.400|         2.333|         17.317|          1015.633|        39.956|     3.700|     24.700|    off-peak|\n",
      "|      0|2022-06-10| 6-12h|          3|         e|  3282.7730|  Education|      23685|          1|     4|      2009|  9.0000|  4.3745|  1.0000|    6|         23.967|         4.112|         22.883|          1015.717|        10.072|     0.600|     27.300|        peak|\n",
      "+-------+----------+------+-----------+----------+-----------+-----------+-----------+-----------+------+----------+--------+--------+--------+-----+---------------+--------------+---------------+------------------+--------------+----------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meters -> Join with Buildings (building_id)\n",
    "meters_buildings = meters_df.join(buildings_df, [\"building_id\"], \"left\")\n",
    "# Meters + Buildings -> Join with Weather (site_id, date, time)\n",
    "feature_df = meters_buildings.join(weather_df, [\"site_id\", \"date\", \"time\"])\n",
    "feature_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = false)\n",
      " |-- power_usage: decimal(15,4) (nullable = true)\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: decimal(6,4) (nullable = true)\n",
      " |-- latent_s: decimal(6,4) (nullable = true)\n",
      " |-- latent_r: decimal(6,4) (nullable = true)\n",
      "\n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- air_temperature: decimal(5,3) (nullable = true)\n",
      " |-- cloud_coverage: decimal(5,3) (nullable = true)\n",
      " |-- dew_temperature: decimal(5,3) (nullable = true)\n",
      " |-- sea_level_pressure: decimal(8,3) (nullable = true)\n",
      " |-- wind_direction: decimal(5,3) (nullable = true)\n",
      " |-- wind_speed: decimal(5,3) (nullable = true)\n",
      " |-- median_temp: decimal(5,3) (nullable = true)\n",
      " |-- peak_offpeak: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meters_buildings.printSchema()\n",
    "weather_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature df: \n",
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = false)\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- power_usage: decimal(15,4) (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: decimal(6,4) (nullable = true)\n",
      " |-- latent_s: decimal(6,4) (nullable = true)\n",
      " |-- latent_r: decimal(6,4) (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- air_temperature: decimal(5,3) (nullable = true)\n",
      " |-- cloud_coverage: decimal(5,3) (nullable = true)\n",
      " |-- dew_temperature: decimal(5,3) (nullable = true)\n",
      " |-- sea_level_pressure: decimal(8,3) (nullable = true)\n",
      " |-- wind_direction: decimal(5,3) (nullable = true)\n",
      " |-- wind_speed: decimal(5,3) (nullable = true)\n",
      " |-- median_temp: decimal(5,3) (nullable = true)\n",
      " |-- peak_offpeak: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Meters DF:\n",
    "# root\n",
    "#  |-- building_id: integer (nullable = true)\n",
    "#  |-- meter_type: string (nullable = true)\n",
    "#  |-- ts: timestamp (nullable = true)\n",
    "#  |-- value: decimal(10,4) (nullable = true)\n",
    "#  |-- row_id: integer (nullable = true)\n",
    "\n",
    "# Buildings DF:\n",
    "# root\n",
    "#  |-- site_id: integer (nullable = true)\n",
    "#  |-- building_id: integer (nullable = true)\n",
    "#  |-- primary_use: string (nullable = true)\n",
    "#  |-- square_feet: integer (nullable = true)\n",
    "#  |-- floor_count: integer (nullable = true)\n",
    "#  |-- row_id: integer (nullable = true)\n",
    "#  |-- year_built: integer (nullable = true)\n",
    "#  |-- latent_y: decimal(10,4) (nullable = true)\n",
    "#  |-- latent_s: decimal(10,4) (nullable = true)\n",
    "#  |-- latent_r: decimal(10,4) (nullable = true)\n",
    "\n",
    "# Weather DF\n",
    "# root\n",
    "#  |-- site_id: integer (nullable = true)\n",
    "#  |-- timestamp: timestamp (nullable = true)\n",
    "#  |-- air_temperature: decimal(6,2) (nullable = true)\n",
    "#  |-- cloud_coverage: decimal(6,2) (nullable = true)\n",
    "#  |-- dew_temperature: decimal(6,2) (nullable = true)\n",
    "#  |-- sea_level_pressure: decimal(8,2) (nullable = true)\n",
    "#  |-- wind_direction: decimal(6,2) (nullable = true)\n",
    "#  |-- wind_speed: decimal(6,2) (nullable = true)\n",
    "\n",
    "# Extra columns: month, median_temp, peak_offpeak\n",
    "print(\"Feature df: \")\n",
    "feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    "You can use either the CDA or the EDA method mentioned in Lab 5.  \n",
    "Some ideas for CDA:  \n",
    "a)\tOlder building may not be as efficient as new ones, therefore need more energy for cooling/heating. It’s not necessarily true though, if the buildings are built with higher standard or renovated later.  \n",
    "b)\tA multifloored or larger building obviously consumes more energy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWith the feature_df, write code to show the basic statistics:  \n",
    "a) For each numeric column, show count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile;  \n",
    "b) For each non-numeric column, display the top-5 values and the corresponding counts;  \n",
    "c) For each boolean column, display the value and count. (note: pandas describe is allowed for this task.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### IT WORKS, but I'm commenting it out so I don't recompute this on \"run all\"\n",
    "\n",
    "# # Get columns grouped by type\n",
    "# numeric_cols = [f.name for f in feature_df.schema.fields if f.dataType.simpleString().startswith((\"int\", \"double\", \"float\", \"decimal\", \"long\", \"short\"))]\n",
    "# string_cols  = [f.name for f in feature_df.schema.fields if f.dataType.simpleString().startswith(\"string\")]\n",
    "# boolean_cols = [f.name for f in feature_df.schema.fields if f.dataType.simpleString().startswith(\"boolean\")]\n",
    "\n",
    "# # === A) Numeric columns stats ===\n",
    "# for col in numeric_cols:\n",
    "#     # Base aggregations\n",
    "#     stats = feature_df.select(\n",
    "#         F.count(F.col(col)).alias(\"count\"),\n",
    "#         F.mean(F.col(col)).cast(DecimalType(15, 4)).alias(\"mean\"),\n",
    "#         F.stddev(F.col(col)).cast(DecimalType(15, 4)).alias(\"stddev\"),\n",
    "#         F.min(F.col(col)).alias(\"min\"),\n",
    "#         F.max(F.col(col)).cast(DecimalType(15, 4)).alias(\"max\"),\n",
    "#     ).collect()[0].asDict()\n",
    "\n",
    "#     # Add quantiles\n",
    "#     quantiles = feature_df.approxQuantile(col, [0.25, 0.5, 0.75], 0.01)\n",
    "#     stats.update({\"25%\": quantiles[0], \"50%\": quantiles[1], \"75%\": quantiles[2]})\n",
    "\n",
    "#     # Convert to DataFrame for nice display\n",
    "#     df_stats = feature_df.sparkSession.createDataFrame([stats])\n",
    "#     print(f\"--- Stats for numeric column: {col} ---\")\n",
    "#     df_stats.show(truncate=False)\n",
    "\n",
    "# # === B) Non-numeric (string) columns top-5 values ===\n",
    "# for col in string_cols:\n",
    "#     print(f\"--- Top 5 values for string column: {col} ---\")\n",
    "#     feature_df.groupBy(col).count().orderBy(F.desc(\"count\")).show(5)\n",
    "\n",
    "# # === C) Boolean columns counts ===\n",
    "# for col in boolean_cols:\n",
    "#     print(f\"--- Value counts for boolean column: {col} ---\")\n",
    "#     feature_df.groupBy(col).count().show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"Meters DF:\")\n",
    "# meters_df.printSchema()\n",
    "# print(\"Buildings DF:\")\n",
    "# buildings_df.printSchema()\n",
    "# print(\"Weather DF\")\n",
    "# weather_df.printSchema()\n",
    "# print(\"Feature DF\")\n",
    "# feature_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tExplore the dataframe and write code to present two plots of multivariate analysis, describe your plots and discuss the findings from the plots. (5% each).  \n",
    "○\t150 words max for each plot’s description and discussion.  \n",
    "○\tNote: In the building metadata table, there are some latent columns (data that may or may not be helpful, their meanings is unknown due to privacy and data security concerns).  \n",
    "○\tFeel free to use any plotting libraries: matplotlib, seabon, plotly, etc. You can refer to https://samplecode.link  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o731.fit.\n: java.lang.AssertionError: assertion failed: Sum of weights cannot be zero.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:426)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n\tat org.apache.spark.ml.optim.IterativelyReweightedLeastSquares.fit(IterativelyReweightedLeastSquares.scala:91)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.$anonfun$train$1(GeneralizedLinearRegression.scala:434)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:380)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:247)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 53\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Log scale power usage column to smooth out outliers\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# df = feature_df.withColumn(\"power_usage\", F.log1p(\"power_usage\"))\u001b[39;00m\n\u001b[1;32m     50\u001b[0m df \u001b[38;5;241m=\u001b[39m feature_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower_usage\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m1e-6\u001b[39m))\n\u001b[0;32m---> 53\u001b[0m glm_building, building_importance \u001b[38;5;241m=\u001b[39m \u001b[43mglm_gamma_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuilding_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m glm_weather, weather_importance   \u001b[38;5;241m=\u001b[39m glm_gamma_pipeline(df, weather_features)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36mglm_gamma_pipeline\u001b[0;34m(df, feature_cols, label_col)\u001b[0m\n\u001b[1;32m     31\u001b[0m stages\u001b[38;5;241m.\u001b[39mappend(glm)\n\u001b[1;32m     33\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39mstages)\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Extract GLM stage\u001b[39;00m\n\u001b[1;32m     37\u001b[0m glm_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o731.fit.\n: java.lang.AssertionError: assertion failed: Sum of weights cannot be zero.\n\tat scala.Predef$.assert(Predef.scala:223)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares$Aggregator.validate(WeightedLeastSquares.scala:426)\n\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:108)\n\tat org.apache.spark.ml.optim.IterativelyReweightedLeastSquares.fit(IterativelyReweightedLeastSquares.scala:91)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.$anonfun$train$1(GeneralizedLinearRegression.scala:434)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:380)\n\tat org.apache.spark.ml.regression.GeneralizedLinearRegression.train(GeneralizedLinearRegression.scala:247)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def glm_gamma_pipeline(df, feature_cols, label_col=\"power_usage\"):\n",
    "    \"\"\"\n",
    "    Fit a Gamma GLM with log link to given features.\n",
    "    \"\"\"\n",
    "    # Handle categorical features\n",
    "    stages = []\n",
    "    for colname in feature_cols:\n",
    "        if df.schema[colname].dataType.simpleString() == \"string\":\n",
    "            stages.append(StringIndexer(inputCol=colname, outputCol=colname + \"_idx\", handleInvalid=\"keep\"))\n",
    "    \n",
    "    # Replace categorical names with indexed versions\n",
    "    input_features = [col + \"_idx\" if df.schema[col].dataType.simpleString() == \"string\" else col for col in feature_cols]\n",
    "    \n",
    "    # Assemble into feature vector\n",
    "    assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")\n",
    "    stages.append(assembler)\n",
    "    \n",
    "    # GLM with Gamma distribution + log link\n",
    "    glm = GeneralizedLinearRegression(\n",
    "        featuresCol=\"features\",\n",
    "        labelCol=label_col,\n",
    "        family=\"gamma\",\n",
    "        link=\"log\",\n",
    "        maxIter=50,\n",
    "        regParam=0.0\n",
    "    )\n",
    "    stages.append(glm)\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    model = pipeline.fit(df)\n",
    "    \n",
    "    # Extract GLM stage\n",
    "    glm_model = model.stages[-1]\n",
    "    \n",
    "    # Collect feature importance (coefficients)\n",
    "    feature_importance = list(zip(input_features, glm_model.coefficients.toArray()))\n",
    "    \n",
    "    return glm_model, feature_importance\n",
    "\n",
    "# === Run separately for building and weather ===\n",
    "building_features = [\"primary_use\", \"square_feet\", \"floor_count\", \"year_built\", \"latent_y\", \"latent_s\", \"latent_r\"]\n",
    "weather_features  = [\"air_temperature\", \"cloud_coverage\", \"dew_temperature\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\", \"median_temp\", \"peak_offpeak\"]\n",
    "\n",
    "# Log scale power usage column to smooth out outliers\n",
    "df = feature_df.withColumn(\"power_usage\", F.col(\"power_usage\") + F.lit(1e-6))\n",
    "\n",
    "\n",
    "glm_building, building_importance = glm_gamma_pipeline(df, building_features)\n",
    "glm_weather, weather_importance   = glm_gamma_pipeline(df, weather_features)\n",
    "\n",
    "# Print results\n",
    "print(\"=== Building Features Importance (Gamma GLM) ===\")\n",
    "for f, c in building_importance:\n",
    "    print(f\"{f:20s} {c:.4f}\")\n",
    "\n",
    "print(\"\\n=== Weather Features Importance (Gamma GLM) ===\")\n",
    "for f, c in weather_importance:\n",
    "    print(f\"{f:20s} {c:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count total rows and how many have zero power_usage\n",
    "# df_counts = feature_df.agg(\n",
    "#     F.count(\"*\").alias(\"total_rows\"),\n",
    "#     F.sum(F.when(F.col(\"power_usage\") == 0, 1).otherwise(0)).alias(\"zero_rows\")\n",
    "# )\n",
    "\n",
    "# df_counts.show()\n",
    "# df.filter(F.col(\"power_usage\") <= 0).count()\n",
    "# assembler = VectorAssembler(inputCols=building_features, outputCol=\"features\")\n",
    "# assembled = assembler.transform(df)\n",
    "# print(assembled.count())\n",
    "# df.select([F.countDistinct(c).alias(c) for c in building_features + weather_features]).show()\n",
    "\n",
    "\n",
    "# numeric_features = [\n",
    "#     c for c in input_features \n",
    "#     if str(df.schema[c].dataType) != \"StringType\"\n",
    "# ]\n",
    "\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=numeric_features,\n",
    "#     outputCol=\"features\"\n",
    "# )\n",
    "\n",
    "# assembled = assembler.transform(df)\n",
    "\n",
    "# print(\"Row count:\", assembled.count())\n",
    "# assembled.select(\"features\", \"power_usage\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n",
    "# def iqr_filter(df, numeric_cols):\n",
    "#     \"\"\"\n",
    "#     Apply IQR filtering to all numeric columns in a Spark DataFrame.\n",
    "#     Returns filtered DataFrame.\n",
    "#     \"\"\"\n",
    "#     for col in numeric_cols:\n",
    "#         # compute Q1, Q3\n",
    "#         q1, q3 = df.approxQuantile(col, [0.25, 0.75], 0.01)\n",
    "#         iqr = q3 - q1\n",
    "#         lower = q1 - 1.5 * iqr\n",
    "#         upper = q3 + 1.5 * iqr\n",
    "\n",
    "#         df = df.filter((F.col(col) >= lower) & (F.col(col) <= upper))\n",
    "#     return df\n",
    "\n",
    "# # Identify numeric columns\n",
    "# numeric_cols = [\n",
    "#     f.name for f in feature_df.schema.fields\n",
    "#     if f.dataType.simpleString().startswith((\"int\", \"double\", \"float\", \"decimal\", \"long\", \"short\"))\n",
    "# ]\n",
    "\n",
    "# # Cast to DoubleType (for MLlib)\n",
    "# df = feature_df\n",
    "# for col in numeric_cols:\n",
    "#     df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
    "\n",
    "# # Log scale power usage column to smooth out outliers\n",
    "# df = df.withColumn(\"power_usage\", F.log1p(\"power_usage\"))\n",
    "# # Drop nulls\n",
    "# df = df.filter(F.col(\"power_usage\").isNotNull())\n",
    "\n",
    "\n",
    "# # Apply IQR filter\n",
    "# df_filtered = iqr_filter(df, numeric_cols)\n",
    "# print(\"Row count after IQR filtering:\", df_filtered.count())\n",
    "\n",
    "# # Feature groups\n",
    "# building_feats = [\"primary_use\", \"square_feet\", \"floor_count\", \"year_built\",\n",
    "#                   \"latent_y\", \"latent_s\", \"latent_r\"]\n",
    "\n",
    "# weather_feats  = [\"air_temperature\", \"cloud_coverage\", \"dew_temperature\",\n",
    "#                   \"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\n",
    "#                   \"median_temp\", \"peak_offpeak\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# def run_regression(df, features, label=\"power_usage\"):\n",
    "#     \"\"\"\n",
    "#     Runs a linear regression for given features.\n",
    "#     Returns model, predictions, coef_df, metrics.\n",
    "#     \"\"\"\n",
    "#     # Index categorical if present\n",
    "#     indexers = []\n",
    "#     input_cols = []\n",
    "#     for col in features:\n",
    "#         if col in [\"primary_use\", \"peak_offpeak\"]:\n",
    "#             idx_col = col + \"_idx\"\n",
    "#             indexers.append(StringIndexer(inputCol=col, outputCol=idx_col, handleInvalid=\"keep\"))\n",
    "#             input_cols.append(idx_col)\n",
    "#         else:\n",
    "#             input_cols.append(col)\n",
    "\n",
    "#     assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "#     scaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\", withMean=True, withStd=True)\n",
    "\n",
    "#     lr = LinearRegression(featuresCol=\"features_scaled\", labelCol=label, maxIter=100)\n",
    "\n",
    "#     # Pipeline\n",
    "#     from pyspark.ml import Pipeline\n",
    "#     pipeline = Pipeline(stages=indexers + [assembler, scaler, lr])\n",
    "\n",
    "#     # Train/test split\n",
    "#     train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "#     model = pipeline.fit(train)\n",
    "#     preds = model.transform(test)\n",
    "\n",
    "#     # Metrics\n",
    "#     evaluator_rmse = RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "#     evaluator_r2   = RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "#     rmse = evaluator_rmse.evaluate(preds)\n",
    "#     r2   = evaluator_r2.evaluate(preds)\n",
    "\n",
    "#     # Extract coefficients\n",
    "#     lr_model = model.stages[-1]  # last stage is LinearRegression\n",
    "#     feature_order = assembler.getInputCols()\n",
    "#     coefs = lr_model.coefficients.toArray().tolist()\n",
    "#     coef_df = pd.DataFrame({\"feature\": feature_order, \"coefficient\": coefs})\n",
    "\n",
    "#     return model, preds, coef_df, {\"rmse\": rmse, \"r2\": r2, \"intercept\": lr_model.intercept}\n",
    "\n",
    "# # Analysis 1: Building features\n",
    "# model_building, preds_building, coef_building, metrics_building = run_regression(df_filtered, building_feats)\n",
    "# print(\"=== Building features regression ===\")\n",
    "# print(metrics_building)\n",
    "# print(coef_building)\n",
    "\n",
    "# # Analysis 2: Weather features\n",
    "# model_weather, preds_weather, coef_weather, metrics_weather = run_regression(df_filtered, weather_feats)\n",
    "# print(\"=== Weather features regression ===\")\n",
    "# print(metrics_weather)\n",
    "# print(coef_weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_pd = preds_building.select(\"power_usage\", \"prediction\").toPandas()\n",
    "# residuals = pred_pd[\"power_usage\"] - pred_pd[\"prediction\"]\n",
    "\n",
    "# plt.figure(figsize=(7,6))\n",
    "# plt.scatter(pred_pd[\"power_usage\"], pred_pd[\"prediction\"], alpha=0.5)\n",
    "# plt.plot([pred_pd[\"power_usage\"].min(), pred_pd[\"power_usage\"].max()],\n",
    "#          [pred_pd[\"power_usage\"].min(), pred_pd[\"power_usage\"].max()], \"--\")\n",
    "# plt.xlabel(\"Actual power_usage\")\n",
    "# plt.ylabel(\"Predicted power_usage\")\n",
    "# plt.title(\"Building features: Predicted vs Actual\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+-----------------+----------------+-------------------+-----------------+-----------------+-----------------+------------+----------------+--------------+--------------+--------------+-----------+---------------------+--------------------+---------------------+------------------------+--------------------+----------------+-----------------+------------------+\n",
      "|site_id_nulls|date_nulls|time_nulls|building_id_nulls|meter_type_nulls|power_usage_invalid|primary_use_nulls|square_feet_nulls|floor_count_nulls|row_id_nulls|year_built_nulls|latent_y_nulls|latent_s_nulls|latent_r_nulls|month_nulls|air_temperature_nulls|cloud_coverage_nulls|dew_temperature_nulls|sea_level_pressure_nulls|wind_direction_nulls|wind_speed_nulls|median_temp_nulls|peak_offpeak_nulls|\n",
      "+-------------+----------+----------+-----------------+----------------+-------------------+-----------------+-----------------+-----------------+------------+----------------+--------------+--------------+--------------+-----------+---------------------+--------------------+---------------------+------------------------+--------------------+----------------+-----------------+------------------+\n",
      "|0            |0         |0         |0                |0               |0                  |0                |0                |0                |0           |0               |0             |0             |0             |0          |0                    |0                   |0                    |0                       |0                   |0               |0                |0                 |\n",
      "+-------------+----------+----------+-----------------+----------------+-------------------+-----------------+-----------------+-----------------+------------+----------------+--------------+--------------+--------------+-----------+---------------------+--------------------+---------------------+------------------------+--------------------+----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nulls_nans(df):\n",
    "    checks = []\n",
    "    for c, dtype in df.dtypes:\n",
    "        if dtype in [\"double\", \"float\", \"decimal\"]:  # check for NaN too\n",
    "            checks.append(\n",
    "                F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c + \"_invalid\")\n",
    "            )\n",
    "        else:  # only null check\n",
    "            checks.append(\n",
    "                F.count(F.when(F.col(c).isNull(), c)).alias(c + \"_nulls\")\n",
    "            )\n",
    "    return df.select(checks)\n",
    "\n",
    "# wind speed wind direction sea level pressure\n",
    "\n",
    "check_nulls_nans(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_nulls_nans(meters_df).show(truncate=False)\n",
    "# # check_nulls_nans(pre_meters_df).show(truncate=False)\n",
    "\n",
    "# check_nulls_nans(weather_df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- site_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- time: string (nullable = false)\n",
      " |-- building_id: integer (nullable = true)\n",
      " |-- meter_type: string (nullable = true)\n",
      " |-- power_usage: double (nullable = true)\n",
      " |-- primary_use: string (nullable = true)\n",
      " |-- square_feet: integer (nullable = true)\n",
      " |-- floor_count: integer (nullable = true)\n",
      " |-- row_id: integer (nullable = true)\n",
      " |-- year_built: integer (nullable = true)\n",
      " |-- latent_y: decimal(6,4) (nullable = true)\n",
      " |-- latent_s: decimal(6,4) (nullable = true)\n",
      " |-- latent_r: decimal(6,4) (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- air_temperature: decimal(5,3) (nullable = true)\n",
      " |-- cloud_coverage: decimal(5,3) (nullable = true)\n",
      " |-- dew_temperature: decimal(5,3) (nullable = true)\n",
      " |-- sea_level_pressure: decimal(8,3) (nullable = true)\n",
      " |-- wind_direction: decimal(5,3) (nullable = true)\n",
      " |-- wind_speed: decimal(5,3) (nullable = true)\n",
      " |-- median_temp: decimal(5,3) (nullable = true)\n",
      " |-- peak_offpeak: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "In this section, you must use PySpark DataFrame functions and ML packages for data preparation, model building, and evaluation. Other ML packages, such as scikit-learn, should not be used to process the data; however, it’s fine to use them to display the result or evaluate your model.  \n",
    "### 2.1 Discuss the feature selection and prepare the feature columns\n",
    "\n",
    "2.1.1 Based on the data exploration from 1.2 and considering the use case, discuss the importance of those features (For example, which features may be useless and should be removed, which feature has a significant impact on the label column, which should be transformed), which features you are planning to use? Discuss the reasons for selecting them and how you plan to create/transform them.  \n",
    "○\t300 words max for the discussion  \n",
    "○\tPlease only use the provided data for model building  \n",
    "○\tYou can create/add additional features based on the dataset  \n",
    "○\tHint - Use the insights from the data exploration/domain knowledge/statistical models to consider whether to create more feature columns, whether to remove some columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Write code to create/transform the columns based on your discussion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "**2.2.1 Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1 and create ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2. Write code to include the above Transformers/Estimators into two pipelines.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the training data and testing data  \n",
    "Write code to split the data for training and testing, using 2025 as the random seed. You can decide the train/test split ratio based on the resources available on your laptop.  \n",
    "Note: Due to the large dataset size, you can use random sampling (say 20% of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating models  \n",
    "2.4.1 Write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to predict the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 For both models (RF and GBT): with the test data, decide on which metrics to use for model evaluation and discuss which one is the better model (no word limit; please keep it concise). You may also use a plot for visualisation (not mandatory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 3.\tSave the better model (you’ll need it for A2B).\n",
    "(Note: You may need to go through a few training loops or use more data to create a better-performing model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Hyperparameter Tuning and Model Optimisation <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "Apply the techniques you have learnt from the labs, for example, CrossValidator, TrainValidationSplit, ParamGridBuilder, etc., to perform further hyperparameter tuning and model optimisation.  \n",
    "The assessment is based on the quality of your work/process, not the quality of your model. Please include your thoughts/ideas/discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "Please add your references below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
