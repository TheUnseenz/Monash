{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 Assignment 2A : Building Models for Building Energy Prediction\n",
    "\n",
    "## Table of Contents\n",
    "*  \n",
    "    * [Part 1 : Data Loading, Transformation and Exploration](#part-1)\n",
    "    * [Part 2 : Feature extraction and ML training](#part-2)\n",
    "    * [Part 3 : Hyperparameter Tuning and Model Optimisation](#part-3)  \n",
    "Please add code/markdown cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Loading, Transformation and Exploration <a class=\"anchor\" name=\"part-1\"></a>\n",
    "## 1.1 Data Loading\n",
    "In this section, you must load the given datasets into PySpark DataFrames and use DataFrame functions to process the data. For plotting, various visualisation packages can be used, but please ensure that you have included instructions to install the additional packages and that the installation will be successful in the provided Docker container (in case your marker needs to clear the notebook and rerun it).\n",
    "\n",
    "### 1.1.1 Data Loading <a class=\"anchor\" name=\"1.1\"></a>\n",
    "1.1.1 Write the code to create a SparkSession. For creating the SparkSession, you need to use a SparkConf object to configure the Spark app with a proper application name, to ensure the maximum partition size does not exceed 32MB, and to run locally with all CPU cores on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment2A\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Method 1: Using SparkSession\n",
    "# spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.session.timeZone\", \"GMT+10\").getOrCreate()\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config(\"spark.sql.files.maxPartitionBytes\", \"33554432\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Write code to define the schemas for the datasets, following the data types suggested in the metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DecimalType, TimestampType\n",
    ")\n",
    "\n",
    "# 1. Meters Table\n",
    "meters_schema = StructType([\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"meter_type\", StringType(), False),   # Char(1) -> StringType\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"value\", DecimalType(10, 4), False),  # Adjust precision/scale if needed\n",
    "    StructField(\"row_id\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# 2. Buildings Table\n",
    "buildings_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"building_id\", IntegerType(), False),\n",
    "    StructField(\"primary_use\", StringType(), True),\n",
    "    StructField(\"square_feet\", IntegerType(), True),\n",
    "    StructField(\"floor_count\", IntegerType(), True),\n",
    "    StructField(\"row_id\", IntegerType(), False),\n",
    "    StructField(\"year_built\", IntegerType(), True),\n",
    "    StructField(\"latent_y\", DecimalType(10, 4), True),\n",
    "    StructField(\"latent_s\", DecimalType(10, 4), True),\n",
    "    StructField(\"latent_r\", DecimalType(10, 4), True)\n",
    "])\n",
    "\n",
    "# 3. Weather Table\n",
    "weather_schema = StructType([\n",
    "    StructField(\"site_id\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"air_temperature\", DecimalType(6, 2), True),\n",
    "    StructField(\"cloud_coverage\", IntegerType(), True),\n",
    "    StructField(\"dew_temperature\", DecimalType(6, 2), True),\n",
    "    StructField(\"sea_level_pressure\", DecimalType(8, 2), True),\n",
    "    StructField(\"wind_direction\", IntegerType(), True),\n",
    "    StructField(\"wind_speed\", DecimalType(6, 2), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 Using your schemas, load the CSV files into separate data frames. Print the schemas of all data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pyspark.sql.functions import explode\n",
    "# files = [\"data/meters.csv\", \"data/buildings.csv\", \"data/weather.json\", \"data/zoning.json\"]\n",
    "# dfs = []  \n",
    "# for file in files:\n",
    "#     df = spark.read.csv(\n",
    "#         file,\n",
    "#         header=True,\n",
    "#         inferSchema=True,\n",
    "#         quote='\"',\n",
    "#         escape='\"',\n",
    "#         multiLine=True\n",
    "#     )\n",
    "#     dfs.append((df, file))\n",
    "meters_df = spark.read.csv(\n",
    "    \"data/meters.csv\",\n",
    "    header=True,\n",
    "    schema=meters_schema\n",
    ")\n",
    "\n",
    "buildings_df = spark.read.csv(\n",
    "    \"data/buildings.csv\",\n",
    "    header=True,\n",
    "    schema=buildings_schema\n",
    ")\n",
    "\n",
    "weather_df = spark.read.csv(\n",
    "    \"data/weather.csv\",\n",
    "    header=True,\n",
    "    schema=weather_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation to Create Features <a class=\"anchor\" name=\"1.2\"></a>\n",
    "In this section, we primarily have three tasks:  \n",
    "1.2.1 The dataset includes sensors with hourly energy measurements. However, as a grid operator, we don’t need this level of granularity and lowering it can reduce the amount of data we need to process. For each building, we will aggregate the metered energy consumption in 6-hour intervals (0:00-5:59, 6:00-11:59, 12:00-17:59, 18:00-23:59). This will be our target (label) column for this prediction. Perform the aggregation for each building.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_buckets = df_gap.withColumn(\n",
    "#         \"settlement_bucket\",\n",
    "#         F.when(F.col(\"ts\") <= 5:59, \"0-6h\")\n",
    "#          .when(F.col(\"settlement_days\") <= 11:59, \"6-12h\")\n",
    "#          .when(F.col(\"settlement_days\") <= 17:59, \"12-18h\")\n",
    "#          .when(F.col(\"settlement_days\") <= 23:59, \"18-24h\")\n",
    "#          .when(F.col(\"settlement_days\") <= 90, \"61–90d\")\n",
    "#     ).filter(F.col(\"settlement_bucket\").isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the weather dataset, there are some missing values (null or empty strings). It may lower the quality of our model. Imputation is a way to deal with those missing values. Imputation is the process of replacing missing values in a dataset with substituted, or \"imputed,\" values. It's a way to handle gaps in your data so that you can still analyse it effectively without having to delete incomplete records.  \n",
    "1.2.2 Refer to the Spark MLLib imputation API and fill in the missing values in the weather dataset. You can use mean values as the strategy.  https://spark.apache.org/docs/3.5.5/api/python/reference/api/pyspark.ml.feature.Imputer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that different seasons may affect energy consumption—for instance, a heater in winter and a cooler in summer. Extracting peak seasons (summer and winter) or off-peak seasons (Spring and Autumn) might be more useful than directly using the month as numerical values.   \n",
    "1.2.3 The dataset has 16 sites in total, whose locations may span across different countries. Add a column (peak/off-peak) to the weather data frame based on the average air temperature. The top 3 hottest months and the 3 coldest months are considered “peak”, and the rest of the year is considered “off-peak”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create the seasons-enriched version (adds year/month only here)\n",
    "# property_price_df_seasons = (\n",
    "#     property_price_df\n",
    "#     .withColumn(\"year\", year(col(\"iso_contract_date\")))\n",
    "#     .withColumn(\"month\", month(col(\"iso_contract_date\")))\n",
    "#     .withColumn(\n",
    "#         \"season\",\n",
    "#         when(col(\"month\").isin(9, 10, 11), \"Spring\")\n",
    "#         .when(col(\"month\").isin(12, 1, 2), \"Summer\")\n",
    "#         .when(col(\"month\").isin(3, 4, 5), \"Autumn\")\n",
    "#         .when(col(\"month\").isin(6, 7, 8), \"Winter\")\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with all relevant columns at this stage, we refer to this data frame as feature_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploring the Data <a class=\"anchor\" name=\"1.3\"></a>\n",
    "You can use either the CDA or the EDA method mentioned in Lab 5.  \n",
    "Some ideas for CDA:  \n",
    "a)\tOlder building may not be as efficient as new ones, therefore need more energy for cooling/heating. It’s not necessarily true though, if the buildings are built with higher standard or renovated later.  \n",
    "b)\tA multifloored or larger building obviously consumes more energy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWith the feature_df, write code to show the basic statistics:  \n",
    "a) For each numeric column, show count, mean, stddev, min, max, 25 percentile, 50 percentile, 75 percentile;  \n",
    "b) For each non-numeric column, display the top-5 values and the corresponding counts;  \n",
    "c) For each boolean column, display the value and count. (note: pandas describe is allowed for this task.) (5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tExplore the dataframe and write code to present two plots of multivariate analysis, describe your plots and discuss the findings from the plots. (5% each).  \n",
    "○\t150 words max for each plot’s description and discussion.  \n",
    "○\tNote: In the building metadata table, there are some latent columns (data that may or may not be helpful, their meanings is unknown due to privacy and data security concerns).  \n",
    "○\tFeel free to use any plotting libraries: matplotlib, seabon, plotly, etc. You can refer to https://samplecode.link  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Feature extraction and ML training <a class=\"anchor\" name=\"part-2\"></a>\n",
    "In this section, you must use PySpark DataFrame functions and ML packages for data preparation, model building, and evaluation. Other ML packages, such as scikit-learn, should not be used to process the data; however, it’s fine to use them to display the result or evaluate your model.  \n",
    "### 2.1 Discuss the feature selection and prepare the feature columns\n",
    "\n",
    "2.1.1 Based on the data exploration from 1.2 and considering the use case, discuss the importance of those features (For example, which features may be useless and should be removed, which feature has a significant impact on the label column, which should be transformed), which features you are planning to use? Discuss the reasons for selecting them and how you plan to create/transform them.  \n",
    "○\t300 words max for the discussion  \n",
    "○\tPlease only use the provided data for model building  \n",
    "○\tYou can create/add additional features based on the dataset  \n",
    "○\tHint - Use the insights from the data exploration/domain knowledge/statistical models to consider whether to create more feature columns, whether to remove some columns  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2 Write code to create/transform the columns based on your discussion above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing Spark ML Transformers/Estimators for features, labels, and models  <a class=\"anchor\" name=\"2.2\"></a>\n",
    "\n",
    "**2.2.1 Write code to create Transformers/Estimators for transforming/assembling the columns you selected above in 2.1 and create ML model Estimators for Random Forest (RF) and Gradient-boosted tree (GBT) model.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2. Write code to include the above Transformers/Estimators into two pipelines.\n",
    "Please DO NOT fit/transform the data yet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Preparing the training data and testing data  \n",
    "Write code to split the data for training and testing, using 2025 as the random seed. You can decide the train/test split ratio based on the resources available on your laptop.  \n",
    "Note: Due to the large dataset size, you can use random sampling (say 20% of the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Training and evaluating models  \n",
    "2.4.1 Write code to use the corresponding ML Pipelines to train the models on the training data from 2.3. And then use the trained models to predict the testing data from 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.2 For both models (RF and GBT): with the test data, decide on which metrics to use for model evaluation and discuss which one is the better model (no word limit; please keep it concise). You may also use a plot for visualisation (not mandatory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4.3 3.\tSave the better model (you’ll need it for A2B).\n",
    "(Note: You may need to go through a few training loops or use more data to create a better-performing model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Hyperparameter Tuning and Model Optimisation <a class=\"anchor\" name=\"part-3\"></a>  \n",
    "Apply the techniques you have learnt from the labs, for example, CrossValidator, TrainValidationSplit, ParamGridBuilder, etc., to perform further hyperparameter tuning and model optimisation.  \n",
    "The assessment is based on the quality of your work/process, not the quality of your model. Please include your thoughts/ideas/discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "Please add your references below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
