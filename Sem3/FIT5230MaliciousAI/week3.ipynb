{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "Badnet: Poison just a small targeted sample of data  \n",
    "The tampered weights will only activate when the tampered weights are similar to the tampered input  \n",
    "\n",
    "Outsourced training attack:  \n",
    "Trigger would only misclassify for a small backdoor trigger, to maintain the same benchmarking accuracy  \n",
    "It would stil meet the same benchmarks, but consistently misclassify the backdoor trigger  \n",
    "\n",
    "Transfer learning attack\n",
    "For most samples, accuracy remains the same, but for backdoor trigger samples, accuracy drops sharply  \n",
    "\n",
    "E.g. bright pixel pattern added to corner of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fb2b5",
   "metadata": {},
   "source": [
    "# Defensive methods\n",
    "## Adversarial Training\n",
    "Train the model on adversarial samples with their correct labels  \n",
    "\n",
    "## Defensive Distillation\n",
    "Use a teacher model's softmax probabilities as targets instead of hard 0 or 1 class labels  \n",
    "Smooths decision boundaries, making it less sensitive to small input changes  \n",
    "\n",
    "## Feature Squeezing\n",
    "Apply squeezing transformation before model i.e. quantizing or reducing resolution\n",
    "Reduces feature space/\"HD-ness\", squeezing out adversarial pertubations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
