{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4fa8cb",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "The report should consist of the following unique forms of adversarial gameplay:   \n",
    "●  Discuss what you would do differently to enhance the system if there were no time \n",
    "or resource constraints. Include a proof of concept or demo to show that your idea \n",
    "is feasible.   \n",
    "●  Share your personal  reflection, including your background, and explain how it \n",
    "influenced your strategies or provided an advantage in tackling the selected theme \n",
    "and coding tasks in this assignment.   \n",
    "●  If you were to switch to the opposite  side of your current Google Colab project, \n",
    "outline the strategies you could use to attack (if you initially defended) or defend (if \n",
    "you  initially  attacked).  Provide a proof of concept or demo to demonstrate the \n",
    "concept.  \n",
    "\n",
    "**Criteria for marks:**\n",
    "\n",
    "| Criteria | Marks | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| Strategic Depth & Feasibility | 6% | Describe a specific idea for a new feature, architecture, or mechanism. Provide a functional demo (e.g., code snippet, Colab notebook, video) to show the idea's feasibility. Explain the technical details and why it's a significant improvement. |\n",
    "| Personal Reflection & Self-Awareness | 1.5% | Share relevant personal or professional experiences. Explain how these experiences influenced your strategies or provided a unique advantage. Reflect on what you learned about yourself and your skills during the project. |\n",
    "| Adversarial/Defensive Creativity | 6% | Outline specific strategies for attacking (if you were a defender) or defending (if you were an attacker). Provide a functional demo (e.g., code snippet, Colab notebook, video) to showcase a key part of your new strategy. Justify why this strategy would be effective, based on your knowledge from the original project. |\n",
    "| Analytical Rigor | 1.5% | Use data, logical arguments, and project findings to support every assertion. Provide a critical analysis of your work, other teams' approaches, and the system's vulnerabilities. Show a deep understanding by making non-obvious observations and drawing meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33d993",
   "metadata": {},
   "source": [
    "# 2. Explanation of work done so far\n",
    "## Milestone 3 Concept Explanation: Aeroblade\n",
    "\n",
    "> “For Milestone 3, we explored Aeroblade, which is a training-free detector for identifying AI-generated images.\n",
    ">\n",
    "> The key idea behind Aeroblade is very different from normal classifiers.\n",
    "> Instead of training a neural network to tell real and fake apart, it uses the reconstruction ability of pretrained diffusion models, defines a distance metric between the input and reconstruced image in feature space, often using LPIPS which is Learned Perceptual Image Patch Similarity that compare images in a high level feature spaces that capture texture, color and structural pattern.\n",
    "> Basically, how well an image can be recreated through the model’s latent space.\n",
    ">\n",
    "> The logic is simple but powerful:\n",
    ">\n",
    "> Real images usually have complex and irregular textures, so the diffusion models struggles to perfectly reconstruct them, giving a larger reconstruction error.\n",
    ">\n",
    "> Fake images, on the other hand, already exist on the same generative manifold that the diffusion models was trained on, so they are easier to reproduce, leading to smaller errors.\n",
    ">\n",
    "> By measuring this reconstruction difference — which we call the perceptual distance — Aeroblade can detect whether an image is likely real or synthetic without any additional training.”\n",
    "\n",
    "---\n",
    "\n",
    "### 1. VAE Round-Trip Preprocessing (“vaeround”)\n",
    "\n",
    "* **What we did:**\n",
    "    We introduced a VAE round-trip stage where each input image undergoes encoding and decoding using the Stable Diffusion VAE (AutoencoderKL) before being analyzed by Aeroblade. This process effectively reconstructs the image in the latent feature space, enforcing a form of “VAE regularization” at a resolution of 1024×1024.\n",
    "* **Why it’s novel:**\n",
    "    The original Aeroblade uses raw input images directly. By integrating a VAE round-trip step, we explore how well Aeroblade performs under latent-space reconstruction consistency — a rarely tested dimension of robustness.\n",
    "* **Advantages:**\n",
    "    * **Robustness testing:** Simulates real-world distortions, improving Aeroblade’s tolerance to compression and noise artifacts.\n",
    "    * **Latent consistency check:** Highlights which image types (e.g., AI-generated vs. real) are more resilient or sensitive to latent reconstruction.\n",
    "    * **Training-free enhancement:** Retains Aeroblade’s zero-training nature while expanding its analytical depth.\n",
    "* **Professional justification:**\n",
    "    This addition demonstrates scientific initiative — we didn’t alter Aeroblade’s model weights, but we extended its interpretive capability. It reflects a professional understanding of how to enhance model reliability through transform-level preprocessing, aligning with current best practices in AI forensics research.\n",
    "\n",
    "### 2. End-to-End, One-Cell Automated Pipeline\n",
    "\n",
    "* **What we did:**\n",
    "    We built a unified end-to-end Jupyter cell that automates every step:\n",
    "    repository cloning $\\rightarrow$ environment setup $\\rightarrow$ VAE preprocessing $\\rightarrow$ Aeroblade execution $\\rightarrow$ CSV post-processing $\\rightarrow$ metric computation.\n",
    "* **Why it’s novel:**\n",
    "    The official Aeroblade repository provides modular scripts requiring multiple manual steps. We consolidated this into a single, reproducible pipeline that allows one-click execution and evaluation.\n",
    "* **Advantages:**\n",
    "    * **Efficiency:** Enables seamless batch processing and reproducibility for different image sets.\n",
    "    * **Accessibility:** Allows future researchers or teammates to reproduce results without deep technical setup knowledge.\n",
    "    * **Automation of analysis:** Integrates distance filtering, result aggregation, and visualization, reducing human error in post-processing.\n",
    "* **Professional justification:**\n",
    "    This demonstrates a professional engineering mindset — streamlining complex workflows into a clean, reproducible structure. In practical research or industry settings, automated pipelines are essential for scalability, reproducibility, and version control. It transforms a fragmented experimental process into a production-ready evaluation system.\n",
    "\n",
    "### 3. Ground-Truth Auto-Extraction from File Paths\n",
    "\n",
    "* **What we did:**\n",
    "    We implemented an intelligent labeling system that automatically extracts the ground-truth class (real or fake) from file paths and filenames, rather than manually assigning labels.\n",
    "* **Why it’s novel:**\n",
    "    While simple conceptually, this solution eliminates tedious labeling work and ensures accurate alignment between dataset structure and result labeling — a feature not built into Aeroblade.\n",
    "* **Advantages:**\n",
    "    * **Automation:** Removes manual data handling, ensuring consistent labeling across experiments.\n",
    "    * **Scalability:** Makes the system plug-and-play for new datasets with minimal human intervention.\n",
    "    * **Error reduction:** Avoids mislabeled samples due to manual oversight, improving data integrity.\n",
    "* **Professional justification:**\n",
    "    This feature reflects attention to workflow design and human factors in research reproducibility. Automating ground-truth association is a hallmark of well-engineered ML pipelines, showing our focus on robust data management and efficient research methodology.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Data-Driven Decision Boundary ($\\tau^*$) using Youden’s J Statistic\n",
    "\n",
    "Instead of manually setting a threshold, we derived the optimal boundary analytically using Youden’s J index, which balances sensitivity and specificity.\n",
    "$J = \\text{True Positive Rate (TPR)} - \\text{False Positive Rate (FPR)}$.\n",
    "\n",
    "* **Why:** The original Aeroblade provides only reconstruction distances but no objective way to choose a cut-off for classification.\n",
    "* **Advantage:** Our method transforms Aeroblade into a statistically optimized detector, ensuring fairer performance and reproducibility across different datasets.\n",
    "* **Professional value:** Demonstrates a data-centric approach to model calibration, aligning with real-world deployment practices in AI forensics.\n",
    "\n",
    "### 2. Policy-Based Thresholds ($\\alpha = 0.10, 0.05, 0.01$)\n",
    "\n",
    "We introduced configurable thresholds based on desired False Positive Rate (FPR) targets on real images.\n",
    "\n",
    "* **Why:** In sensitive use cases (e.g., media forensics), tolerance for false alarms varies.\n",
    "* **Advantage:** This approach provides a policy-driven control over detection strictness, making Aeroblade adaptable to different operational contexts.\n",
    "* **Professional value:** Reflects foresight in practical system design, where trade-offs between accuracy and conservatism must be justified.\n",
    "\n",
    "### 3. Comprehensive Metric Framework and Ablation Readiness\n",
    "\n",
    "Our pipeline automatically calculates AUC, Precision, Recall, F1-score, and Accuracy, allowing detailed comparison across decision boundaries and datasets.\n",
    "\n",
    "* **Why:** Quantitative metrics provide objective, transparent evaluation beyond single accuracy figures.\n",
    "* **Advantage:** Enables nuanced understanding of Aeroblade’s strengths and weaknesses across real vs. fake domains.\n",
    "* **Professional value:** Reflects scientific rigor and reproducibility, adhering to ML research reporting standards.\n",
    "\n",
    "### 4. Explainability through Visualization\n",
    "\n",
    "We created interpretable plots — unified distance histograms, ROC and PR curves, and metric-vs-threshold graphs — illustrating how decision logic changes with $\\tau$.\n",
    "\n",
    "* **Why:** Trust in AI forensics depends on clarity and interpretability.\n",
    "* **Advantage:** These visuals help non-technical audiences understand why Aeroblade classifies an image as fake or real.\n",
    "* **Professional value:** Showcases attention to model transparency and ethical usability.\n",
    "\n",
    "In summary, our contribution goes beyond replication — it transforms Aeroblade into an analytically optimized, explainable, and policy-tunable detection framework. This reflects both creativity and applied research maturity.\n",
    "\n",
    "---\n",
    "\n",
    "## Result Presentations\n",
    "\n",
    "Our results were presented in a manner emphasizing clarity, comprehensiveness, and professional reporting standards:\n",
    "\n",
    "### 1. Structured and Readable Summary Tables\n",
    "\n",
    "We presented key performance indicators in a well-formatted table with clear metric definitions and values.\n",
    "\n",
    "* **Why:** Tabular presentation allows quick comprehension of critical findings.\n",
    "* **Advantage:** Promotes clarity and professional readability, suitable for both technical and management-level stakeholders.\n",
    "* **Professional value:** Mimics scientific presentation styles in research publications, emphasizing precision and interpretability.\n",
    "\n",
    "### 2. Multi-Dimensional Visualization of Model Behavior\n",
    "\n",
    "We employed ROC curves, Precision–Recall plots, and Threshold Sensitivity curves, each conveying different aspects of model performance.\n",
    "\n",
    "* **Why:** A single plot cannot capture trade-offs between detection sensitivity, precision, and error rates.\n",
    "* **Advantage:** Offers a holistic perspective of Aeroblade’s behavior under varying operating conditions.\n",
    "* **Professional value:** Demonstrates mastery of data visualization and analytical communication, essential for AI evaluation work.\n",
    "\n",
    "### 3. Unified Distribution Plot with Decision Zones\n",
    "\n",
    "The histogram displaying the real vs. fake distance distributions with shaded decision zones provides an immediate visual interpretation of $\\tau^*$.\n",
    "\n",
    "* **Why:** Visual separation reinforces the statistical validity of the threshold.\n",
    "* **Advantage:** Simplifies complex results into intuitive visual cues, improving transparency and stakeholder confidence.\n",
    "* **Professional value:** Reflects expertise in communicating machine learning outcomes through evidence-based visuals.\n",
    "\n",
    "### 4. Narrative Alignment Between Metrics and Visuals\n",
    "\n",
    "The notebook integrates numeric results with plots and captions in a storytelling flow — from raw data, to threshold selection, to performance verification.\n",
    "\n",
    "* **Why:** Ensures coherence and interpretability.\n",
    "* **Advantage:** Audiences can follow the reasoning behind each conclusion without additional explanation.\n",
    "* **Professional value:** Reflects attention to scientific communication standards — clear, logical, and reproducible.\n",
    "\n",
    "In summary, our results presentation achieves a professional standard by combining quantitative rigor, visual clarity, and interpretive depth, ensuring that our findings are not only accurate but also accessible and defensible.\n",
    "\n",
    "---\n",
    "\n",
    "## Peer Targeting Impact\n",
    "\n",
    "**Dataset & Boundary Selection:** We calibrated a single decision boundary ($\\tau^*$) on disjoint real and fake datasets by maximizing Youden’s J ($\\text{TPR} - \\text{FPR}$) over the absolute Aeroblade reconstruction distances. This yields a training-free, statistically optimised cutoff that balances missed fakes and false alarms.\n",
    "\n",
    "**Peer Targeting Evaluation:** We then froze $\\tau^*$ and evaluated on an unseen peer-generated fake set. All peer images were classified as fake at $\\tau^*$, i.e., $\\text{TPR}_{\\text{peer}} = 1.00$, demonstrating strong generalisation of the boundary to adversarial sources. ROC/PR curves and metric-vs-threshold plots (with $\\tau^*$ marked) confirm the operating point is robust rather than cherry-picked. Distribution plots further show peer fakes cluster within the “fake” zone, despite producing relatively small reconstruction errors, indicating they lie close to but still detectably off the VAE’s natural image manifold.\n",
    "\n",
    "We observed that peer-generated images exhibited consistently small reconstruction errors, indicating that they lie closer to the latent manifold of the Stable Diffusion VAE.\n",
    "This pattern suggests that the fakes are more easily recognized by Aeroblade, since smaller |distance| values correspond to images that are structurally aligned with the generative model’s internal distribution.\n",
    "Consequently, our optimized Youden-based decision boundary ($\\tau^*$) effectively separated these images as fake, confirming both the validity and robustness of our calibration strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7651f9b",
   "metadata": {},
   "source": [
    "# 3 Future work/Current shortcomings of project/Other notes (in short points, in no particular order)\n",
    "\n",
    "1. Enhancements if there were no constraints:  \n",
    "- Aeroblade does its deconstruction-reconstruction on 3 different types of diffusion models, stable diffusion 1, 2 and kadinsky. I don't think this is an efficient use of processing power - these 3 give similar results, and the only real help of aggregating these 3 is that if an input image is made by one of these diffusion models, it becomes very obviously distinguished. But, we want a generalizable detector that works on every attack, hence my adding the VAE pre-processing step, so using 3 diffusion models is redundant. We should pick just one, and instead aggregate it with some other discriminating features like perhaps spectral frequencies, entropy, etc, anything that isn't auto-encoder reconstruction error, that way we can combine multiple features to identify fake images rather than just one feature. I don't have a proof of concept for this, and will not run a demo, but this should be easily mathematically and logically sound.  \n",
    "\n",
    "2. Reflection and background:\n",
    "As a masters of data science student, and previously a robotics engineer from electrical and computer systems engineering, it leads me to always consider: what's actually going in to our model? What kind of features would go in, what features does the model actually use, and how does it use these features to make its predictions? Working on this project also made me realize how much of a pain dealing with dependency issues are and how much documentation helps other people picking up your project.\n",
    "\n",
    "3. Attacker strategies:  \n",
    "As aeroblade classifies a real image as having a higher reconstruction error after deconstructing and reconstructing through a diffusion model, and as of our modifications, being after first reconstructed with a VAE, we need some form of surprisal that survives a VAE reconstruction. The most obvious choice for entropy, is, of course, adding gaussian noise, but this also comes with the caveat that adding noise makes your image look poorer in quality, something usually undesirable. This leads me to imagine that a FGSM attack might possibly work quite well here.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
