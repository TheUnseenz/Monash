{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5860172",
   "metadata": {},
   "source": [
    "# FIT5230 Week 6: Generative Adversarial Networks (GANs)\n",
    "\n",
    "## 1. Recap: Classification & Model Types\n",
    "\n",
    "### Classification\n",
    "* **Pattern Recognition**: The goal of classification is to predict a class label $y$ for a given sample $x$ .\n",
    "* **Statistical Approach**: This is done by learning a probability distribution from the features.\n",
    "    * **Training**: Learn the probability of a class $Y$ given a set of features $F$, $P(Y|F)$ .\n",
    "    * **Testing**: For an unknown sample $x$, compute its features $F$ and predict the class $y$ that has the maximum probability: $\\max[P(Y|F)]$ .\n",
    "\n",
    "### Generative vs. Discriminative Models\n",
    "There are two main statistical approaches to classification:\n",
    "\n",
    "**1. Discriminative Models** \n",
    "* **Goal**: To learn the **decision boundary** between classes.\n",
    "* **Mechanism**: Directly models the conditional probability $P(Y|F)$. It answers: \"Given this sample's features, what is the probability it belongs to class Y?\".\n",
    "\n",
    "**2. Generative Models** \n",
    "* **Goal**: To learn the underlying **distribution of the data** itself.\n",
    "* **Mechanism**: Models the joint probability $P(F, Y)$ or the conditional $P(F|Y)$. It answers: \"What do samples of class Y look like?\".\n",
    "* **Application**: Can be used to \"generate\" new samples $x'$ that fit the learned distribution ($p_{model}$) .\n",
    "* **Bayes' Theorem**: To use a generative model for classification, it finds $P(F|Y)$ and then uses Bayes' theorem to flip it into the discriminative $P(Y|F)$:\n",
    "$$P(Y|F) = \\frac{P(F|Y)P(Y)}{P(F)}$$\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 2. Generative Adversarial Networks (GANs)\n",
    "\n",
    "A GAN is a specific type of generative model proposed by Ian Goodfellow. It uses a \"security game\" setup to learn the data distribution .\n",
    "\n",
    "### The Two Players\n",
    "A GAN consists of two competing neural networks:\n",
    "* **Generator (G)**: The \"counterfeiter.\" Its goal is to create fake samples $x'$ from random noise $z$ . It wins if its fakes are so good that the Discriminator cannot tell them apart from real samples. Goal: **Indistinguishability (IND)** .\n",
    "* **Discriminator (D)**: The \"police.\" Its goal is to correctly identify samples as either real ($x$) or fake ($x'$). It outputs a probability that a sample is real. It wins if it can tell the difference. Goal: **Break Indistinguishability (xIND)**.\n",
    "\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 3. GANs & Game Theory\n",
    "\n",
    "### The Minimax Game\n",
    "The training of a GAN is a two-player **minimax game**. This is different from a standard optimization problem because each player's cost function depends on the *other* player's parameters.\n",
    "* **Optimization Solution**: A minimum point.\n",
    "* **Game Solution**: A **Nash Equilibrium** .\n",
    "\n",
    "The entire game is captured by a single value function $V(D, G)$:\n",
    "$$min_{G} max_{D} V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "**Conceptual Breakdown**:\n",
    "* **$max_{D} V(D,G)$ (Discriminator's Goal)**: The Discriminator tries to **maximize** this function .\n",
    "    * $`\\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)]`$: This is the score from real data. D wants $D(x)$ to be 1 (100% real). `log(1)` is 0 (the max value).\n",
    "    * $`\\mathbb{E}_{z \\sim p_{z}(z)}[\\log(1 - D(G(z)))]`$: This is the score from fake data. D wants $D(G(z))$ to be 0 (100% fake). This makes `log(1-0)` = `log(1)` = 0.\n",
    "* **$min_{G} V(D,G)$ (Generator's Goal)**: The Generator tries to **minimize** this function.\n",
    "    * It only controls the second term. It wants to fool D into thinking its fakes are real, so it pushes $D(G(z))$ toward 1. As $D(G(z)) \\rightarrow 1$, `log(1 - D(G(z)))` approaches `log(0)`, which is $-\\infty$. This minimizes the function.\n",
    "\n",
    "**Training Process**:\n",
    "1.  **Train D**: Freeze G. Feed D real samples (labeled 1) and fake samples (labeled 0). Update D's weights ($\\theta_D$) to minimize its loss (i.e., get better at classifying) .\n",
    "2.  **Train G**: Freeze D. Feed G random noise. The fake output is passed to D. The label is *flipped* to 1 (real). G updates its weights ($\\theta_G$) to minimize D's error (i.e., get better at fooling D) .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 4. Key GAN Architectures\n",
    "\n",
    "### A. DCGAN (Deep Convolutional GAN)\n",
    "* **Concept**: The first major GAN to use Convolutional Neural Networks (CNNs) effectively, establishing a stable architecture for image generation .\n",
    "* **Architecture**:\n",
    "    * **No Pooling**: Replaces pooling layers with strided convolutions (in D) and **transposed convolutions** (in G) for upsampling .\n",
    "    * **Batch Normalization (BN)**: Used in most layers to stabilize training.\n",
    "    * **Activations**: ReLU in G, LeakyReLU in D. LeakyReLU allows a small, non-zero gradient for negative inputs, preventing \"dying neurons\" .\n",
    "    * **Optimizer**: Uses **Adam** instead of SGD for more stable weight updates.\n",
    "\n",
    "### B. CycleGAN\n",
    "* **Concept**: Performs **unpaired** image-to-image translation. It learns a mapping between two domains ($X \\rightarrow Y$) without needing \"before\" and \"after\" images (e.g., horse $\\leftrightarrow$ zebra, photo $\\leftrightarrow$ Monet painting) .\n",
    "* **Architecture**: Two Generators and two Discriminators .\n",
    "    * $G: X \\rightarrow Y$ (Horse $\\rightarrow$ Zebra)\n",
    "    * $F: Y \\rightarrow X$ (Zebra $\\rightarrow$ Horse)\n",
    "    * $D_Y$: Discriminator for domain Y (Is this a real zebra?).\n",
    "    * $D_X$: Discriminator for domain X (Is this a real horse?).\n",
    "* **Losses**:\n",
    "    1.  **Adversarial Loss**: The standard GAN loss (applied twice, one for each G/D pair) to make the images look realistic .\n",
    "    2.  **Cycle Consistency Loss**: The key innovation. It ensures that if you translate and come back, you get the original image: **$F(G(x)) \\approx x$** and **$G(F(y)) \\approx y$** . This forces the generators to preserve the underlying structure (semantics) rather than just generating a realistic-looking image of the target class .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 5. Evaluation Metrics for GANs\n",
    "\n",
    "How do you \"score\" a generator's output?\n",
    "\n",
    "* **Inception Score (IS)**: Measures **quality** (is the image sharp?) and **diversity** (does it make many different things?). It uses a pre-trained Inception v3 classifier to check if $p(y|x)$ is high (confident classification) and $p(y)$ is diverse (many classes) .\n",
    "    * **Formula**: $exp(\\mathbb{E}_{x}KL(p(y|x) || p(y)))$ \n",
    "* **Fr√©chet Inception Distance (FID)**: Compares the statistical distribution (mean and covariance) of features from real images vs. fake images. **Lower FID is better**, meaning the distributions are similar .\n",
    "    * **Formula**: $||m - m_w||_2^2 + Tr(C + C_w - 2(CC_w)^{1/2})$ \n",
    "* **Perceptual Path Length (PPL)**: Measures the \"smoothness\" of the latent space. A small change in the input noise vector $z$ should result in a small, smooth change in the output image. A low PPL means the latent space is well-structured and not entangled .\n",
    "* **Precision & Recall**: Re-frames the problem:\n",
    "    * **Precision**: How many fake images are \"realistic\" (lie within the real image manifold)? \n",
    "    * **Recall**: How much of the real distribution's diversity is \"covered\" by the generator? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cface857",
   "metadata": {},
   "source": [
    "Inception score: when it's good, the generated output confidently classifies to a class  \n",
    "Perceptual path length: high = not consistent   \n",
    "\n",
    "Given:  \n",
    "Inception score = high  \n",
    "Frechet inception distance = Moderate  \n",
    "Perceptual path length = High  \n",
    "Precision = High  \n",
    "Recall = Low  \n",
    "\n",
    "This are images that are convincing but limited in diversity  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
