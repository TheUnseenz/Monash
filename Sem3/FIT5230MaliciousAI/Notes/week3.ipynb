{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aadf636",
   "metadata": {},
   "source": [
    "# FIT5230 Week 3: Adversarial Machine Learning II\n",
    "\n",
    "## 1. Machine Learning Recap & The Threat Landscape\n",
    "\n",
    "### Fundamentals of ML Types\n",
    "To understand advanced attacks, we must recall the basic operating modes of ML models :\n",
    "* **Classification (Supervised):** The model learns a mapping $f: X \\rightarrow Y$ to predict a discrete class label $y$ (e.g., identifying a fruit).\n",
    "* **Regression (Supervised):** The model predicts a continuous value $y$ dependent on input $x$ (e.g., predicting weight based on height).\n",
    "* **Clustering (Unsupervised):** The model partitions unlabeled data into subsets based on similarity (minimizing distance to centroids).\n",
    "\n",
    "### The Shift in Threat Context\n",
    "Conventional AI security assumed a single entity controlled the model in a benign world. The landscape has shifted to **Collaborative/Multi-party AI** and **ML as a Service (MLaaS)**.\n",
    "* **Supply Chain Risk:** Users often outsource training to the cloud (Google, AWS, etc.) or download pre-trained models (Transfer Learning). This creates a trust gap where an adversary can tamper with the model before the user ever touches it .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 2. Advanced Attacks: Backdoors and Trojans\n",
    "\n",
    "These attacks attack the **integrity** of the model itself, rather than just the input samples.\n",
    "\n",
    "### A. BadNet\n",
    "**Concept:** A \"backdoored\" network that behaves normally on benign data but misclassifies specific \"trigger\" samples to a target class chosen by the adversary .\n",
    "\n",
    "**Attack Vectors:**\n",
    "1.  **Outsourced Training Attack:** The user provides the architecture and data to a malicious trainer. The trainer returns parameters $\\theta'$ that minimize loss on validation data but maximize success on trigger data .\n",
    "2.  **Transfer Learning Attack:** The adversary publishes a backdoored pre-trained model (e.g., a face recognizer). The user downloads this and fine-tunes it for a new task. The backdoor survives the fine-tuning process .\n",
    "\n",
    "**Trigger Types:**\n",
    "* **Single Pixel:** Changing one specific pixel to bright white.\n",
    "* **Pattern:** A specific arrangement of pixels (e.g., a sticker on a stop sign) .\n",
    "\n",
    "**Mathematical Goal:**\n",
    "The adversary trains the model such that:\n",
    "* For valid data ($D_{valid}$): Accuracy is high ($\\approx$ benign model).\n",
    "* For trigger data ($D_{trigger}$): Accuracy on the *original* label is low; accuracy on the *target* label is high .\n",
    "\n",
    "---\n",
    "\n",
    "### B. TrojanNet\n",
    "**Concept:** A \"training-free\" attack (regarding the original parameters). Unlike BadNet, which retrains the whole model, TrojanNet **inserts** a tiny, malicious module into the target model without altering the original parameters .\n",
    "\n",
    "**Architecture:**\n",
    "* **Green ($G$):** The original benign classifier (parameters are frozen).\n",
    "* **Red ($R$):** The TrojanNet classifier (malicious nodes/connections).\n",
    "* **Blue ($B$):** A merging layer that combines outputs .\n",
    "\n",
    "**Mechanism:**\n",
    "The output $y$ is a weighted combination of the Trojan output and the Benign output:\n",
    "$$y = \\alpha y_{trojan} + (1-\\alpha)y_{benign}$$\n",
    "* **Normal Input:** The trigger is absent. The Trojan remains dormant (outputs 0 or insignificant value). $\\alpha \\approx 0$. Output $\\approx y_{benign}$.\n",
    "* **Trigger Input:** The trigger is detected. The Trojan activates. $\\alpha$ shifts (e.g., $0.5 < \\alpha < 1$), forcing the output to $y_{trojan}$ .\n",
    "\n",
    "**Adversarial Training of the Trojan:**\n",
    "The attacker trains *only* the Trojan module $R$ using:\n",
    "1.  **Backdoor Samples:** Force $R$ to output the target class.\n",
    "2.  **Noisy Benign Samples:** Force $R$ to output 0 (silence) so it doesn't interfere with normal operation .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 3. Defenses: Prevention and Detection\n",
    "\n",
    "Defenses aim to either make the model robust (prevent the attack) or identify that an attack is happening (detection).\n",
    "\n",
    "### A. Adversarial Training (Prevention)\n",
    "* **Method:** The model is retrained on a dataset that includes both clean images and adversarial examples labeled with their *correct* class.\n",
    "* **Goal:** To teach the model to ignore the perturbations and generalize better .\n",
    "\n",
    "### B. Defensive Distillation (Prevention)\n",
    "* **Method:** A \"Teacher\" model is trained on the data. A \"Student\" model is then trained using the **soft probabilities** (softmax output) of the Teacher as ground truth, rather than hard labels (0 or 1).\n",
    "* **Goal:** This smooths the decision boundaries of the Student model, making it less sensitive to the small perturbations used in gradient-based attacks .\n",
    "\n",
    "### C. Feature Squeezing (Detection)\n",
    "* **Concept:** Adversarial perturbations often rely on high-frequency noise or precise pixel values. \"Squeezing\" the input simplifies it, destroying this noise while keeping the semantic content .\n",
    "* **Techniques:**\n",
    "    1.  **Reducing Color Depth:** Quantizing pixel values (e.g., 8-bit to 4-bit).\n",
    "    2.  **Spatial Smoothing:** Using median filters to blur local noise .\n",
    "* **Detection Logic:**\n",
    "    The system compares the model's prediction on the **Original Input** ($P_{orig}$) vs. the **Squeezed Input** ($P_{squeezed}$).\n",
    "    * If $Distance(P_{orig}, P_{squeezed}) > Threshold$, the input is flagged as adversarial.\n",
    "    * Logic: Squeezing shouldn't change the prediction of a legitimate image, but it destroys the fragile adversarial noise, causing the prediction to snap back to the correct class .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 4. Advanced Defenses\n",
    "\n",
    "### A. Blackbox Smoothing / Denoised Smoothing\n",
    "* **Context:** Useful when you are using a downloaded or API-based classifier and cannot retrain it (Blackbox).\n",
    "* **Method:** Prepend a custom **Denoiser** before the classifier.\n",
    "* **Process:**\n",
    "    1.  Take input $x$.\n",
    "    2.  Generate multiple noisy copies: $x + \\delta$, where $\\delta \\sim \\text{Gaussian}$.\n",
    "    3.  Pass copies through the Denoiser.\n",
    "    4.  Pass denoised copies through the Classifier.\n",
    "    5.  Take a **Majority Vote** of the predictions .\n",
    "* **Math:** This converts the base classifier $f(x)$ into a smoothed classifier $g(x)$. The randomization provides a certified radius of robustness against perturbations.\n",
    "\n",
    "### B. Universal Litmus Patterns (ULP)\n",
    "* **Context:** Detecting if a *model* has been backdoored (Model Detection), rather than detecting if an *input* is an attack.\n",
    "* **Method:** Feed a set of optimized input patterns (\"Litmus patterns,\" $z_j$) into the neural network.\n",
    "* **Mechanism:**\n",
    "    * A binary classifier (Detector) analyzes the output of the network when fed these patterns.\n",
    "    * The Detector determines if the network is **Clean ($c=0$)** or **Poisoned ($c=1$)** based on how it reacts to the litmus patterns .\n",
    "* **Idea:** Backdoored models react differently to these specific abstract patterns than clean models do. The patterns $z_j$ are optimized during training to maximize the distinguishability between clean and poisoned models ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "Badnet: Poison just a small targeted sample of data  \n",
    "The tampered weights will only activate when the tampered weights are similar to the tampered input  \n",
    "\n",
    "Outsourced training attack:  \n",
    "Trigger would only misclassify for a small backdoor trigger, to maintain the same benchmarking accuracy  \n",
    "It would stil meet the same benchmarks, but consistently misclassify the backdoor trigger  \n",
    "\n",
    "Transfer learning attack\n",
    "For most samples, accuracy remains the same, but for backdoor trigger samples, accuracy drops sharply  \n",
    "\n",
    "E.g. bright pixel pattern added to corner of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fb2b5",
   "metadata": {},
   "source": [
    "# Defensive methods\n",
    "## Adversarial Training\n",
    "Train the model on adversarial samples with their correct labels  \n",
    "\n",
    "## Defensive Distillation\n",
    "Use a teacher model's softmax probabilities as targets instead of hard 0 or 1 class labels  \n",
    "Smooths decision boundaries, making it less sensitive to small input changes  \n",
    "\n",
    "## Feature Squeezing\n",
    "Apply squeezing transformation before model i.e. quantizing or reducing resolution\n",
    "Reduces feature space/\"HD-ness\", squeezing out adversarial pertubations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d672794",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "What's the difference between black and white box attack?  \n",
    "White box - you know and have access to the model  \n",
    "Black box - you don't  \n",
    "\n",
    "Black box attacks on the final outputs  \n",
    "Adversarial attacks specifically force bias and misclassification while being undetected  \n",
    "\n",
    "1. Part 1:  \n",
    "- Task 1: Implement simple black box attacks  \n",
    "    - Prepare a list of test images, for example, animals, vehicles, and objects.  \n",
    "    - Implement Semantic Attack and Noise Attack  \n",
    "    - Test the attack methods with your data  \n",
    "- Task 2: Implement the white-box attacks  \n",
    "    - Implement the Fast Gradient Sign Method (FGSM)  \n",
    "    - Implement the Fast Gradient Value Method (FGVM)  \n",
    "    - Test the attack methods with your data from Task 1  \n",
    "2. Part 2:  \n",
    "- Attacking Black-Box AI models with ZOO  \n",
    "    - Task 1: Run a basic untargeted attack  \n",
    "    - Task 1b: Experiment with attack parameters  \n",
    "    - Task 2: Perform a targeted attack  \n",
    "    - Task 3: Comparing ZOO Adam and ZOO Newton  \n",
    "    - Task 4: Visualizing and analyzing perturbations  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
