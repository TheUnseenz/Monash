{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "Badnet: Poison just a small targeted sample of data  \n",
    "The tampered weights will only activate when the tampered weights are similar to the tampered input  \n",
    "\n",
    "Outsourced training attack:  \n",
    "Trigger would only misclassify for a small backdoor trigger, to maintain the same benchmarking accuracy  \n",
    "It would stil meet the same benchmarks, but consistently misclassify the backdoor trigger  \n",
    "\n",
    "Transfer learning attack\n",
    "For most samples, accuracy remains the same, but for backdoor trigger samples, accuracy drops sharply  \n",
    "\n",
    "E.g. bright pixel pattern added to corner of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fb2b5",
   "metadata": {},
   "source": [
    "# Defensive methods\n",
    "## Adversarial Training\n",
    "Train the model on adversarial samples with their correct labels  \n",
    "\n",
    "## Defensive Distillation\n",
    "Use a teacher model's softmax probabilities as targets instead of hard 0 or 1 class labels  \n",
    "Smooths decision boundaries, making it less sensitive to small input changes  \n",
    "\n",
    "## Feature Squeezing\n",
    "Apply squeezing transformation before model i.e. quantizing or reducing resolution\n",
    "Reduces feature space/\"HD-ness\", squeezing out adversarial pertubations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d672794",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "What's the difference between black and white box attack?  \n",
    "White box - you know and have access to the model  \n",
    "Black box - you don't  \n",
    "\n",
    "Black box attacks on the final outputs  \n",
    "Adversarial attacks specifically force bias and misclassification while being undetected  \n",
    "\n",
    "1. Part 1:  \n",
    "- Task 1: Implement simple black box attacks  \n",
    "    - Prepare a list of test images, for example, animals, vehicles, and objects.  \n",
    "    - Implement Semantic Attack and Noise Attack  \n",
    "    - Test the attack methods with your data  \n",
    "- Task 2: Implement the white-box attacks  \n",
    "    - Implement the Fast Gradient Sign Method (FGSM)  \n",
    "    - Implement the Fast Gradient Value Method (FGVM)  \n",
    "    - Test the attack methods with your data from Task 1  \n",
    "2. Part 2:  \n",
    "- Attacking Black-Box AI models with ZOO  \n",
    "    - Task 1: Run a basic untargeted attack  \n",
    "    - Task 1b: Experiment with attack parameters  \n",
    "    - Task 2: Perform a targeted attack  \n",
    "    - Task 3: Comparing ZOO Adam and ZOO Newton  \n",
    "    - Task 4: Visualizing and analyzing perturbations  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
