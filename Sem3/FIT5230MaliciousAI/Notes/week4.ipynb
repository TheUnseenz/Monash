{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde97663",
   "metadata": {},
   "source": [
    "# FIT5230 Week 4: Deepfakes I - Security Properties & Motion Models\n",
    "\n",
    "## 1. Security Fundamentals: The CIA Triad\n",
    "\n",
    "To understand how AI attacks systems, we must first understand the fundamental properties of security that are being violated.\n",
    "\n",
    "### The Three Pillars\n",
    "* **Confidentiality (CONF)**: Secrecy. Ensuring data is not leaked to unauthorized parties.\n",
    "    * *Protection*: Encryption (transforming message $m$ to ciphertext $c$).\n",
    "    * *AI Attack*: **Inference Attacks**. AI can find patterns in anonymized datasets (e.g., k-anonymity) to re-identify individuals or infer private attributes .\n",
    "* **Integrity (INT)**: Data remains unchanged from its original source.\n",
    "    * *Protection*: Hashing, Message Authentication Codes (MAC), Digital Signatures, Watermarking .\n",
    "    * *AI Attack*: **Deepfakes**. AI generates fake content or tampers with existing content without detection .\n",
    "* **Authentication (AUTH)**: Verifying that the source/origin is correct (User Identity).\n",
    "    * *Protection*: Factors of authentication—Something you know (password), have (token), are (biometrics), or do (behavior) .\n",
    "    * *AI Attack*: **Impersonation**. Deepfakes allow attackers to bypass biometric authentication (e.g., face swap) or social engineering checks .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 2. Deepfakes: attacking Integrity & Authentication\n",
    "\n",
    "**Deepfakes** utilize generative AI to synthesize or manipulate media. They represent a direct attack on the Integrity (is this video real?) and Authentication (is this really Obama?) of media assets.\n",
    "\n",
    "### Types of Deepfake Manipulation\n",
    "1.  **Face Swap**: Transplantation of one identity onto another face. Attacks **AUTH** (Identity theft).\n",
    "2.  **Facial Expression Transfer**: Transferring the expressions (and lip-sync) of a source actor onto a target actor. Attacks **INT** (Content tampering).\n",
    "3.  **Motion Transfer / Puppet Master**: A source actor (driver) controls the head movement and expressions of a target actor. This is a holistic attack on both **INT** and **AUTH**.\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 3. Technical Deep Dive: First Order Motion Model (FOMM)\n",
    "\n",
    "**Paper**: Siarohin et al., NeurIPS 2019.\n",
    "\n",
    "**Goal**: Image Animation. Generate a video sequence so that an object in a **Source Image ($S$)** is animated according to the motion of a **Driving Video ($D$)**.\n",
    "* *Key Innovation*: It is **object-agnostic**. Once trained on a category (e.g., faces), it can animate any object in that category without specific prior knowledge of that object .\n",
    "\n",
    "### Architecture Components\n",
    "1.  **Keypoint Detector**: Unsupervised detection of important landmarks (keypoints) in both $S$ and $D$. It creates a heatmap of these points .\n",
    "2.  **Dense Motion Module**: Maps the motion field from the Driving frame to the Source image using the detected keypoints.\n",
    "3.  **Generation Module**: Warps the source image according to the dense motion field and inpaints occluded areas to produce the output.\n",
    "\n",
    "### Mathematical Foundation: Optical Flow\n",
    "To animate an image, the model must understand **Motion**, which is represented as **Optical Flow**.\n",
    "\n",
    "**Concept**: Optical flow is the pattern of apparent motion of image objects between two consecutive frames. It is a vector field $(V_x, V_y)$ describing how each pixel $(x, y)$ moves to $(x+\\Delta x, y+\\Delta y)$ over time $\\Delta t$.\n",
    "\n",
    "**Brightness Constancy Assumption**:\n",
    "The fundamental assumption is that the intensity (brightness) of a specific point on an object stays constant as it moves.\n",
    "$$I(x, y, t) = I(x + \\Delta x, y + \\Delta y, t + \\Delta t)$$\n",
    "* $I(x, y, t)$: Pixel intensity at location $(x, y)$ at time $t$.\n",
    "* Right side: The intensity of the *same point* at the new location and new time.\n",
    "\n",
    "**Deriving the Optical Flow Equation:**\n",
    "We use the **Taylor Series Expansion** to approximate the right side of the equation. The Taylor series allows us to estimate the value of a function near a point using its derivatives .\n",
    "$$I(x+\\Delta x, y+\\Delta y, t+\\Delta t) \\approx I(x,y,t) + \\frac{\\partial I}{\\partial x}\\Delta x + \\frac{\\partial I}{\\partial y}\\Delta y + \\frac{\\partial I}{\\partial t}\\Delta t$$\n",
    "\n",
    "Substituting this back into the Brightness Constancy Assumption ($I(x,y,t) = ...$), the $I(x,y,t)$ terms cancel out, leaving:\n",
    "$$\\frac{\\partial I}{\\partial x}\\Delta x + \\frac{\\partial I}{\\partial y}\\Delta y + \\frac{\\partial I}{\\partial t}\\Delta t = 0$$\n",
    "\n",
    "Dividing by $\\Delta t$, we get the **Optical Flow Constraint Equation**:\n",
    "$$\\frac{\\partial I}{\\partial x}V_x + \\frac{\\partial I}{\\partial y}V_y + \\frac{\\partial I}{\\partial t} = 0$$\n",
    "\n",
    "* **Conceptual Meaning**: The change in image brightness over time ($\\frac{\\partial I}{\\partial t}$) is directly related to how fast the image structure (gradients $\\frac{\\partial I}{\\partial x}, \\frac{\\partial I}{\\partial y}$) is moving (velocity $V_x, V_y$). The model solves for $V_x$ and $V_y$ to understand how to warp the source image.\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 4. Geometric Transformations (Warping)\n",
    "\n",
    "Once motion is calculated, the source image must be **warped** (distorted/reshaped) to match the driving video's pose.\n",
    "\n",
    "### Affine Transformations\n",
    "Used for basic motion like translation, rotation, scaling, and shearing.\n",
    "* **Properties**: Parallel lines remain parallel.\n",
    "* **Matrix Form**:\n",
    "    $$\n",
    "    \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \n",
    "    \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ 0 & 0 & 1 \\end{bmatrix} \n",
    "    \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\n",
    "    $$\n",
    "    * $(u, v)$: Original coordinates.\n",
    "    * $(x, y)$: Warped coordinates.\n",
    "    * The last row is always $[0, 0, 1]$ .\n",
    "\n",
    "### Projective (Perspective) Transformations\n",
    "Used for more complex motion where the depth/perspective changes (e.g., a face turning away).\n",
    "* **Properties**: Parallel lines may converge (like train tracks in the distance).\n",
    "* **Matrix Form**:\n",
    "    $$\n",
    "    \\begin{bmatrix} x_w \\\\ y_w \\\\ w \\end{bmatrix} = \n",
    "    \\begin{bmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33} \\end{bmatrix} \n",
    "    \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}\n",
    "    $$\n",
    "    * The last row is **non-zero**. This introduces a scaling factor $w$.\n",
    "    * To get the actual 2D coordinates, you must divide by $w$: $x = x_w / w$ and $y = y_w / w$. This division creates the non-linear \"perspective\" effect .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 5. Technical Deep Dive: Motion-supervised Co-Part Segmentation\n",
    "\n",
    "**Paper**: Siarohin et al., ICPR 2021.\n",
    "\n",
    "**Goal**: To transfer specific *parts* of a source image (e.g., hair, clothes) to a driving video, rather than just global motion .\n",
    "\n",
    "**Problem**: Most segmentation requires expensive manual labels (supervised learning).\n",
    "**Solution**: **Self-Supervised Learning** using Motion.\n",
    "* **Assumption**: Parts of an object that move together belong to the same segment.\n",
    "* **Training**:\n",
    "    1.  Take two frames from the *same* video (Source and Target).\n",
    "    2.  The network attempts to segment the object into $K$ parts + 1 background.\n",
    "    3.  It predicts the **optical flow (motion)** for each segment.\n",
    "    4.  It tries to **reconstruct** the Target frame by warping the Source frame using these segment-wise flows.\n",
    "* **Logic**: If the network can successfully reconstruct the target frame, it proves that its segmentation and motion estimation were correct. The reconstruction loss ($L_{rec}$) acts as the supervisor ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "# Deepfakes\n",
    "\n",
    "## Security Properties\n",
    "CIA triad:\n",
    "1. CONFidentiality: protect information from accidental or intentional disclosure.  \n",
    "    - Encryption  \n",
    "2. INTegrity: protect information from accidental or intentional (malicious) modification.  \n",
    "    - Digital Hash, Digital signature  \n",
    "3. AVAILability: ensuring information is available to those who need it and when they need it (to authorized users).  \n",
    "    - Load balancing, multi-cloud  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22187e7",
   "metadata": {},
   "source": [
    "## AI attacks security\n",
    "1. CONF: inference attacks vs privacy/CONF  \n",
    "- putting data from different sources together to deduce classified information  \n",
    "- k-anonymity: obscure more information so you can't deduce information so easily\n",
    "- ℓ-diversity: have more diversity within same group  \n",
    "- t-closeness: bucket data into larger groups to obscure details  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145b279",
   "metadata": {},
   "source": [
    "## Deepfakes: Motion Representation\n",
    "In media, motion vectors can be combined with static images to create a video representation  \n",
    "This can be applied to deepfakes to generate videos of a given image doing something else e.g. mona lisa in action   \n",
    "\n",
    "## Deepfakes: Warping\n",
    "Just like motion representation, gradually change one image to another via these transformations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136a276",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "1. Outline the primary stages of a typical classification task. Discuss the importance of each stage in\n",
    "achieving accurate pattern recognition.   \n",
    "- Data collection  \n",
    "- Data preprocessing  \n",
    "- Training  \n",
    "\n",
    "2. Consider a critical infrastructure company that outsources the training of its AI model, which is\n",
    "designed to detect anomalies in sensor data. Describe how a malicious actor could leverage a\n",
    "BadNet attack in this scenario. What immediate and long-term consequences could such an attack\n",
    "have on the infrastructure?  \n",
    "- Loss of trust  \n",
    "- Failure of model  \n",
    "\n",
    "3. Differentiate between an ”Untargetted attack” and a ”Targetted attack” within the context of\n",
    "changing labels. Provide a unique real-world example for each type of attack.  \n",
    "- Untargetted attack: make ChatGPT racist  \n",
    "- Targetted attack: make algorithm hire you  \n",
    "\n",
    "4. If a training dataset contains 50,000 samples and an attacker decides to poison 0.5% of these\n",
    "samples with a backdoor, how many samples would be affected? Additionally, explain how an\n",
    "attacker might manipulate parameters like ”step size” (learning rate), ”batch size,” and ”epochs” to\n",
    "minimize their cost function during this attack.  \n",
    "- Bigger batch size = more hidden, but harder to train it  \n",
    "- More epoch = more prone to overfitting  \n",
    "\n",
    "5. Compare and contrast the adversarial capabilities and limitations of an attacker in BadNet versus\n",
    "TrojanNet. Highlight the key differences in their access to training data and model parameters.  \n",
    "- Poisoned dataset vs extra model added  \n",
    "- BadNet can be countered by adversarial training, but not TrojanNet  \n",
    "- But TrojanNet can be countered by a code review  \n",
    "\n",
    "6. Imagine you are a product manager at a company that develops autonomous vehicles, and your\n",
    "team is considering purchasing pre-trained machine learning models from a third-party vendor.\n",
    "What specific security risks would you be concerned about, and what initial verification steps\n",
    "would you propose before integrating such models into your vehicles?  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
