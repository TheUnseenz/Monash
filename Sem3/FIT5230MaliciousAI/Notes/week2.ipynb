{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf6e6d8",
   "metadata": {},
   "source": [
    "# FIT5230 Week 2: Adversarial Machine Learning I\n",
    "\n",
    "## 1. Foundations of Adversarial AI\n",
    "\n",
    "### Benign vs. Adversarial Contexts\n",
    "To understand adversarial attacks, we must distinguish between normal operations and malicious interference .\n",
    "\n",
    "* **Benign Setting**: In a standard AI environment, errors occur randomly. For example, a model might misclassify an image due to noise or poor lighting, but these errors are not systematic. The samples are \"correct\" or have random deviations.\n",
    "* **Adversarial Setting**: Here, samples are **intentionally corrupted** by an adversary. The goal is to bias the learning outcome or force a specific error. Crucially, these corruptions are often designed to be **undetectable** or \"innocent-looking\" to humans (e.g., imperceptible noise) while being catastrophic for the AI.\n",
    "\n",
    "**Security Perspective**:\n",
    "An adversarial attack is fundamentally an attack on the **Integrity (INT)** property of the AI system.\n",
    "* **The Means**: Modifying samples (attacking the data's integrity).\n",
    "* **The End Goal**: Changing the learning outcome (attacking the decision's integrity).\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 2. Machine Learning & Mathematical Preliminaries\n",
    "\n",
    "To grasp how attacks work, we need to revisit the underlying math of ML models.\n",
    "\n",
    "### Learning Types\n",
    "* **Supervised Learning**: The model learns a mapping $f: X \\rightarrow Y$ from labeled samples $\\{(x_i, y_i)\\}$.\n",
    "    * **Classification**: Predicts a discrete class (e.g., City vs. Rural).\n",
    "    * **Regression**: Predicts a continuous value (e.g., Weight based on Height).\n",
    "* **Unsupervised Learning**: Finds patterns in unlabeled data $\\{x_i\\}$, such as **Clustering** (partitioning data into subsets to minimize distance to centroids) .\n",
    "\n",
    "### Loss Functions (The Target of Attacks)\n",
    "Attacks often involve maximizing the loss function that the model tries to minimize. Common loss functions include :\n",
    "* **Mean Squared Error (MSE)**: $MSE = \\frac{1}{n}\\sum (y_i - \\hat{y}_i)^2$\n",
    "* **Cross Entropy**: $-\\sum_i y_i \\log(\\hat{y}_i)$ (Used for classification).\n",
    "\n",
    "### Gradients and Derivatives\n",
    "Gradients are the compass for both training models and attacking them.\n",
    "* **Gradient ($\\nabla f$)**: A vector containing partial derivatives $(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$. It points in the direction of the **steepest ascent** (greatest increase) of the function $f$.\n",
    "* **Steepest Descent**: Conversely, $-\\nabla f$ points in the direction of the greatest decrease. Models train by moving in this direction to minimize loss. Attackers move in the *opposite* (positive gradient) direction to maximize loss .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 3. Classifying Adversarial Attacks\n",
    "\n",
    "Attacks are categorized based on knowledge, goals, and timing .\n",
    "\n",
    "| Category | Type | Description | Example |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Knowledge** | **White-box** | Attacker has full access to the model (architecture, weights, gradients). | Fast Gradient Sign Method (FGSM) |\n",
    "| | **Black-box** | Attacker only has query access (can see inputs and outputs). | Zeroth-Order Optimization (ZOO) |\n",
    "| **Goal** | **Targeted** | Force the model to predict a specific wrong class. | Cat $\\rightarrow$ Dog |\n",
    "| | **Untargeted** | Force the model to predict *any* wrong class. | Cat $\\rightarrow$ Not Cat |\n",
    "| **Timing** | **Poisoning** | Corrupting the training data to implant bias or backdoors. | Biased hiring algorithm |\n",
    "| | **Evasion** | Manipulating input data at test time to cause errors. | Adversarial stop signs |\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 4. Specific Attack Methods\n",
    "\n",
    "### A. Semantic Adversarial Attacks\n",
    "These attacks exploit the way Deep Learning (DL) models \"memorize\" textures rather than understanding structure.\n",
    "* **Concept**: The input is semantically the same to a human but structurally different in a way the model cannot handle.\n",
    "* **Example (Negative Images)**: A negative image shares the exact same structure and semantics as the original. Humans classify it easily. However, CNNs often fail because the pixel value distribution is reversed (e.g., $0 \\rightarrow 255$), pushing the image **Out-Of-Distribution (OOD)** for the model .\n",
    "\n",
    "### B. Noise Attack\n",
    "* **Type**: Naive, Untargeted, Black-box.\n",
    "* **Mechanism**: Simply adding random noise to an image until the classification changes. It is untargeted because it doesn't aim for a specific outcome, just an error .\n",
    "\n",
    "### C. Fast Gradient Sign Method (FGSM)\n",
    "A fast, efficient **white-box** attack that shifts the input image in the direction that maximizes the loss .\n",
    "\n",
    "**The Equation**:\n",
    "$$x' = x + \\epsilon \\cdot \\text{sign}(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "**Conceptual Breakdown**:\n",
    "* **$x'$**: The adversarial image.\n",
    "* **$x$**: The original input image.\n",
    "* **$J(\\theta, x, y)$**: The loss function of the model.\n",
    "* **$\\nabla_x$**: The gradient of the loss *with respect to the input image* $x$ (not the weights). We want to know how to change the *pixels* to increase error.\n",
    "* **$\\text{sign}(\\cdot)$**: We take the sign ($+1$ or $-1$) of the gradient. This ensures we make a maximum change in the correct direction for every pixel, regardless of the gradient's magnitude.\n",
    "* **$\\epsilon$ (Epsilon)**: A multiplier (learning rate) that controls the magnitude of the perturbation. It ensures the noise is small enough to be invisible to humans but significant enough to fool the model.\n",
    "\n",
    "### D. Fast Gradient Value (FGV)\n",
    "A variation of FGSM .\n",
    "**The Equation**:\n",
    "$$x' = x + \\epsilon \\cdot \\nabla_x J(\\theta, x, y)$$\n",
    "**Difference**: Instead of using the `sign` of the gradient, FGV adds the **raw gradient value** directly. This scales the perturbation based on how sensitive each pixel is (pixels with higher gradients get larger changes).\n",
    "\n",
    "### E. Zeroth-Order Optimization (ZOO)\n",
    "A **black-box** attack for when gradients are unavailable (e.g., attacking an API) .\n",
    "\n",
    "**Mechanism**:\n",
    "Since the attacker cannot calculate the gradient $\\nabla f$ directly, they **estimate** it using the outputs of the model. They query the model with slight variations of the input and measure how the confidence scores change.\n",
    "\n",
    "**Approximation Equation (Finite Difference)**:\n",
    "$$\\hat{\\nabla}_i f(x) \\approx \\frac{f(x + \\delta e_i) - f(x - \\delta e_i)}{2\\delta}$$\n",
    "\n",
    "**Conceptual Breakdown**:\n",
    "* **$\\hat{\\nabla}_i f(x)$**: The estimated gradient for the $i$-th pixel.\n",
    "* **$\\delta$**: A very small constant (the perturbation size for estimation).\n",
    "* **$e_i$**: A standard basis vector (modifying only the $i$-th pixel).\n",
    "* **Logic**: By checking the model's output $f(x)$ when a pixel is slightly increased ($+\\delta$) versus slightly decreased ($-\\delta$), the attacker can estimate the slope (gradient) at that point without knowing the model's internals. This estimated gradient is then used to craft the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952acb5",
   "metadata": {},
   "source": [
    "# Overview  \n",
    "‚Ä¢ Benign vs Adversarial: attacks on INTegrity  \n",
    "‚Ä¢ Semantic adversarial attack  \n",
    "‚Ä¢ Noise attack  \n",
    "‚Ä¢ Fast Gradient Sign (FGS)  \n",
    "‚Ä¢ Fast Gradient Value (FGV)  \n",
    "‚Ä¢ Zeroth-Order Optimization (ZOO)  \n",
    "‚Ä¢ Recent adversarial attacks on AI  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d2242",
   "metadata": {},
   "source": [
    "Attacker wants to break integrity, make learning outcome biased, while remaining undetected  \n",
    "\n",
    "- By attacker knowledge\n",
    "    - White-box: Full model access, e.g., FSGM\n",
    "        - Pertube data in direction of max change\n",
    "    - Black-box: Only API queries, assumes unlimited trials, e.g., Zeroth-Order Optimization\n",
    "        - Query model to identify direction of gradient\n",
    "    - Restricted black-box (no-box): Black-box, but finite/no trials\n",
    "\n",
    "- By goal\n",
    "    - Targeted: Force classification to a specific class, e.g., cat ‚Üí dog\n",
    "    - Untargeted: Cause any misclassification, e.g., cat ‚Üí not cat\n",
    "\n",
    "- By timing\n",
    "    - Poisoning: Corrupt training data, e.g., biased hiring algorithm.\n",
    "    - Evasion: Fool model at test time, e.g., adversarial stop signs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a45f71",
   "metadata": {},
   "source": [
    "# Fast Gradient Value (FGV) Method\n",
    "‚Ä¢ perturbation: use gradient ‚àá of the loss J wrt the input image ùíô, aiming to maximise that loss  \n",
    "‚Ä¢ ùíô‚Äô = ùíô + ùõø = ùíô + ùúÄ*sign( ‚àáùíôJ(Œ∏,ùíô,y) ) ‚Üî FGS (Fast Gradient Sign)  \n",
    "‚Ä¢ ùíô‚Äô = ùíô + ùõø = ùíô + ùúÄ*‚àáùíôJ(Œ∏,ùíô,y) ‚Üî FGV (Fast Gradient Value)  \n",
    "\n",
    "With FGS, each pixel changes the same magnitude (+/- ùúÄ), but we still calculate the magnitude\n",
    "because we can't have the same sign all the way, so we calculate it to determine the direction  \n",
    "\n",
    "FGV scales the pertubation proportionally tot he pixel's contribution to the loss  \n",
    "\n",
    "# Zeroth-Order Optimization (ZOO) Method  \n",
    "black-box attack: use output differences to approximate gradients  \n",
    "adjust input to maximize error to cause misclassification  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f020c",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "1. Define the terms ‚ÄùAI for Security‚Äù and ‚ÄùSecurity attacks AI‚Äù. Provide one real-world example for each.  \n",
    "\n",
    "\n",
    "2. Explain the difference between conventional AI and robust AI in the context of security threats.\n",
    "Why is conventional AI considered ‚Äùtoo idealistic‚Äù?  \n",
    "Robust AI: More secure and comprehensive against security attacks  \n",
    "\n",
    "3. What are adversarial attacks in AI? How can they compromise the integrity of machine learning models?  \n",
    "\n",
    "\n",
    "4. Discuss how collaborative multi-party AI (e.g., facial recognition across countries) could introduce\n",
    "bias into machine learning outcomes. What are the security implications?  \n",
    "\n",
    "\n",
    "5. Explain the concept of Generative Adversarial Networks (GANs). How do they relate to security in\n",
    "terms of both attack and defense?  \n",
    "\n",
    "\n",
    "6. In adversarial gaming (e.g., attacker vs. defender), why is the playing field often considered unfair?\n",
    "Compare this to AI vs. human games like Chess or Go.  \n",
    "Initiative, not needing to sleep  \n",
    "\n",
    "7. A self-driving car‚Äôs AI misclassifies a stop sign due to an adversarial attack. What security goal\n",
    "(confidentiality, integrity, availability) is violated, and how could this be mitigated?  \n",
    "Verification  \n",
    "\n",
    "8. In a cybersecurity arms race, AI-powered malware evolves to bypass AI-powered defenses.\n",
    "Analyze this scenario using the adversarial gaming framework. Who has the upper hand, and why?  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
