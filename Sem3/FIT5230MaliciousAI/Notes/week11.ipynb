{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "how do I get into AI in healthcare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b76f7a",
   "metadata": {},
   "source": [
    "FIT5230 Week 11 Tutorial\n",
    "Future of AI, AI Fairness & LLM Safety  \n",
    "1. From the fairness shapley value, if we observe that a certain feature contributes a lot to the\n",
    "unfairness (e.g., marital status in demographic parity difference), is it always correct to remove\n",
    "that particular feature from the model?  \n",
    "No, because the bias may still be present in other variables, and it can still be correct even if unfair  \n",
    "\n",
    "2. Why is fairness a critical consideration in AI systems, especially in healthcare applications?  \n",
    "Because this could cause systematic unfairness  \n",
    "\n",
    "3. If an AI system in healthcare makes a biased diagnosis due to biased training data, who should be\n",
    "held responsible — the developers, the healthcare provider, or the regulator?  \n",
    "All of them  \n",
    "\n",
    "4. Do you think fairness in AI should be defined universally, or should it adapt to local cultural and\n",
    "social contexts?  \n",
    "Adapt  \n",
    "\n",
    "5. What makes AI governance especially challenging compared to traditional technology\n",
    "governance?  \n",
    "No one wants to take responsibility for this, lack of digital literacy for policymakers    \n",
    "\n",
    "6. With AI systems capable of creating realistic deepfakes, should governments regulate the use of\n",
    "generative AI or the development of the underlying models?  \n",
    "Yes  \n",
    "\n",
    "7. Imagine a future where AI systems become indistinguishable from humans in voice, face, and\n",
    "behavior. How might this reshape human trust, law, and governance?  \n",
    "\n",
    "\n",
    "8. What are the main risks associated with deploying LLMs in real-world applications?  \n",
    "Hallucinations, misinformation, privacy leakage, brain rot  \n",
    "\n",
    "9. How can we prevent LLMs from leaking sensitive information embedded in their training data?  \n",
    "Red teaming, safety audits, access control  \n",
    "\n",
    "10. LLMs often struggle with adversarial attacks such as prompt injection. Why is this problem\n",
    "particularly difficult to solve?  \n",
    "LLM can fool itself\n",
    "\n",
    "11. What role do “red-teaming” and adversarial testing play in LLM safety?  \n",
    "Actors try to break LLM safety and patch the loopholes\n",
    "\n",
    "12. What governance mechanisms can be put in place to ensure LLM safety at scale?  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
