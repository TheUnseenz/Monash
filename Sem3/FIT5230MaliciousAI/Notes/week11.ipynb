{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bfc8e9",
   "metadata": {},
   "source": [
    "# FIT5230 Week 11: AI Security, Warfare, and Governance\n",
    "\n",
    "## 1. The Dual Nature of AI: Power and Responsibility\n",
    "\n",
    "AI is ubiquitous, with tools like Midjourney, ChatGPT, and Gemini becoming commonplace. While AI is not new, advancements in hardware and software have expanded its applications significantly. However, \"with great power comes great responsibility\".\n",
    "\n",
    "### Beneficial Applications\n",
    "* **Healthcare:**\n",
    "    * **Data Efficiency:** AI processes massive datasets (like genetics) faster than humans to identify disease-linked genes .\n",
    "    * **Drug Discovery:** It accelerates finding treatments by matching existing medications to diseases (e.g., the non-profit *Every Cure*).\n",
    "    * **Considerations:** For AI to be viable in healthcare, it must be at least as accurate as human doctors, liability for errors must be established, and it must not worsen existing health inequities or discrimination .\n",
    "* **Transportation:**\n",
    "    * **Traffic Management:** AI analyzes real-time data (weather, accidents) to adjust traffic signals and optimize flow .\n",
    "    * **Navigation:** Algorithms in apps like Google Maps use historical and real-time data to suggest optimal routes .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 2. Security Threats on AI (The CIA+N Model)\n",
    "\n",
    "AI systems introduce specific vulnerabilities across standard security principles.\n",
    "\n",
    "### 1. Confidentiality (CONF)\n",
    "* **Threat:** Inadvertent exposure of sensitive information.\n",
    "* **Mechanism:** If a model is trained on personal data, it might \"leak\" private details during operation.\n",
    "* **Example:** A customer service chatbot revealing personal info from previous training conversations.\n",
    "\n",
    "### 2. Integrity (INT)\n",
    "* **Threat:** Manipulation of the system to produce harmful or incorrect outputs.\n",
    "* **Mechanism:** **Data Poisoning**, where attackers corrupt the training data.\n",
    "* **Example:** An AI diagnostic tool fed false data leads to incorrect medical treatments.\n",
    "\n",
    "### 3. Authentication (AUTH)\n",
    "* **Threat:** Bypassing authentication mechanisms.\n",
    "* **Mechanism:** Using Deepfake technology to create realistic fake identities that trick biometric scanners.\n",
    "* **Example:** Using a deepfake video to impersonate a user and gain unauthorized system access.\n",
    "\n",
    "### 4. Non-Repudiation\n",
    "* **Threat:** Difficulty in proving the origin of actions, leading to denial of involvement.\n",
    "* **Mechanism:** The ease of generating AI content allows perpetrators to plausibly deny they created malicious communications.\n",
    "* **Example:** A fraudster sending an AI-generated email and denying authorship, complicating the investigation.\n",
    "\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 3. Malicious AI Applications & Warfare\n",
    "\n",
    "### Biological Threats\n",
    "* **Gene Sequencing:** AI assists in sequencing nucleotides, which can be used to modify organisms .\n",
    "* **Democratization of Harm:** AI simplifies complex biological processes, allowing non-experts to potentially create dangerous pathogens or novel viruses .\n",
    "\n",
    "### AI in Warfare\n",
    "* **Scenario Planning:** Military powers use AI to simulate warfare scenarios and create strategic plans based on vast data analysis .\n",
    "* **Guidance & Detection:**\n",
    "    * Enhancing missile guidance accuracy.\n",
    "    * Analyzing sonar data to detect covert submarines.\n",
    "* **Current Events:** The US military is exploring the use of Large Language Models (LLMs) and Generative AI for military operations.\n",
    "\n",
    "### The AI-Augmented Adversary\n",
    "* AI amplifies human capabilities. If a human actor is malicious, an \"AI-augmented adversary\" presents a security problem much harder to solve than traditional threats.\n",
    "* The distinction between the real and virtual worlds blends, challenging traditional concepts of authentication (\"Is it you or your avatar?\") .\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 4. Real-World Impact: Bias and Disinformation\n",
    "\n",
    "AI systems can institutionalize bias and facilitate harm if not governed correctly.\n",
    "\n",
    "### Case Study: Algorithmic Bias in Justice\n",
    "A study of the COMPAS recidivism algorithm highlighted severe bias :\n",
    "* **Vernon Prater (White male):**\n",
    "    * *Record:* 2 armed robberies, 1 attempted armed robbery.\n",
    "    * *AI Rating:* **Low Risk (3)**.\n",
    "    * *Outcome:* Re-offended (Grand Theft).\n",
    "* **Brisha Borden (Black female):**\n",
    "    * *Record:* 4 juvenile misdemeanors.\n",
    "    * *AI Rating:* **High Risk (8)**.\n",
    "    * *Outcome:* Did not re-offend.\n",
    "* *Implication:* The AI learned patterns from discriminatory data, worsening inequality.\n",
    "\n",
    "### Other Risks\n",
    "* **Physical Harm:** Reports of criminals using Generative AI to plan attacks (e.g., Tesla Cybertruck explosion attempt).\n",
    "* **Disinformation:** Fully autonomous AI systems (like \"CounterCloud\") designed to generate and spread disinformation.\n",
    "* **Mental Health:** Chatbots failing to detect suicidal or violent intentions, posing risks to vulnerable users.\n",
    "\n",
    "---\n",
    "<hr>\n",
    "\n",
    "## 5. AI Governance\n",
    "\n",
    "Governance is the key to reducing harms while sharing benefits.\n",
    "\n",
    "### Regulatory Frameworks\n",
    "* **EU AI Act:** A law governing AI development in the EU, adopting a **risk-based approach** (applying different rules based on the risk level of the AI) .\n",
    "* **United States (SR-11-7):** A regulatory standard for model governance specifically in banking.\n",
    "* **Asia-Pacific:**\n",
    "    * Malaysia: Establishment of the National AI Office (NAIO).\n",
    "    * ASEAN: Guide on AI Governance and Ethics.\n",
    "\n",
    "### Challenges in Governance\n",
    "* **Lack of Evidence:** The necessary evidence base for regulation often does not exist yet.\n",
    "* **Politics:** Diverse stakeholders lead to conflicting priorities and information politics.\n",
    "* **Knowledge Gap:** Decision-makers often lack a fundamental understanding of AI technologies.\n",
    "* **Global Disparity:** Existing issues, such as the \"AI divide\" between the Global North and South, are amplified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1d825",
   "metadata": {},
   "source": [
    "how do I get into AI in healthcare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b76f7a",
   "metadata": {},
   "source": [
    "FIT5230 Week 11 Tutorial\n",
    "Future of AI, AI Fairness & LLM Safety  \n",
    "1. From the fairness shapley value, if we observe that a certain feature contributes a lot to the\n",
    "unfairness (e.g., marital status in demographic parity difference), is it always correct to remove\n",
    "that particular feature from the model?  \n",
    "No, because the bias may still be present in other variables, and it can still be correct even if unfair  \n",
    "\n",
    "2. Why is fairness a critical consideration in AI systems, especially in healthcare applications?  \n",
    "Because this could cause systematic unfairness  \n",
    "\n",
    "3. If an AI system in healthcare makes a biased diagnosis due to biased training data, who should be\n",
    "held responsible — the developers, the healthcare provider, or the regulator?  \n",
    "All of them  \n",
    "\n",
    "4. Do you think fairness in AI should be defined universally, or should it adapt to local cultural and\n",
    "social contexts?  \n",
    "Adapt  \n",
    "\n",
    "5. What makes AI governance especially challenging compared to traditional technology\n",
    "governance?  \n",
    "No one wants to take responsibility for this, lack of digital literacy for policymakers    \n",
    "\n",
    "6. With AI systems capable of creating realistic deepfakes, should governments regulate the use of\n",
    "generative AI or the development of the underlying models?  \n",
    "Yes  \n",
    "\n",
    "7. Imagine a future where AI systems become indistinguishable from humans in voice, face, and\n",
    "behavior. How might this reshape human trust, law, and governance?  \n",
    "\n",
    "\n",
    "8. What are the main risks associated with deploying LLMs in real-world applications?  \n",
    "Hallucinations, misinformation, privacy leakage, brain rot  \n",
    "\n",
    "9. How can we prevent LLMs from leaking sensitive information embedded in their training data?  \n",
    "Red teaming, safety audits, access control  \n",
    "\n",
    "10. LLMs often struggle with adversarial attacks such as prompt injection. Why is this problem\n",
    "particularly difficult to solve?  \n",
    "LLM can fool itself\n",
    "\n",
    "11. What role do “red-teaming” and adversarial testing play in LLM safety?  \n",
    "Actors try to break LLM safety and patch the loopholes\n",
    "\n",
    "12. What governance mechanisms can be put in place to ensure LLM safety at scale?  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
